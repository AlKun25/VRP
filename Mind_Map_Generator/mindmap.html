<html>
<head>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis/4.16.1/vis.css" type="text/css" />
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/vis/4.16.1/vis-network.min.js"> </script>
<center>
<h1>None</h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->

<style type="text/css">

        #mynetwork {
            width: 500px;
            height: The Visual Research Paper;
            background-color: #ffffff;
            border: 1px solid lightgray;
            position: relative;
            float: left;
        }

        

        
        #config {
            float: left;
            width: 400px;
            height: 600px;
        }
        

        
</style>

</head>

<body>
<div id = "mynetwork"></div>


<div id = "config"></div>

<script type="text/javascript">

    // initialize global variables.
    var edges;
    var nodes;
    var network; 
    var container;
    var options, data;

    
    // This method is responsible for drawing the graph, returns the drawn network
    function drawGraph() {
        var container = document.getElementById('mynetwork');
        
        

        // parsing and collecting nodes and edges from the python
        nodes = new vis.DataSet([{"color": "ffb3bf", "fill": "pink", "id": "Your Visual Research Paper", "label": "Your Visual Research Paper", "shape": "ellipse", "title": "Your Visual Research Paper", "value": 5000}, {"color": "b37d8b", "fill": "red", "id": "Abstract", "label": "Abstract", "shape": "ellipse", "title": "Abstract", "value": 3000}, {"color": "b37d8b", "fill": "red", "id": "Introduction", "label": "Introduction", "shape": "ellipse", "title": "Introduction", "value": 3000}, {"color": "b37d8b", "fill": "red", "id": "Background", "label": "Background", "shape": "ellipse", "title": "Background", "value": 3000}, {"color": "b37d8b", "fill": "red", "id": "Method", "label": "Method", "shape": "ellipse", "title": "Method", "value": 3000}, {"color": "b37d8b", "fill": "red", "id": "Multimodal Fully Convolutional Network", "label": "Multimodal Fully Convolutional Network", "shape": "ellipse", "title": "Multimodal Fully Convolutional Network", "value": 3000}, {"color": "b37d8b", "fill": "red", "id": "Text Embedding Map", "label": "Text Embedding Map", "shape": "ellipse", "title": "Text Embedding Map", "value": 3000}, {"color": "b37d8b", "fill": "red", "id": "Unsupervised Tasks", "label": "Unsupervised Tasks", "shape": "ellipse", "title": "Unsupervised Tasks", "value": 3000}, {"color": "b37d8b", "fill": "red", "id": "Synthetic Document Data", "label": "Synthetic Document Data", "shape": "ellipse", "title": "Synthetic Document Data", "value": 3000}, {"color": "b37d8b", "fill": "red", "id": "Implementation Details", "label": "Implementation Details", "shape": "ellipse", "title": "Implementation Details", "value": 3000}, {"color": "b37d8b", "fill": "red", "id": "Experiments", "label": "Experiments", "shape": "ellipse", "title": "Experiments", "value": 3000}, {"color": "b37d8b", "fill": "red", "id": "Ablation Experiment on Model Architecture", "label": "Ablation Experiment on Model Architecture", "shape": "ellipse", "title": "Ablation Experiment on Model Architecture", "value": 3000}, {"color": "b37d8b", "fill": "red", "id": "Adding Textual Information", "label": "Adding Textual Information", "shape": "ellipse", "title": "Adding Textual Information", "value": 3000}, {"color": "b37d8b", "fill": "red", "id": "Methods", "label": "Methods", "shape": "ellipse", "title": "Methods", "value": 3000}, {"color": "b37d8b", "fill": "red", "id": "Unsupervised Learning Tasks", "label": "Unsupervised Learning Tasks", "shape": "ellipse", "title": "Unsupervised Learning Tasks", "value": 3000}, {"color": "b37d8b", "fill": "red", "id": "Comparisons with Prior Art", "label": "Comparisons with Prior Art", "shape": "ellipse", "title": "Comparisons with Prior Art", "value": 3000}, {"color": "b37d8b", "fill": "red", "id": "Conclusion", "label": "Conclusion", "shape": "ellipse", "title": "Conclusion", "value": 3000}, {"color": "ffecf1", "id": "We proposed a multimodal fully convolutional network (MFCN) for document semantic structure extraction. The proposed model uses both visual and textual information. Moreover, we propose an efficient synthetic data generation method that yields per-pixel ground-truth. Our unsupervised auxiliary tasks help boost performance tapping into unlabeled real documents, facilitating better representation learning. We showed that both the multimodal approach and unsupervised tasks can help improve performance. Our results indicate that we have improved the state of the art on previously established benchmarks. In addition, we are publicly providing the large synthetic dataset (135,000 pages) as well as a new benchmark dataset: DSSE-200.", "label": "We proposed a multimodal fully convolutional network (MFCN) for document semantic structure extraction. The proposed model uses both visual and textual information. Moreover, we propose an efficient synthetic data generation method that yields per-pixel ground-truth. Our unsupervised auxiliary tasks help boost performance tapping into unlabeled real documents, facilitating better representation learning. We showed that both the multimodal approach and unsupervised tasks can help improve performance. Our results indicate that we have improved the state of the art on previously established benchmarks. In addition, we are publicly providing the large synthetic dataset (135,000 pages) as well as a new benchmark dataset: DSSE-200.", "shape": "ellipse", "title": "We proposed a multimodal fully convolutional network (MFCN) for document semantic structure extraction. The proposed model uses both visual and textual information. Moreover, we propose an efficient synthetic data generation method that yields per-pixel ground-truth. Our unsupervised auxiliary tasks help boost performance tapping into unlabeled real documents, facilitating better representation learning. We showed that both the multimodal approach and unsupervised tasks can help improve performance. Our results indicate that we have improved the state of the art on previously established benchmarks. In addition, we are publicly providing the large synthetic dataset (135,000 pages) as well as a new benchmark dataset: DSSE-200.", "value": 100}, {"color": "ffecf1", "id": "Document semantic structure extraction (DSSE) is an actively-researched area dedicated to understanding images of documents. The goal is to split a document image into regions of interest and to recognize the role of each region. It is usually done in two steps: the first step, often referred to as page segmentation, is appearance-based and attempts to distinguish text regions from regions like figures, tables and line segments. The second step, often referred to as logical structure analysis, is semantics-based and categorizes each region into semantically-relevant classes like paragraph and caption.", "label": "Document semantic structure extraction (DSSE) is an actively-researched area dedicated to understanding images of documents. The goal is to split a document image into regions of interest and to recognize the role of each region. It is usually done in two steps: the first step, often referred to as page segmentation, is appearance-based and attempts to distinguish text regions from regions like figures, tables and line segments. The second step, often referred to as logical structure analysis, is semantics-based and categorizes each region into semantically-relevant classes like paragraph and caption.", "shape": "ellipse", "title": "Document semantic structure extraction (DSSE) is an actively-researched area dedicated to understanding images of documents. The goal is to split a document image into regions of interest and to recognize the role of each region. It is usually done in two steps: the first step, often referred to as page segmentation, is appearance-based and attempts to distinguish text regions from regions like figures, tables and line segments. The second step, often referred to as logical structure analysis, is semantics-based and categorizes each region into semantically-relevant classes like paragraph and caption.", "value": 1000}, {"color": "ffecf1", "id": "In this work, we propose a unified multimodal fully convolutional network (MFCN) that simultaneously identifies both appearance-based and semantics-based classes. It is a generalized page segmentation model that additionally performs fine-grained recognition on text regions: text regions are assigned specific labels based on their semantic functionality in the document. Our approach simplifies DSSE and better supports document image understanding.", "label": "In this work, we propose a unified multimodal fully convolutional network (MFCN) that simultaneously identifies both appearance-based and semantics-based classes. It is a generalized page segmentation model that additionally performs fine-grained recognition on text regions: text regions are assigned specific labels based on their semantic functionality in the document. Our approach simplifies DSSE and better supports document image understanding.", "shape": "ellipse", "title": "In this work, we propose a unified multimodal fully convolutional network (MFCN) that simultaneously identifies both appearance-based and semantics-based classes. It is a generalized page segmentation model that additionally performs fine-grained recognition on text regions: text regions are assigned specific labels based on their semantic functionality in the document. Our approach simplifies DSSE and better supports document image understanding.", "value": 1000}, {"color": "ffecf1", "id": "We consider DSSE as a pixel-wise segmentation problem: each pixel is labeled as background, figure, table, paragraph, section heading, list, caption, etc. We show that our MFCN model trained in an end-to-end, pixels-topixels manner on document images exceeds the state-ofthe-art significantly. It eliminates the need to design complex heuristic rules and extract hand-crafted features ", "label": "We consider DSSE as a pixel-wise segmentation problem: each pixel is labeled as background, figure, table, paragraph, section heading, list, caption, etc. We show that our MFCN model trained in an end-to-end, pixels-topixels manner on document images exceeds the state-ofthe-art significantly. It eliminates the need to design complex heuristic rules and extract hand-crafted features ", "shape": "ellipse", "title": "We consider DSSE as a pixel-wise segmentation problem: each pixel is labeled as background, figure, table, paragraph, section heading, list, caption, etc. We show that our MFCN model trained in an end-to-end, pixels-topixels manner on document images exceeds the state-ofthe-art significantly. It eliminates the need to design complex heuristic rules and extract hand-crafted features ", "value": 1000}, {"color": "ffecf1", "id": "In many cases, regions like section headings or captions can be visually identified. In ", "label": "In many cases, regions like section headings or captions can be visually identified. In ", "shape": "ellipse", "title": "In many cases, regions like section headings or captions can be visually identified. In ", "value": 1000}, {"color": "ffecf1", "id": "To this end, our multimodal fully convolutional network is designed to leverage the textual information in the document as well. To incorporate textual information in a CNNbased architecture, we build a text embedding map and feed it to our MFCN. More specifically, we embed each sentence and map the embedding to the corresponding pixels where the sentence is represented in the document. ", "label": "To this end, our multimodal fully convolutional network is designed to leverage the textual information in the document as well. To incorporate textual information in a CNNbased architecture, we build a text embedding map and feed it to our MFCN. More specifically, we embed each sentence and map the embedding to the corresponding pixels where the sentence is represented in the document. ", "shape": "ellipse", "title": "To this end, our multimodal fully convolutional network is designed to leverage the textual information in the document as well. To incorporate textual information in a CNNbased architecture, we build a text embedding map and feed it to our MFCN. More specifically, we embed each sentence and map the embedding to the corresponding pixels where the sentence is represented in the document. ", "value": 1000}, {"color": "ffecf1", "id": "One of the bottlenecks in training fully convolutional networks is the need for pixel-wise ground truth data. Previous document understanding datasets ", "label": "One of the bottlenecks in training fully convolutional networks is the need for pixel-wise ground truth data. Previous document understanding datasets ", "shape": "ellipse", "title": "One of the bottlenecks in training fully convolutional networks is the need for pixel-wise ground truth data. Previous document understanding datasets ", "value": 1000}, {"color": "ffecf1", "id": "Our main contributions are summarized as follows:", "label": "Our main contributions are summarized as follows:", "shape": "ellipse", "title": "Our main contributions are summarized as follows:", "value": 1000}, {"color": "ffecf1", "id": "\u2022 We propose an end-to-end, unified network to address document semantic structure extraction. Unlike previous two-step processes, we simultaneously identify both appearance-based and semantics-based classes.", "label": "\u2022 We propose an end-to-end, unified network to address document semantic structure extraction. Unlike previous two-step processes, we simultaneously identify both appearance-based and semantics-based classes.", "shape": "ellipse", "title": "\u2022 We propose an end-to-end, unified network to address document semantic structure extraction. Unlike previous two-step processes, we simultaneously identify both appearance-based and semantics-based classes.", "value": 1000}, {"color": "ffecf1", "id": "\u2022 Our network supports both supervised training on image and text of documents, as well as unsupervised auxiliary training for better representation learning.", "label": "\u2022 Our network supports both supervised training on image and text of documents, as well as unsupervised auxiliary training for better representation learning.", "shape": "ellipse", "title": "\u2022 Our network supports both supervised training on image and text of documents, as well as unsupervised auxiliary training for better representation learning.", "value": 1000}, {"color": "ffecf1", "id": "\u2022 We propose a synthetic data generation process and use it to synthesize a large-scale dataset for training the supervised part of our deep MFCN model.", "label": "\u2022 We propose a synthetic data generation process and use it to synthesize a large-scale dataset for training the supervised part of our deep MFCN model.", "shape": "ellipse", "title": "\u2022 We propose a synthetic data generation process and use it to synthesize a large-scale dataset for training the supervised part of our deep MFCN model.", "value": 1000}, {"color": "ffecf1", "id": "Page Segmentation. Most earlier works on page segmentation ", "label": "Page Segmentation. Most earlier works on page segmentation ", "shape": "ellipse", "title": "Page Segmentation. Most earlier works on page segmentation ", "value": 700}, {"color": "ffecf1", "id": "With recent advances in deep convolutional neural networks, several neural-based models have been proposed. Chen et al. ", "label": "With recent advances in deep convolutional neural networks, several neural-based models have been proposed. Chen et al. ", "shape": "ellipse", "title": "With recent advances in deep convolutional neural networks, several neural-based models have been proposed. Chen et al. ", "value": 700}, {"color": "ffecf1", "id": "Logical Structure Analysis. Logical structure is defined as a hierarchy of logical components in documents, such as section headings, paragraphs and lists ", "label": "Logical Structure Analysis. Logical structure is defined as a hierarchy of logical components in documents, such as section headings, paragraphs and lists ", "shape": "ellipse", "title": "Logical Structure Analysis. Logical structure is defined as a hierarchy of logical components in documents, such as section headings, paragraphs and lists ", "value": 700}, {"color": "ffecf1", "id": "Collecting pixel-wise annotations for thousands or millions of images requires massive labor and cost. To this end, several methods ", "label": "Collecting pixel-wise annotations for thousands or millions of images requires massive labor and cost. To this end, several methods ", "shape": "ellipse", "title": "Collecting pixel-wise annotations for thousands or millions of images requires massive labor and cost. To this end, several methods ", "value": 700}, {"color": "ffecf1", "id": "Unsupervised Learning. Several methods have been proposed to use unsupervised learning to improve supervised learning tasks. Mairal et al. ", "label": "Unsupervised Learning. Several methods have been proposed to use unsupervised learning to improve supervised learning tasks. Mairal et al. ", "shape": "ellipse", "title": "Unsupervised Learning. Several methods have been proposed to use unsupervised learning to improve supervised learning tasks. Mairal et al. ", "value": 700}, {"color": "ffecf1", "id": "Wen et al. ", "label": "Wen et al. ", "shape": "ellipse", "title": "Wen et al. ", "value": 700}, {"color": "ffecf1", "id": "Language and Vision. Several joint learning tasks such as image captioning ", "label": "Language and Vision. Several joint learning tasks such as image captioning ", "shape": "ellipse", "title": "Language and Vision. Several joint learning tasks such as image captioning ", "value": 700}, {"color": "ffecf1", "id": "Our method does supervised training for pixel-wise segmentation with a specialized multimodal fully convolutional network that uses a text embedding map jointly with the visual cues. Moreover, our MFCN architecture also supports two unsupervised learning tasks to improve the learned document representation: a reconstruction task based on an auxiliary decoder and a consistency task evaluated in the main decoder branch along with the per-pixel segmentation loss.", "label": "Our method does supervised training for pixel-wise segmentation with a specialized multimodal fully convolutional network that uses a text embedding map jointly with the visual cues. Moreover, our MFCN architecture also supports two unsupervised learning tasks to improve the learned document representation: a reconstruction task based on an auxiliary decoder and a consistency task evaluated in the main decoder branch along with the per-pixel segmentation loss.", "shape": "ellipse", "title": "Our method does supervised training for pixel-wise segmentation with a specialized multimodal fully convolutional network that uses a text embedding map jointly with the visual cues. Moreover, our MFCN architecture also supports two unsupervised learning tasks to improve the learned document representation: a reconstruction task based on an auxiliary decoder and a consistency task evaluated in the main decoder branch along with the per-pixel segmentation loss.", "value": 100}, {"color": "ffecf1", "id": "As shown in ", "label": "As shown in ", "shape": "ellipse", "title": "As shown in ", "value": 300}, {"color": "ffecf1", "id": "First, we observe that several semantic-based classes such as section heading and caption usually occupy relatively small areas. Moreover, correctly identifying certain regions often relies on small visual cues, like lists being identified by small bullets or numbers in front of each item. This suggests that low-level features need to be used. However, because max-pooling naturally loses information during downsampling, FCN often performs poorly for small objects. Long et al. ", "label": "First, we observe that several semantic-based classes such as section heading and caption usually occupy relatively small areas. Moreover, correctly identifying certain regions often relies on small visual cues, like lists being identified by small bullets or numbers in front of each item. This suggests that low-level features need to be used. However, because max-pooling naturally loses information during downsampling, FCN often performs poorly for small objects. Long et al. ", "shape": "ellipse", "title": "First, we observe that several semantic-based classes such as section heading and caption usually occupy relatively small areas. Moreover, correctly identifying certain regions often relies on small visual cues, like lists being identified by small bullets or numbers in front of each item. This suggests that low-level features need to be used. However, because max-pooling naturally loses information during downsampling, FCN often performs poorly for small objects. Long et al. ", "value": 300}, {"color": "ffecf1", "id": "We also notice that broader context information is needed to identify certain objects. For an instance, it is often difficult to tell the difference between a list and several paragraphs by only looking at parts of them. In ", "label": "We also notice that broader context information is needed to identify certain objects. For an instance, it is often difficult to tell the difference between a list and several paragraphs by only looking at parts of them. In ", "shape": "ellipse", "title": "We also notice that broader context information is needed to identify certain objects. For an instance, it is often difficult to tell the difference between a list and several paragraphs by only looking at parts of them. In ", "value": 300}, {"color": "ffecf1", "id": "Traditional image semantic segmentation models learn the semantic meanings of objects from a visual perspective. Our task, however, also requires understanding the text in images from a linguistic perspective. Therefore, we build a text embedding map and feed it to our multimodal model to make use of both visual and textual representations.", "label": "Traditional image semantic segmentation models learn the semantic meanings of objects from a visual perspective. Our task, however, also requires understanding the text in images from a linguistic perspective. Therefore, we build a text embedding map and feed it to our multimodal model to make use of both visual and textual representations.", "shape": "ellipse", "title": "Traditional image semantic segmentation models learn the semantic meanings of objects from a visual perspective. Our task, however, also requires understanding the text in images from a linguistic perspective. Therefore, we build a text embedding map and feed it to our multimodal model to make use of both visual and textual representations.", "value": 600}, {"color": "ffecf1", "id": "We treat a sentence as the minimum unit that conveys certain semantic meanings, and represent it using a lowdimensional vector. Our sentence embedding is built by averaging embeddings for individual words. This is a simple yet effective method that has been shown to be useful in many applications, including sentiment analysis ", "label": "We treat a sentence as the minimum unit that conveys certain semantic meanings, and represent it using a lowdimensional vector. Our sentence embedding is built by averaging embeddings for individual words. This is a simple yet effective method that has been shown to be useful in many applications, including sentiment analysis ", "shape": "ellipse", "title": "We treat a sentence as the minimum unit that conveys certain semantic meanings, and represent it using a lowdimensional vector. Our sentence embedding is built by averaging embeddings for individual words. This is a simple yet effective method that has been shown to be useful in many applications, including sentiment analysis ", "value": 600}, {"color": "ffecf1", "id": "Specifically, our word embedding is learned using the skip-gram model ", "label": "Specifically, our word embedding is learned using the skip-gram model ", "shape": "ellipse", "title": "Specifically, our word embedding is learned using the skip-gram model ", "value": 600}, {"color": "ffecf1", "id": ", we maximize the average log probability", "label": ", we maximize the average log probability", "shape": "ellipse", "title": ", we maximize the average log probability", "value": 600}, {"color": "ffecf1", "id": "where T is the length of the sequence and C is the size of the context window. The probability of outputting a word w o given an input word w i is defined using softmax:", "label": "where T is the length of the sequence and C is the size of the context window. The probability of outputting a word w o given an input word w i is defined using softmax:", "shape": "ellipse", "title": "where T is the length of the sequence and C is the size of the context window. The probability of outputting a word w o given an input word w i is defined using softmax:", "value": 600}, {"color": "ffecf1", "id": "where v w and v \u2032 w are the \"input\" and \"output\" Ndimensional vector representations of w.", "label": "where v w and v \u2032 w are the \"input\" and \"output\" Ndimensional vector representations of w.", "shape": "ellipse", "title": "where v w and v \u2032 w are the \"input\" and \"output\" Ndimensional vector representations of w.", "value": 600}, {"color": "ffecf1", "id": "Although our synthetic documents (Sec. 4) provide a large amount of labeled data for training, they are limited in the variations of their layouts. To this end, we define two unsupervised loss functions to make use of real documents and to encourage better representation learning.", "label": "Although our synthetic documents (Sec. 4) provide a large amount of labeled data for training, they are limited in the variations of their layouts. To this end, we define two unsupervised loss functions to make use of real documents and to encourage better representation learning.", "shape": "ellipse", "title": "Although our synthetic documents (Sec. 4) provide a large amount of labeled data for training, they are limited in the variations of their layouts. To this end, we define two unsupervised loss functions to make use of real documents and to encourage better representation learning.", "value": 900}, {"color": "ffecf1", "id": "Reconstruction Task. It has been shown that reconstruction can help learning better representations and therefore improves performance for supervised tasks ", "label": "Reconstruction Task. It has been shown that reconstruction can help learning better representations and therefore improves performance for supervised tasks ", "shape": "ellipse", "title": "Reconstruction Task. It has been shown that reconstruction can help learning better representations and therefore improves performance for supervised tasks ", "value": 900}, {"color": "ffecf1", "id": "Consistency Task. Pixel-wise annotations are laborintensive to obtain, however it is relatively easy to get a set of bounding boxes for detected objects in a document. For documents in PDF format, one can find bounding boxes by analyzing the rendering commands in the PDF files (See our supplementary document for typical examples). Even if their labels remain unknown, these bounding boxes are still beneficial: they provide knowledge of which parts of a document belongs to the same objects and thus should not be segmented into different fragments.", "label": "Consistency Task. Pixel-wise annotations are laborintensive to obtain, however it is relatively easy to get a set of bounding boxes for detected objects in a document. For documents in PDF format, one can find bounding boxes by analyzing the rendering commands in the PDF files (See our supplementary document for typical examples). Even if their labels remain unknown, these bounding boxes are still beneficial: they provide knowledge of which parts of a document belongs to the same objects and thus should not be segmented into different fragments.", "shape": "ellipse", "title": "Consistency Task. Pixel-wise annotations are laborintensive to obtain, however it is relatively easy to get a set of bounding boxes for detected objects in a document. For documents in PDF format, one can find bounding boxes by analyzing the rendering commands in the PDF files (See our supplementary document for typical examples). Even if their labels remain unknown, these bounding boxes are still beneficial: they provide knowledge of which parts of a document belongs to the same objects and thus should not be segmented into different fragments.", "value": 900}, {"color": "ffecf1", "id": "By building on the intuition that regions belonging to same objects should have similar feature representations, we define the consistency task loss L cons as follows. Let", "label": "By building on the intuition that regions belonging to same objects should have similar feature representations, we define the consistency task loss L cons as follows. Let", "shape": "ellipse", "title": "By building on the intuition that regions belonging to same objects should have similar feature representations, we define the consistency task loss L cons as follows. Let", "value": 900}, {"color": "ffecf1", "id": "be activations at location (i, j) in a feature map of size C \u00d7 H \u00d7 W , and b be the rectangular area in a bounding box. Let each rectangular area b is of size H b \u00d7 W b . Then, for each b \u2208 B, L cons will be given by", "label": "be activations at location (i, j) in a feature map of size C \u00d7 H \u00d7 W , and b be the rectangular area in a bounding box. Let each rectangular area b is of size H b \u00d7 W b . Then, for each b \u2208 B, L cons will be given by", "shape": "ellipse", "title": "be activations at location (i, j) in a feature map of size C \u00d7 H \u00d7 W , and b be the rectangular area in a bounding box. Let each rectangular area b is of size H b \u00d7 W b . Then, for each b \u2208 B, L cons will be given by", "value": 900}, {"color": "ffecf1", "id": "Minimizing consistency loss L cons encourages intra-region consistency.", "label": "Minimizing consistency loss L cons encourages intra-region consistency.", "shape": "ellipse", "title": "Minimizing consistency loss L cons encourages intra-region consistency.", "value": 900}, {"color": "ffecf1", "id": "The consistency loss L cons is differentiable and can be optimized using stochastic gradient descent. The gradient of L cons with respect to", "label": "The consistency loss L cons is differentiable and can be optimized using stochastic gradient descent. The gradient of L cons with respect to", "shape": "ellipse", "title": "The consistency loss L cons is differentiable and can be optimized using stochastic gradient descent. The gradient of L cons with respect to", "value": 900}, {"color": "ffecf1", "id": "since H b W b \u226b 1, for efficiency it can be approximated by:", "label": "since H b W b \u226b 1, for efficiency it can be approximated by:", "shape": "ellipse", "title": "since H b W b \u226b 1, for efficiency it can be approximated by:", "value": 900}, {"color": "ffecf1", "id": "We use the unsupervised consistency loss, L cons , as a loss layer, that is evaluated at the main decoder branch (blue branch in ", "label": "We use the unsupervised consistency loss, L cons , as a loss layer, that is evaluated at the main decoder branch (blue branch in ", "shape": "ellipse", "title": "We use the unsupervised consistency loss, L cons , as a loss layer, that is evaluated at the main decoder branch (blue branch in ", "value": 900}, {"color": "ffecf1", "id": "Since our MFCN aims to generate a segmentation mask of the whole document image, pixel-wise annotations are required for the supervised task. While there are several publicly available datasets for page segmentation ", "label": "Since our MFCN aims to generate a segmentation mask of the whole document image, pixel-wise annotations are required for the supervised task. While there are several publicly available datasets for page segmentation ", "shape": "ellipse", "title": "Since our MFCN aims to generate a segmentation mask of the whole document image, pixel-wise annotations are required for the supervised task. While there are several publicly available datasets for page segmentation ", "value": 900}, {"color": "ffecf1", "id": "To address these issues, we created a synthetic data engine, capable of generating large-scale, pixel-wise annotated documents.", "label": "To address these issues, we created a synthetic data engine, capable of generating large-scale, pixel-wise annotated documents.", "shape": "ellipse", "title": "To address these issues, we created a synthetic data engine, capable of generating large-scale, pixel-wise annotated documents.", "value": 900}, {"color": "ffecf1", "id": "Our synthetic document engine uses two methods to generate documents. The first produces completely automated and random layout of partial data scraped from the web. More specifically, we generate LaTeX source files in which paragraphs, figures, tables, captions, section headings and lists are randomly arranged to make up single, double, or triple-column PDFs. Candidate figures include academicstyle figures and graphic drawings downloaded using web image search, and natural images from MS COCO ", "label": "Our synthetic document engine uses two methods to generate documents. The first produces completely automated and random layout of partial data scraped from the web. More specifically, we generate LaTeX source files in which paragraphs, figures, tables, captions, section headings and lists are randomly arranged to make up single, double, or triple-column PDFs. Candidate figures include academicstyle figures and graphic drawings downloaded using web image search, and natural images from MS COCO ", "shape": "ellipse", "title": "Our synthetic document engine uses two methods to generate documents. The first produces completely automated and random layout of partial data scraped from the web. More specifically, we generate LaTeX source files in which paragraphs, figures, tables, captions, section headings and lists are randomly arranged to make up single, double, or triple-column PDFs. Candidate figures include academicstyle figures and graphic drawings downloaded using web image search, and natural images from MS COCO ", "value": 900}, {"color": "ffecf1", "id": "\u2022 For paragraphs, we randomly sample sentences from a 2016 English Wikipedia dump ", "label": "\u2022 For paragraphs, we randomly sample sentences from a 2016 English Wikipedia dump ", "shape": "ellipse", "title": "\u2022 For paragraphs, we randomly sample sentences from a 2016 English Wikipedia dump ", "value": 900}, {"color": "ffecf1", "id": "\u2022 For section headings, we only sample sentences and phrases that are section or subsection headings in the \"Contents\" block in a Wikipedia page.", "label": "\u2022 For section headings, we only sample sentences and phrases that are section or subsection headings in the \"Contents\" block in a Wikipedia page.", "shape": "ellipse", "title": "\u2022 For section headings, we only sample sentences and phrases that are section or subsection headings in the \"Contents\" block in a Wikipedia page.", "value": 900}, {"color": "ffecf1", "id": "\u2022 For lists, we ensure that all items in a list come from the same Wikipedia page.", "label": "\u2022 For lists, we ensure that all items in a list come from the same Wikipedia page.", "shape": "ellipse", "title": "\u2022 For lists, we ensure that all items in a list come from the same Wikipedia page.", "value": 900}, {"color": "ffecf1", "id": "\u2022 For captions, we either use the associated caption (for images from MS COCO) or the title of the image in web image search, which can be found in the span with class name \"irc pt\".", "label": "\u2022 For captions, we either use the associated caption (for images from MS COCO) or the title of the image in web image search, which can be found in the span with class name \"irc pt\".", "shape": "ellipse", "title": "\u2022 For captions, we either use the associated caption (for images from MS COCO) or the title of the image in web image search, which can be found in the span with class name \"irc pt\".", "value": 900}, {"color": "ffecf1", "id": "To further increase the complexity of the generated document layouts, we collected and labeled 271 documents with varied, complicated layouts. We then randomly replaced each element with a standalone paragraph, figure, table, caption, section heading or list generated as stated above.", "label": "To further increase the complexity of the generated document layouts, we collected and labeled 271 documents with varied, complicated layouts. We then randomly replaced each element with a standalone paragraph, figure, table, caption, section heading or list generated as stated above.", "shape": "ellipse", "title": "To further increase the complexity of the generated document layouts, we collected and labeled 271 documents with varied, complicated layouts. We then randomly replaced each element with a standalone paragraph, figure, table, caption, section heading or list generated as stated above.", "value": 900}, {"color": "ffecf1", "id": "In total, our synthetic dataset contains 135,000 document images. Examples of our synthetic documents are shown in ", "label": "In total, our synthetic dataset contains 135,000 document images. Examples of our synthetic documents are shown in ", "shape": "ellipse", "title": "In total, our synthetic dataset contains 135,000 document images. Examples of our synthetic documents are shown in ", "value": 900}, {"color": "ffecf1", "id": "We perform per-channel mean subtraction and resize each input image so that its longer side is less than 384 pixels. No other pre-processing is applied. We use Adadelta ", "label": "We perform per-channel mean subtraction and resize each input image so that its longer side is less than 384 pixels. No other pre-processing is applied. We use Adadelta ", "shape": "ellipse", "title": "We perform per-channel mean subtraction and resize each input image so that its longer side is less than 384 pixels. No other pre-processing is applied. We use Adadelta ", "value": 300}, {"color": "ffecf1", "id": "For text embedding, we represent each word as a 128dimensional vector and train a skip-gram model on the 2016 English Wikipedia dump ", "label": "For text embedding, we represent each word as a 128dimensional vector and train a skip-gram model on the 2016 English Wikipedia dump ", "shape": "ellipse", "title": "For text embedding, we represent each word as a 128dimensional vector and train a skip-gram model on the 2016 English Wikipedia dump ", "value": 300}, {"color": "ffecf1", "id": "Post-processing. We apply an optional post-processing step as a cleanup strategy for segment masks. For documents in PDF format, we obtain a set of candidate bounding boxes by analyzing the PDF format to find element boxes. We then refine the segmentation masks by first calculating the average class probability for pixels belonging to the same box, followed by assigning the most likely label to these pixels.", "label": "Post-processing. We apply an optional post-processing step as a cleanup strategy for segment masks. For documents in PDF format, we obtain a set of candidate bounding boxes by analyzing the PDF format to find element boxes. We then refine the segmentation masks by first calculating the average class probability for pixels belonging to the same box, followed by assigning the most likely label to these pixels.", "shape": "ellipse", "title": "Post-processing. We apply an optional post-processing step as a cleanup strategy for segment masks. For documents in PDF format, we obtain a set of candidate bounding boxes by analyzing the PDF format to find element boxes. We then refine the segmentation masks by first calculating the average class probability for pixels belonging to the same box, followed by assigning the most likely label to these pixels.", "value": 300}, {"color": "ffecf1", "id": "We used three datasets for evaluations: ICDAR2015 ", "label": "We used three datasets for evaluations: ICDAR2015 ", "shape": "ellipse", "title": "We used three datasets for evaluations: ICDAR2015 ", "value": 200}, {"color": "ffecf1", "id": "The performance is measured in terms of pixel-wise intersection-over-union (IoU), which is standard in semantic segmentation tasks. We optimize the architecture of our MFCN model based on the DSSE-200 dataset since it contains both appearance-based and semantics-based labels. Sec. 6.4 compares our results to state-of-the-art methods on the ICDAR2015 and SectLabel datasets.", "label": "The performance is measured in terms of pixel-wise intersection-over-union (IoU), which is standard in semantic segmentation tasks. We optimize the architecture of our MFCN model based on the DSSE-200 dataset since it contains both appearance-based and semantics-based labels. Sec. 6.4 compares our results to state-of-the-art methods on the ICDAR2015 and SectLabel datasets.", "shape": "ellipse", "title": "The performance is measured in terms of pixel-wise intersection-over-union (IoU), which is standard in semantic segmentation tasks. We optimize the architecture of our MFCN model based on the DSSE-200 dataset since it contains both appearance-based and semantics-based labels. Sec. 6.4 compares our results to state-of-the-art methods on the ICDAR2015 and SectLabel datasets.", "value": 200}, {"color": "ffecf1", "id": "We first systematically evaluate the effectiveness of different network architectures. Results are shown in ", "label": "We first systematically evaluate the effectiveness of different network architectures. Results are shown in ", "shape": "ellipse", "title": "We first systematically evaluate the effectiveness of different network architectures. Results are shown in ", "value": 500}, {"color": "ffecf1", "id": "As a simple baseline ", "label": "As a simple baseline ", "shape": "ellipse", "title": "As a simple baseline ", "value": 500}, {"color": "ffecf1", "id": "Next, we add skip connections to the model, resulting in Model2. Note that this model is similar to the SharpMask model. We observe a mean IoU of 65.4%, 4% better than the base model. The improvements are even more significant for small objects like captions.", "label": "Next, we add skip connections to the model, resulting in Model2. Note that this model is similar to the SharpMask model. We observe a mean IoU of 65.4%, 4% better than the base model. The improvements are even more significant for small objects like captions.", "shape": "ellipse", "title": "Next, we add skip connections to the model, resulting in Model2. Note that this model is similar to the SharpMask model. We observe a mean IoU of 65.4%, 4% better than the base model. The improvements are even more significant for small objects like captions.", "value": 500}, {"color": "ffecf1", "id": "We further evaluate the effectiveness of replacing bilinear upsampling with unpooling, giving Model3. All upsampling layers in Model2 are replaced by unpooling while other parts are kept unchanged. Doing so results in a significant improvement for mean IoU (65.4% vs. 71.2%). This suggests that the pooled index should not be discarded during decoding. These indexes are helpful to disambiguate the location information when constructing the segmentation mask in the decoder.", "label": "We further evaluate the effectiveness of replacing bilinear upsampling with unpooling, giving Model3. All upsampling layers in Model2 are replaced by unpooling while other parts are kept unchanged. Doing so results in a significant improvement for mean IoU (65.4% vs. 71.2%). This suggests that the pooled index should not be discarded during decoding. These indexes are helpful to disambiguate the location information when constructing the segmentation mask in the decoder.", "shape": "ellipse", "title": "We further evaluate the effectiveness of replacing bilinear upsampling with unpooling, giving Model3. All upsampling layers in Model2 are replaced by unpooling while other parts are kept unchanged. Doing so results in a significant improvement for mean IoU (65.4% vs. 71.2%). This suggests that the pooled index should not be discarded during decoding. These indexes are helpful to disambiguate the location information when constructing the segmentation mask in the decoder.", "value": 500}, {"color": "ffecf1", "id": "Finally, we investigate the use of dilated convolutions. Model3 is equivalent to using dilated convolution when d = 1. Model4 sets d = 8 while Model5 uses the dilated block illustrated in ", "label": "Finally, we investigate the use of dilated convolutions. Model3 is equivalent to using dilated convolution when d = 1. Model4 sets d = 8 while Model5 uses the dilated block illustrated in ", "shape": "ellipse", "title": "Finally, we investigate the use of dilated convolutions. Model3 is equivalent to using dilated convolution when d = 1. Model4 sets d = 8 while Model5 uses the dilated block illustrated in ", "value": 500}, {"color": "ffecf1", "id": "We now investigate the importance of textual information in our multimodal model. We take the best architecture, Model5, as our vision-only model, and incorporate a text embedding map via a bridge module depicted in ", "label": "We now investigate the importance of textual information in our multimodal model. We take the best architecture, Model5, as our vision-only model, and incorporate a text embedding map via a bridge module depicted in ", "shape": "ellipse", "title": "We now investigate the importance of textual information in our multimodal model. We take the best architecture, Model5, as our vision-only model, and incorporate a text embedding map via a bridge module depicted in ", "value": 200}, {"color": "ffecf1", "id": "75.4 75.9 ", "label": "75.4 75.9 ", "shape": "ellipse", "title": "75.4 75.9 ", "value": 200}, {"color": "ffecf1", "id": "non-text text Leptonica ", "label": "non-text text Leptonica ", "shape": "ellipse", "title": "non-text text Leptonica ", "value": 100}, {"color": "ffecf1", "id": "Here, we examine how the proposed two unsupervised learning tasks -reconstruction and consistency taskscan complement the pixel-wise classification during training. We take the best model in Sec. 6.2, and only change the training objectives. Our model is then fine-tuned in a semisupervised manner as described in Sec. 5. The results are shown in ", "label": "Here, we examine how the proposed two unsupervised learning tasks -reconstruction and consistency taskscan complement the pixel-wise classification during training. We take the best model in Sec. 6.2, and only change the training objectives. Our model is then fine-tuned in a semisupervised manner as described in Sec. 5. The results are shown in ", "shape": "ellipse", "title": "Here, we examine how the proposed two unsupervised learning tasks -reconstruction and consistency taskscan complement the pixel-wise classification during training. We take the best model in Sec. 6.2, and only change the training objectives. Our model is then fine-tuned in a semisupervised manner as described in Sec. 5. The results are shown in ", "value": 100}, {"color": "ffecf1", "id": "Comparisons on ICDAR2015 dataset ", "label": "Comparisons on ICDAR2015 dataset ", "shape": "ellipse", "title": "Comparisons on ICDAR2015 dataset ", "value": 200}, {"color": "ffecf1", "id": "Comparisons on SectLabel dataset ", "label": "Comparisons on SectLabel dataset ", "shape": "ellipse", "title": "Comparisons on SectLabel dataset ", "value": 200}]);
        edges = new vis.DataSet([{"from": "Your Visual Research Paper", "to": "Abstract", "weight": 0.97}, {"from": "Your Visual Research Paper", "to": "Introduction", "weight": 0.97}, {"from": "Your Visual Research Paper", "to": "Background", "weight": 0.97}, {"from": "Your Visual Research Paper", "to": "Method", "weight": 0.97}, {"from": "Your Visual Research Paper", "to": "Multimodal Fully Convolutional Network", "weight": 0.97}, {"from": "Your Visual Research Paper", "to": "Text Embedding Map", "weight": 0.97}, {"from": "Your Visual Research Paper", "to": "Unsupervised Tasks", "weight": 0.97}, {"from": "Your Visual Research Paper", "to": "Synthetic Document Data", "weight": 0.97}, {"from": "Your Visual Research Paper", "to": "Implementation Details", "weight": 0.97}, {"from": "Your Visual Research Paper", "to": "Experiments", "weight": 0.97}, {"from": "Your Visual Research Paper", "to": "Ablation Experiment on Model Architecture", "weight": 0.97}, {"from": "Your Visual Research Paper", "to": "Adding Textual Information", "weight": 0.97}, {"from": "Your Visual Research Paper", "to": "Methods", "weight": 0.97}, {"from": "Your Visual Research Paper", "to": "Unsupervised Learning Tasks", "weight": 0.97}, {"from": "Your Visual Research Paper", "to": "Comparisons with Prior Art", "weight": 0.97}, {"from": "Your Visual Research Paper", "to": "Conclusion", "weight": 0.97}, {"from": "Conclusion", "to": "We proposed a multimodal fully convolutional network (MFCN) for document semantic structure extraction. The proposed model uses both visual and textual information. Moreover, we propose an efficient synthetic data generation method that yields per-pixel ground-truth. Our unsupervised auxiliary tasks help boost performance tapping into unlabeled real documents, facilitating better representation learning. We showed that both the multimodal approach and unsupervised tasks can help improve performance. Our results indicate that we have improved the state of the art on previously established benchmarks. In addition, we are publicly providing the large synthetic dataset (135,000 pages) as well as a new benchmark dataset: DSSE-200.", "weight": 0.87}, {"from": "Conclusion", "to": "Document semantic structure extraction (DSSE) is an actively-researched area dedicated to understanding images of documents. The goal is to split a document image into regions of interest and to recognize the role of each region. It is usually done in two steps: the first step, often referred to as page segmentation, is appearance-based and attempts to distinguish text regions from regions like figures, tables and line segments. The second step, often referred to as logical structure analysis, is semantics-based and categorizes each region into semantically-relevant classes like paragraph and caption.", "weight": 0.87}, {"from": "Conclusion", "to": "In this work, we propose a unified multimodal fully convolutional network (MFCN) that simultaneously identifies both appearance-based and semantics-based classes. It is a generalized page segmentation model that additionally performs fine-grained recognition on text regions: text regions are assigned specific labels based on their semantic functionality in the document. Our approach simplifies DSSE and better supports document image understanding.", "weight": 0.87}, {"from": "Conclusion", "to": "We consider DSSE as a pixel-wise segmentation problem: each pixel is labeled as background, figure, table, paragraph, section heading, list, caption, etc. We show that our MFCN model trained in an end-to-end, pixels-topixels manner on document images exceeds the state-ofthe-art significantly. It eliminates the need to design complex heuristic rules and extract hand-crafted features ", "weight": 0.87}, {"from": "Conclusion", "to": "In many cases, regions like section headings or captions can be visually identified. In ", "weight": 0.87}, {"from": "Conclusion", "to": "To this end, our multimodal fully convolutional network is designed to leverage the textual information in the document as well. To incorporate textual information in a CNNbased architecture, we build a text embedding map and feed it to our MFCN. More specifically, we embed each sentence and map the embedding to the corresponding pixels where the sentence is represented in the document. ", "weight": 0.87}, {"from": "Conclusion", "to": "One of the bottlenecks in training fully convolutional networks is the need for pixel-wise ground truth data. Previous document understanding datasets ", "weight": 0.87}, {"from": "Conclusion", "to": "Our main contributions are summarized as follows:", "weight": 0.87}, {"from": "Conclusion", "to": "\u2022 We propose an end-to-end, unified network to address document semantic structure extraction. Unlike previous two-step processes, we simultaneously identify both appearance-based and semantics-based classes.", "weight": 0.87}, {"from": "Conclusion", "to": "\u2022 Our network supports both supervised training on image and text of documents, as well as unsupervised auxiliary training for better representation learning.", "weight": 0.87}, {"from": "Conclusion", "to": "\u2022 We propose a synthetic data generation process and use it to synthesize a large-scale dataset for training the supervised part of our deep MFCN model.", "weight": 0.87}, {"from": "Conclusion", "to": "Page Segmentation. Most earlier works on page segmentation ", "weight": 0.87}, {"from": "Conclusion", "to": "With recent advances in deep convolutional neural networks, several neural-based models have been proposed. Chen et al. ", "weight": 0.87}, {"from": "Conclusion", "to": "Logical Structure Analysis. Logical structure is defined as a hierarchy of logical components in documents, such as section headings, paragraphs and lists ", "weight": 0.87}, {"from": "Conclusion", "to": "Collecting pixel-wise annotations for thousands or millions of images requires massive labor and cost. To this end, several methods ", "weight": 0.87}, {"from": "Conclusion", "to": "Unsupervised Learning. Several methods have been proposed to use unsupervised learning to improve supervised learning tasks. Mairal et al. ", "weight": 0.87}, {"from": "Conclusion", "to": "Wen et al. ", "weight": 0.87}, {"from": "Conclusion", "to": "Language and Vision. Several joint learning tasks such as image captioning ", "weight": 0.87}, {"from": "Conclusion", "to": "Our method does supervised training for pixel-wise segmentation with a specialized multimodal fully convolutional network that uses a text embedding map jointly with the visual cues. Moreover, our MFCN architecture also supports two unsupervised learning tasks to improve the learned document representation: a reconstruction task based on an auxiliary decoder and a consistency task evaluated in the main decoder branch along with the per-pixel segmentation loss.", "weight": 0.87}, {"from": "Conclusion", "to": "As shown in ", "weight": 0.87}, {"from": "Conclusion", "to": "First, we observe that several semantic-based classes such as section heading and caption usually occupy relatively small areas. Moreover, correctly identifying certain regions often relies on small visual cues, like lists being identified by small bullets or numbers in front of each item. This suggests that low-level features need to be used. However, because max-pooling naturally loses information during downsampling, FCN often performs poorly for small objects. Long et al. ", "weight": 0.87}, {"from": "Conclusion", "to": "We also notice that broader context information is needed to identify certain objects. For an instance, it is often difficult to tell the difference between a list and several paragraphs by only looking at parts of them. In ", "weight": 0.87}, {"from": "Conclusion", "to": "Traditional image semantic segmentation models learn the semantic meanings of objects from a visual perspective. Our task, however, also requires understanding the text in images from a linguistic perspective. Therefore, we build a text embedding map and feed it to our multimodal model to make use of both visual and textual representations.", "weight": 0.87}, {"from": "Conclusion", "to": "We treat a sentence as the minimum unit that conveys certain semantic meanings, and represent it using a lowdimensional vector. Our sentence embedding is built by averaging embeddings for individual words. This is a simple yet effective method that has been shown to be useful in many applications, including sentiment analysis ", "weight": 0.87}, {"from": "Conclusion", "to": "Specifically, our word embedding is learned using the skip-gram model ", "weight": 0.87}, {"from": "Conclusion", "to": ", we maximize the average log probability", "weight": 0.87}, {"from": "Conclusion", "to": "where T is the length of the sequence and C is the size of the context window. The probability of outputting a word w o given an input word w i is defined using softmax:", "weight": 0.87}, {"from": "Conclusion", "to": "where v w and v \u2032 w are the \"input\" and \"output\" Ndimensional vector representations of w.", "weight": 0.87}, {"from": "Conclusion", "to": "Although our synthetic documents (Sec. 4) provide a large amount of labeled data for training, they are limited in the variations of their layouts. To this end, we define two unsupervised loss functions to make use of real documents and to encourage better representation learning.", "weight": 0.87}, {"from": "Conclusion", "to": "Reconstruction Task. It has been shown that reconstruction can help learning better representations and therefore improves performance for supervised tasks ", "weight": 0.87}, {"from": "Conclusion", "to": "Consistency Task. Pixel-wise annotations are laborintensive to obtain, however it is relatively easy to get a set of bounding boxes for detected objects in a document. For documents in PDF format, one can find bounding boxes by analyzing the rendering commands in the PDF files (See our supplementary document for typical examples). Even if their labels remain unknown, these bounding boxes are still beneficial: they provide knowledge of which parts of a document belongs to the same objects and thus should not be segmented into different fragments.", "weight": 0.87}, {"from": "Conclusion", "to": "By building on the intuition that regions belonging to same objects should have similar feature representations, we define the consistency task loss L cons as follows. Let", "weight": 0.87}, {"from": "Conclusion", "to": "be activations at location (i, j) in a feature map of size C \u00d7 H \u00d7 W , and b be the rectangular area in a bounding box. Let each rectangular area b is of size H b \u00d7 W b . Then, for each b \u2208 B, L cons will be given by", "weight": 0.87}, {"from": "Conclusion", "to": "Minimizing consistency loss L cons encourages intra-region consistency.", "weight": 0.87}, {"from": "Conclusion", "to": "The consistency loss L cons is differentiable and can be optimized using stochastic gradient descent. The gradient of L cons with respect to", "weight": 0.87}, {"from": "Conclusion", "to": "since H b W b \u226b 1, for efficiency it can be approximated by:", "weight": 0.87}, {"from": "Conclusion", "to": "We use the unsupervised consistency loss, L cons , as a loss layer, that is evaluated at the main decoder branch (blue branch in ", "weight": 0.87}, {"from": "Conclusion", "to": "Since our MFCN aims to generate a segmentation mask of the whole document image, pixel-wise annotations are required for the supervised task. While there are several publicly available datasets for page segmentation ", "weight": 0.87}, {"from": "Conclusion", "to": "To address these issues, we created a synthetic data engine, capable of generating large-scale, pixel-wise annotated documents.", "weight": 0.87}, {"from": "Conclusion", "to": "Our synthetic document engine uses two methods to generate documents. The first produces completely automated and random layout of partial data scraped from the web. More specifically, we generate LaTeX source files in which paragraphs, figures, tables, captions, section headings and lists are randomly arranged to make up single, double, or triple-column PDFs. Candidate figures include academicstyle figures and graphic drawings downloaded using web image search, and natural images from MS COCO ", "weight": 0.87}, {"from": "Conclusion", "to": "\u2022 For paragraphs, we randomly sample sentences from a 2016 English Wikipedia dump ", "weight": 0.87}, {"from": "Conclusion", "to": "\u2022 For section headings, we only sample sentences and phrases that are section or subsection headings in the \"Contents\" block in a Wikipedia page.", "weight": 0.87}, {"from": "Conclusion", "to": "\u2022 For lists, we ensure that all items in a list come from the same Wikipedia page.", "weight": 0.87}, {"from": "Conclusion", "to": "\u2022 For captions, we either use the associated caption (for images from MS COCO) or the title of the image in web image search, which can be found in the span with class name \"irc pt\".", "weight": 0.87}, {"from": "Conclusion", "to": "To further increase the complexity of the generated document layouts, we collected and labeled 271 documents with varied, complicated layouts. We then randomly replaced each element with a standalone paragraph, figure, table, caption, section heading or list generated as stated above.", "weight": 0.87}, {"from": "Conclusion", "to": "In total, our synthetic dataset contains 135,000 document images. Examples of our synthetic documents are shown in ", "weight": 0.87}, {"from": "Conclusion", "to": "We perform per-channel mean subtraction and resize each input image so that its longer side is less than 384 pixels. No other pre-processing is applied. We use Adadelta ", "weight": 0.87}, {"from": "Conclusion", "to": "For text embedding, we represent each word as a 128dimensional vector and train a skip-gram model on the 2016 English Wikipedia dump ", "weight": 0.87}, {"from": "Conclusion", "to": "Post-processing. We apply an optional post-processing step as a cleanup strategy for segment masks. For documents in PDF format, we obtain a set of candidate bounding boxes by analyzing the PDF format to find element boxes. We then refine the segmentation masks by first calculating the average class probability for pixels belonging to the same box, followed by assigning the most likely label to these pixels.", "weight": 0.87}, {"from": "Conclusion", "to": "We used three datasets for evaluations: ICDAR2015 ", "weight": 0.87}, {"from": "Conclusion", "to": "The performance is measured in terms of pixel-wise intersection-over-union (IoU), which is standard in semantic segmentation tasks. We optimize the architecture of our MFCN model based on the DSSE-200 dataset since it contains both appearance-based and semantics-based labels. Sec. 6.4 compares our results to state-of-the-art methods on the ICDAR2015 and SectLabel datasets.", "weight": 0.87}, {"from": "Conclusion", "to": "We first systematically evaluate the effectiveness of different network architectures. Results are shown in ", "weight": 0.87}, {"from": "Conclusion", "to": "As a simple baseline ", "weight": 0.87}, {"from": "Conclusion", "to": "Next, we add skip connections to the model, resulting in Model2. Note that this model is similar to the SharpMask model. We observe a mean IoU of 65.4%, 4% better than the base model. The improvements are even more significant for small objects like captions.", "weight": 0.87}, {"from": "Conclusion", "to": "We further evaluate the effectiveness of replacing bilinear upsampling with unpooling, giving Model3. All upsampling layers in Model2 are replaced by unpooling while other parts are kept unchanged. Doing so results in a significant improvement for mean IoU (65.4% vs. 71.2%). This suggests that the pooled index should not be discarded during decoding. These indexes are helpful to disambiguate the location information when constructing the segmentation mask in the decoder.", "weight": 0.87}, {"from": "Conclusion", "to": "Finally, we investigate the use of dilated convolutions. Model3 is equivalent to using dilated convolution when d = 1. Model4 sets d = 8 while Model5 uses the dilated block illustrated in ", "weight": 0.87}, {"from": "Conclusion", "to": "We now investigate the importance of textual information in our multimodal model. We take the best architecture, Model5, as our vision-only model, and incorporate a text embedding map via a bridge module depicted in ", "weight": 0.87}, {"from": "Conclusion", "to": "75.4 75.9 ", "weight": 0.87}, {"from": "Conclusion", "to": "non-text text Leptonica ", "weight": 0.87}, {"from": "Conclusion", "to": "Here, we examine how the proposed two unsupervised learning tasks -reconstruction and consistency taskscan complement the pixel-wise classification during training. We take the best model in Sec. 6.2, and only change the training objectives. Our model is then fine-tuned in a semisupervised manner as described in Sec. 5. The results are shown in ", "weight": 0.87}, {"from": "Conclusion", "to": "Comparisons on ICDAR2015 dataset ", "weight": 0.87}, {"from": "Conclusion", "to": "Comparisons on SectLabel dataset ", "weight": 0.87}]);

        // adding nodes and edges to the graph
        data = {nodes: nodes, edges: edges};

        var options = {
    "configure": {
        "enabled": true,
        "filter": [
            "physics"
        ]
    },
    "edges": {
        "color": {
            "inherit": true
        },
        "smooth": {
            "enabled": false,
            "type": "continuous"
        }
    },
    "interaction": {
        "dragNodes": true,
        "hideEdgesOnDrag": false,
        "hideNodesOnDrag": false
    },
    "physics": {
        "enabled": true,
        "stabilization": {
            "enabled": true,
            "fit": true,
            "iterations": 1000,
            "onlyDynamicEdges": false,
            "updateInterval": 50
        }
    }
};
        
        

        
        // if this network requires displaying the configure window,
        // put it in its div
        options.configure["container"] = document.getElementById("config");
        

        network = new vis.Network(container, data, options);

        


        

        return network;

    }

    drawGraph();

</script>
</body>
</html>