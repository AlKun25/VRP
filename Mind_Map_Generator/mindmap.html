<html>
<head>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis/4.16.1/vis.css" type="text/css" />
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/vis/4.16.1/vis-network.min.js"> </script>
<center>
<h1>None</h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->

<style type="text/css">

        #mynetwork {
            width: 500px;
            height: The Visual Research Paper;
            background-color: #ffffff;
            border: 1px solid lightgray;
            position: relative;
            float: left;
        }

        

        
        #config {
            float: left;
            width: 400px;
            height: 600px;
        }
        

        
</style>

</head>

<body>
<div id = "mynetwork"></div>


<div id = "config"></div>

<script type="text/javascript">

    // initialize global variables.
    var edges;
    var nodes;
    var network; 
    var container;
    var options, data;

    
    // This method is responsible for drawing the graph, returns the drawn network
    function drawGraph() {
        var container = document.getElementById('mynetwork');
        
        

        // parsing and collecting nodes and edges from the python
        nodes = new vis.DataSet([{"id": "Your Visual Research Paper", "label": "Your Visual Research Paper", "shape": "circle", "title": "Your Visual Research Paper", "value": 500000}, {"id": "Abstract", "label": "Abstract", "shape": "ellipse", "title": "Abstract", "value": 30000}, {"id": "Spatial understanding is a fundamental problem with widereaching real-world applications. The representation of spatial knowledge is often modeled with spatial templates, i.e., regions of acceptability of two objects under an explicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with prior work that restricts spatial templates to explicit spatial prepositions (e.g., \"glass on table\"), here we extend this concept to implicit spatial language, i.e., those relationships (generally actions) for which the spatial arrangement of the objects is only implicitly implied (e.g., \"man riding horse\"). In contrast with explicit relationships, predicting spatial arrangements from implicit spatial language requires significant common sense spatial understanding. Here, we introduce the task of predicting spatial templates for two objects under a relationship, which can be seen as a spatial question-answering task with a (2D) continuous output (\"where is the man w.r.t. a horse when the man is walking the horse?\"). We present two simple neural-based models that leverage annotated images and structured text to learn this task. The good performance of these models reveals that spatial locations are to a large extent predictable from implicit spatial language. Crucially, the models attain similar performance in a challenging generalized setting, where the object-relation-object combinations (e.g.,\"man walking dog\") have never been seen before. Next, we go one step further by presenting the models with unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging word embeddings enables the models to output accurate spatial predictions, proving that the models acquire solid common sense spatial knowledge allowing for such generalization.", "label": "Spatial understanding is a fundamental problem with widereaching real-world applications. The representation of spatial knowledge is often modeled with spatial templates, i.e., regions of acceptability of two objects under an explicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with prior work that restricts spatial templates to explicit spatial prepositions (e.g., \"glass on table\"), here we extend this concept to implicit spatial language, i.e., those relationships (generally actions) for which the spatial arrangement of the objects is only implicitly implied (e.g., \"man riding horse\"). In contrast with explicit relationships, predicting spatial arrangements from implicit spatial language requires significant common sense spatial understanding. Here, we introduce the task of predicting spatial templates for two objects under a relationship, which can be seen as a spatial question-answering task with a (2D) continuous output (\"where is the man w.r.t. a horse when the man is walking the horse?\"). We present two simple neural-based models that leverage annotated images and structured text to learn this task. The good performance of these models reveals that spatial locations are to a large extent predictable from implicit spatial language. Crucially, the models attain similar performance in a challenging generalized setting, where the object-relation-object combinations (e.g.,\"man walking dog\") have never been seen before. Next, we go one step further by presenting the models with unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging word embeddings enables the models to output accurate spatial predictions, proving that the models acquire solid common sense spatial knowledge allowing for such generalization.", "shape": "textbox", "title": "Spatial understanding is a fundamental problem with widereaching real-world applications. The representation of spatial knowledge is often modeled with spatial templates, i.e., regions of acceptability of two objects under an explicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with prior work that restricts spatial templates to explicit spatial prepositions (e.g., \"glass on table\"), here we extend this concept to implicit spatial language, i.e., those relationships (generally actions) for which the spatial arrangement of the objects is only implicitly implied (e.g., \"man riding horse\"). In contrast with explicit relationships, predicting spatial arrangements from implicit spatial language requires significant common sense spatial understanding. Here, we introduce the task of predicting spatial templates for two objects under a relationship, which can be seen as a spatial question-answering task with a (2D) continuous output (\"where is the man w.r.t. a horse when the man is walking the horse?\"). We present two simple neural-based models that leverage annotated images and structured text to learn this task. The good performance of these models reveals that spatial locations are to a large extent predictable from implicit spatial language. Crucially, the models attain similar performance in a challenging generalized setting, where the object-relation-object combinations (e.g.,\"man walking dog\") have never been seen before. Next, we go one step further by presenting the models with unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging word embeddings enables the models to output accurate spatial predictions, proving that the models acquire solid common sense spatial knowledge allowing for such generalization.", "value": 8000}, {"id": "Introduction", "label": "Introduction", "shape": "ellipse", "title": "Introduction", "value": 30000}, {"id": "To provide machines with common sense is one of the major long term goals of artificial intelligence. Common sense knowledge regards knowledge that humans have acquired through a lifetime of experiences. It is crucial in language understanding because a lot of content needed for correct understanding is not expressed explicitly but resides in the mind of communicator and audience. In addition, humans rely on their common sense knowledge when performing a variety of tasks including interpreting images, navigation and reasoning, to name a few. Representing and understanding spatial knowledge are in fact imperative for any agent (human, animal or robot) that navigates in a physical world. In this paper, we are interested in acquiring spatial commonsense knowledge from language paired with visual data.", "label": "To provide machines with common sense is one of the major long term goals of artificial intelligence. Common sense knowledge regards knowledge that humans have acquired through a lifetime of experiences. It is crucial in language understanding because a lot of content needed for correct understanding is not expressed explicitly but resides in the mind of communicator and audience. In addition, humans rely on their common sense knowledge when performing a variety of tasks including interpreting images, navigation and reasoning, to name a few. Representing and understanding spatial knowledge are in fact imperative for any agent (human, animal or robot) that navigates in a physical world. In this paper, we are interested in acquiring spatial commonsense knowledge from language paired with visual data.", "shape": "textbox", "title": "To provide machines with common sense is one of the major long term goals of artificial intelligence. Common sense knowledge regards knowledge that humans have acquired through a lifetime of experiences. It is crucial in language understanding because a lot of content needed for correct understanding is not expressed explicitly but resides in the mind of communicator and audience. In addition, humans rely on their common sense knowledge when performing a variety of tasks including interpreting images, navigation and reasoning, to name a few. Representing and understanding spatial knowledge are in fact imperative for any agent (human, animal or robot) that navigates in a physical world. In this paper, we are interested in acquiring spatial commonsense knowledge from language paired with visual data.", "value": 12000}, {"id": "Computational and cognitive models often handle spatial representations as spatial templates or regions of acceptability for two objects under an explicit (a.k.a. deictic) spatial preposition such as \"on\", \"below\" or \"left\" ", "label": "Computational and cognitive models often handle spatial representations as spatial templates or regions of acceptability for two objects under an explicit (a.k.a. deictic) spatial preposition such as \"on\", \"below\" or \"left\" ", "shape": "textbox", "title": "Computational and cognitive models often handle spatial representations as spatial templates or regions of acceptability for two objects under an explicit (a.k.a. deictic) spatial preposition such as \"on\", \"below\" or \"left\" ", "value": 12000}, {"id": "Predicting spatial templates for implicit relationships is notably more challenging than for explicit relationships. Firstly, whereas there are only a few tens of explicit spatial prepositions, there exist thousands of actions, entailing thus a drastic increase in the sparsity of (object 1 , relationship, object 2 ) combinations. Secondly, the complexity of the task radically increases in implicit language. More precisely, while explicit spatial prepositions 1 are highly deterministic about the spatial arrangements (e.g., (object 1 , below, object 2 ) unequivocally implies that object 1 is relatively lower than object 2 ), actions generally are not. E.g., the relative spatial configuration of \"man\" and the object is clearly distinct in (man, pulling, kite) than in (man, pulling, luggage) yet the action is the same. Contrarily, other relationships such as \"jumping\" are highly informative about the spatial template, i.e., in (object 1 , jumping, object 2 ), object 2 is in a lower position than object 1 . Hence, unlike explicit relationships, predicting spatial layouts from implicit spatial language requires spatial common sense knowledge about the objects, actions and their interaction, which suggests the need of learning to compose the triplet (Subject, Relationship, Object) as a whole instead of learning a template for each Relationship.", "label": "Predicting spatial templates for implicit relationships is notably more challenging than for explicit relationships. Firstly, whereas there are only a few tens of explicit spatial prepositions, there exist thousands of actions, entailing thus a drastic increase in the sparsity of (object 1 , relationship, object 2 ) combinations. Secondly, the complexity of the task radically increases in implicit language. More precisely, while explicit spatial prepositions 1 are highly deterministic about the spatial arrangements (e.g., (object 1 , below, object 2 ) unequivocally implies that object 1 is relatively lower than object 2 ), actions generally are not. E.g., the relative spatial configuration of \"man\" and the object is clearly distinct in (man, pulling, kite) than in (man, pulling, luggage) yet the action is the same. Contrarily, other relationships such as \"jumping\" are highly informative about the spatial template, i.e., in (object 1 , jumping, object 2 ), object 2 is in a lower position than object 1 . Hence, unlike explicit relationships, predicting spatial layouts from implicit spatial language requires spatial common sense knowledge about the objects, actions and their interaction, which suggests the need of learning to compose the triplet (Subject, Relationship, Object) as a whole instead of learning a template for each Relationship.", "shape": "textbox", "title": "Predicting spatial templates for implicit relationships is notably more challenging than for explicit relationships. Firstly, whereas there are only a few tens of explicit spatial prepositions, there exist thousands of actions, entailing thus a drastic increase in the sparsity of (object 1 , relationship, object 2 ) combinations. Secondly, the complexity of the task radically increases in implicit language. More precisely, while explicit spatial prepositions 1 are highly deterministic about the spatial arrangements (e.g., (object 1 , below, object 2 ) unequivocally implies that object 1 is relatively lower than object 2 ), actions generally are not. E.g., the relative spatial configuration of \"man\" and the object is clearly distinct in (man, pulling, kite) than in (man, pulling, luggage) yet the action is the same. Contrarily, other relationships such as \"jumping\" are highly informative about the spatial template, i.e., in (object 1 , jumping, object 2 ), object 2 is in a lower position than object 1 . Hence, unlike explicit relationships, predicting spatial layouts from implicit spatial language requires spatial common sense knowledge about the objects, actions and their interaction, which suggests the need of learning to compose the triplet (Subject, Relationship, Object) as a whole instead of learning a template for each Relationship.", "value": 12000}, {"id": "To systematically study these questions, we propose the task of predicting the relative spatial locations of two objects given a structured text input (Subject, Relationship, Object). We introduce two simple neural-based models trained from annotated images that successfully address the two challenges of implicit spatial language discussed above. Our quantitative evaluation reveals that spatial templates can be reliably predicted from implicit spatial language-as accurately as from explicit spatial language. We also show that our models generalize well to templates of unseen combinations, e.g., predicting (man, riding, elephant) without having been exposed to such scene before, tackling thus the challenge of sparsity. Furthermore, by leveraging word embeddings, the models can correctly generalize to spatial templates with unseen words, e.g., predicting (man, riding, elephant) without having ever seen an \"elephant\" before. Since word embeddings capture attributes of objects ", "label": "To systematically study these questions, we propose the task of predicting the relative spatial locations of two objects given a structured text input (Subject, Relationship, Object). We introduce two simple neural-based models trained from annotated images that successfully address the two challenges of implicit spatial language discussed above. Our quantitative evaluation reveals that spatial templates can be reliably predicted from implicit spatial language-as accurately as from explicit spatial language. We also show that our models generalize well to templates of unseen combinations, e.g., predicting (man, riding, elephant) without having been exposed to such scene before, tackling thus the challenge of sparsity. Furthermore, by leveraging word embeddings, the models can correctly generalize to spatial templates with unseen words, e.g., predicting (man, riding, elephant) without having ever seen an \"elephant\" before. Since word embeddings capture attributes of objects ", "shape": "textbox", "title": "To systematically study these questions, we propose the task of predicting the relative spatial locations of two objects given a structured text input (Subject, Relationship, Object). We introduce two simple neural-based models trained from annotated images that successfully address the two challenges of implicit spatial language discussed above. Our quantitative evaluation reveals that spatial templates can be reliably predicted from implicit spatial language-as accurately as from explicit spatial language. We also show that our models generalize well to templates of unseen combinations, e.g., predicting (man, riding, elephant) without having been exposed to such scene before, tackling thus the challenge of sparsity. Furthermore, by leveraging word embeddings, the models can correctly generalize to spatial templates with unseen words, e.g., predicting (man, riding, elephant) without having ever seen an \"elephant\" before. Since word embeddings capture attributes of objects ", "value": 12000}, {"id": "The rest of the paper is organized as follows. In Sect. 2 we review related research. In Sect. 3 we first introduce the task of predicting spatial templates and then present two simple neural models. Then, in Sect. 4, we describe our experimental setup. In Sect. 5 we present and discuss our results. Finally, in Sect. 6 we summarize the contributions of this article.", "label": "The rest of the paper is organized as follows. In Sect. 2 we review related research. In Sect. 3 we first introduce the task of predicting spatial templates and then present two simple neural models. Then, in Sect. 4, we describe our experimental setup. In Sect. 5 we present and discuss our results. Finally, in Sect. 6 we summarize the contributions of this article.", "shape": "textbox", "title": "The rest of the paper is organized as follows. In Sect. 2 we review related research. In Sect. 3 we first introduce the task of predicting spatial templates and then present two simple neural models. Then, in Sect. 4, we describe our experimental setup. In Sect. 5 we present and discuss our results. Finally, in Sect. 6 we summarize the contributions of this article.", "value": 12000}, {"id": "Related work", "label": "Related work", "shape": "ellipse", "title": "Related work", "value": 30000}, {"id": "Spatial processing has drawn significant attention from the cognitive ", "label": "Spatial processing has drawn significant attention from the cognitive ", "shape": "textbox", "title": "Spatial processing has drawn significant attention from the cognitive ", "value": 12000}, {"id": "Spatial templates. Earlier approaches have predominantly considered rule-based spatial representations ", "label": "Spatial templates. Earlier approaches have predominantly considered rule-based spatial representations ", "shape": "textbox", "title": "Spatial templates. Earlier approaches have predominantly considered rule-based spatial representations ", "value": 12000}, {"id": "Additionally, while they build a spatial template for each (explicit) Relationship, we build a template for each (Subject, Relationship, Object) combination, allowing the template to be determined by the interaction/composition of the Subject, Relationship and Object instead of the Relationship alone. Additionally, the model from ", "label": "Additionally, while they build a spatial template for each (explicit) Relationship, we build a template for each (Subject, Relationship, Object) combination, allowing the template to be determined by the interaction/composition of the Subject, Relationship and Object instead of the Relationship alone. Additionally, the model from ", "shape": "textbox", "title": "Additionally, while they build a spatial template for each (explicit) Relationship, we build a template for each (Subject, Relationship, Object) combination, allowing the template to be determined by the interaction/composition of the Subject, Relationship and Object instead of the Relationship alone. Additionally, the model from ", "value": 12000}, {"id": "Leveraging spatial knowledge in tasks. It has been shown that knowledge of the spatial structure in images improves the task of image captioning ", "label": "Leveraging spatial knowledge in tasks. It has been shown that knowledge of the spatial structure in images improves the task of image captioning ", "shape": "textbox", "title": "Leveraging spatial knowledge in tasks. It has been shown that knowledge of the spatial structure in images improves the task of image captioning ", "value": 12000}, {"id": "Common sense spatial knowledge. ", "label": "Common sense spatial knowledge. ", "shape": "textbox", "title": "Common sense spatial knowledge. ", "value": 12000}, {"id": "Image generation. Although models that generate images from text exist (e.g., DRAW model ", "label": "Image generation. Although models that generate images from text exist (e.g., DRAW model ", "shape": "textbox", "title": "Image generation. Although models that generate images from text exist (e.g., DRAW model ", "value": 12000}, {"id": "As discussed, spatial knowledge can improve a wide range of tasks ", "label": "As discussed, spatial knowledge can improve a wide range of tasks ", "shape": "textbox", "title": "As discussed, spatial knowledge can improve a wide range of tasks ", "value": 12000}, {"id": "Obj.", "label": "Obj.", "shape": "textbox", "title": "Obj.", "value": 12000}, {"id": "Subj.", "label": "Subj.", "shape": "textbox", "title": "Subj.", "value": 12000}, {"id": "x y Elliott and Keller 2013). This suggests that the predictions of our models can be used as spatial common sense input for methods that rely on good spatial priors. Additionally, existing methods require the spatial information to be present in the data and lack the capacity to extrapolate/generalize. Thus, in this paper we focus, first, on showing that generalizing spatial arrangements to unseen objects and object-relation-object combinations is possible and, second, on ensuring that the predicted templates are accurate by performing several quantitative and qualitative evaluations.", "label": "x y Elliott and Keller 2013). This suggests that the predictions of our models can be used as spatial common sense input for methods that rely on good spatial priors. Additionally, existing methods require the spatial information to be present in the data and lack the capacity to extrapolate/generalize. Thus, in this paper we focus, first, on showing that generalizing spatial arrangements to unseen objects and object-relation-object combinations is possible and, second, on ensuring that the predicted templates are accurate by performing several quantitative and qualitative evaluations.", "shape": "textbox", "title": "x y Elliott and Keller 2013). This suggests that the predictions of our models can be used as spatial common sense input for methods that rely on good spatial priors. Additionally, existing methods require the spatial information to be present in the data and lack the capacity to extrapolate/generalize. Thus, in this paper we focus, first, on showing that generalizing spatial arrangements to unseen objects and object-relation-object combinations is possible and, second, on ensuring that the predicted templates are accurate by performing several quantitative and qualitative evaluations.", "value": 12000}, {"id": "3 Proposed task and model", "label": "3 Proposed task and model", "shape": "textbox", "title": "3 Proposed task and model", "value": 12000}, {"id": "Proposed task", "label": "Proposed task", "shape": "ellipse", "title": "Proposed task", "value": 30000}, {"id": "To learn spatial templates, we propose the task of predicting the 2D relative spatial arrangement of two objects under a relationship given a structured text input of the form (Subject, Relationship, Object)-henceforth abbreviated as (S, R, O). Let us denote the 2D coordinates of the center (\"c\") of the Object\u0027s box as", "label": "To learn spatial templates, we propose the task of predicting the 2D relative spatial arrangement of two objects under a relationship given a structured text input of the form (Subject, Relationship, Object)-henceforth abbreviated as (S, R, O). Let us denote the 2D coordinates of the center (\"c\") of the Object\u0027s box as", "shape": "textbox", "title": "To learn spatial templates, we propose the task of predicting the 2D relative spatial arrangement of two objects under a relationship given a structured text input of the form (Subject, Relationship, Object)-henceforth abbreviated as (S, R, O). Let us denote the 2D coordinates of the center (\"c\") of the Object\u0027s box as", "value": 13000}, {"id": "y \u2208 R are the horizontal and vertical components respectively. Let", "label": "y \u2208 R are the horizontal and vertical components respectively. Let", "shape": "textbox", "title": "y \u2208 R are the horizontal and vertical components respectively. Let", "value": 13000}, {"id": "x , O b y ] \u2208 R 2 be half of the width (O b x ) and half of the height (O b y ) of the Object\u0027s box (\"b\"). We employ a similar notation for the Subject (S c , S b ), and model predictions are denoted with a hat O c , O b . The task consists in predicting the Object\u0027s location and size", "label": "x , O b y ] \u2208 R 2 be half of the width (O b x ) and half of the height (O b y ) of the Object\u0027s box (\"b\"). We employ a similar notation for the Subject (S c , S b ), and model predictions are denoted with a hat O c , O b . The task consists in predicting the Object\u0027s location and size", "shape": "textbox", "title": "x , O b y ] \u2208 R 2 be half of the width (O b x ) and half of the height (O b y ) of the Object\u0027s box (\"b\"). We employ a similar notation for the Subject (S c , S b ), and model predictions are denoted with a hat O c , O b . The task consists in predicting the Object\u0027s location and size", "value": 13000}, {"id": "given the structured text input (S, R, O) and the location S c and size S b of the Subject ", "label": "given the structured text input (S, R, O) and the location S c and size S b of the Subject ", "shape": "textbox", "title": "given the structured text input (S, R, O) and the location S c and size S b of the Subject ", "value": 13000}, {"id": "2 Crucially, we notice that knowing the Subject\u0027s coordinates is not a requirement for generating templates (but only for evaluating against the ground truth) since inputting arbitrary coordinates (e.g., S c =[0.5, 0.5]) still enables visualizing relative object locations. Additionally, we input the Subject\u0027s size in order to provide a reference size to the model. However, we find that without this input, the model still learns to predict an \"average size\" for each Object.", "label": "2 Crucially, we notice that knowing the Subject\u0027s coordinates is not a requirement for generating templates (but only for evaluating against the ground truth) since inputting arbitrary coordinates (e.g., S c =[0.5, 0.5]) still enables visualizing relative object locations. Additionally, we input the Subject\u0027s size in order to provide a reference size to the model. However, we find that without this input, the model still learns to predict an \"average size\" for each Object.", "shape": "textbox", "title": "2 Crucially, we notice that knowing the Subject\u0027s coordinates is not a requirement for generating templates (but only for evaluating against the ground truth) since inputting arbitrary coordinates (e.g., S c =[0.5, 0.5]) still enables visualizing relative object locations. Additionally, we input the Subject\u0027s size in order to provide a reference size to the model. However, we find that without this input, the model still learns to predict an \"average size\" for each Object.", "value": 13000}, {"id": "Hence, the goal is to answer common sense spatial questions such as \"if a man is feeding a horse, where is the man relative to the horse?\" or \"where would a child wear her shoes?\"", "label": "Hence, the goal is to answer common sense spatial questions such as \"if a man is feeding a horse, where is the man relative to the horse?\" or \"where would a child wear her shoes?\"", "shape": "textbox", "title": "Hence, the goal is to answer common sense spatial questions such as \"if a man is feeding a horse, where is the man relative to the horse?\" or \"where would a child wear her shoes?\"", "value": 13000}, {"id": "Proposed models", "label": "Proposed models", "shape": "ellipse", "title": "Proposed models", "value": 30000}, {"id": "To build a mapping from the input to the output in our task (Sect. 3.1) we propose two simple neural models ( ", "label": "To build a mapping from the input to the output in our task (Sect. 3.1) we propose two simple neural models ( ", "shape": "textbox", "title": "To build a mapping from the input to the output in our task (Sect. 3.1) we propose two simple neural models ( ", "value": 15000}, {"id": "are one-hot encodings of S, R, O (a.k.a. one-of-k encoding, i.e., a sparse vector with 0 everywhere except for a 1 at the position of the k-th word) and", "label": "are one-hot encodings of S, R, O (a.k.a. one-of-k encoding, i.e., a sparse vector with 0 everywhere except for a 1 at the position of the k-th word) and", "shape": "textbox", "title": "are one-hot encodings of S, R, O (a.k.a. one-of-k encoding, i.e., a sparse vector with 0 everywhere except for a 1 at the position of the k-th word) and", "value": 15000}, {"id": "This layer represents objects and relationships as continuous features, enabling thus to introduce external knowledge of unseen objects as features. The embeddings are then concatenated with the Subject center S c and size", "label": "This layer represents objects and relationships as continuous features, enabling thus to introduce external knowledge of unseen objects as features. The embeddings are then concatenated with the Subject center S c and size", "shape": "textbox", "title": "This layer represents objects and relationships as continuous features, enabling thus to introduce external knowledge of unseen objects as features. The embeddings are then concatenated with the Subject center S c and size", "value": 15000}, {"id": "which is inputted to a stack of hidden layers that compose S, R and O into a joint hidden representation z h :", "label": "which is inputted to a stack of hidden layers that compose S, R and O into a joint hidden representation z h :", "shape": "textbox", "title": "which is inputted to a stack of hidden layers that compose S, R and O into a joint hidden representation z h :", "value": 15000}, {"id": "where f (\u2022) is an element-wise non-linear function and W h and b h are the weight matrix and bias respectively. 3 (ii) Output and loss. We consider two different possibilities for the output\u0177 that follows immediately after the last layer: ", "label": "where f (\u2022) is an element-wise non-linear function and W h and b h are the weight matrix and bias respectively. 3 (ii) Output and loss. We consider two different possibilities for the output\u0177 that follows immediately after the last layer: ", "shape": "textbox", "title": "where f (\u2022) is an element-wise non-linear function and W h and b h are the weight matrix and bias respectively. 3 (ii) Output and loss. We consider two different possibilities for the output\u0177 that follows immediately after the last layer: ", "value": 15000}, {"id": "These models are conceptually different and have different capabilities. While the REG model outputs \"crisp\" pointwise predictions, PIX can model more diffuse spatial templates where the location of the object has more variability, e.g., in (man, flying, kite) the \"kite\" can easily move around. Notice that in contrast with convolutional neural networks (CNNs) our approach does not make use of the image pixels ( ", "label": "These models are conceptually different and have different capabilities. While the REG model outputs \"crisp\" pointwise predictions, PIX can model more diffuse spatial templates where the location of the object has more variability, e.g., in (man, flying, kite) the \"kite\" can easily move around. Notice that in contrast with convolutional neural networks (CNNs) our approach does not make use of the image pixels ( ", "shape": "textbox", "title": "These models are conceptually different and have different capabilities. While the REG model outputs \"crisp\" pointwise predictions, PIX can model more diffuse spatial templates where the location of the object has more variability, e.g., in (man, flying, kite) the \"kite\" can easily move around. Notice that in contrast with convolutional neural networks (CNNs) our approach does not make use of the image pixels ( ", "value": 15000}, {"id": "Experimental setup", "label": "Experimental setup", "shape": "ellipse", "title": "Experimental setup", "value": 30000}, {"id": "We employ a 10-fold cross-validation (CV) setting. Data are randomly split into 10 disjoint parts and 10% is employed for testing and 90% for training, repeating this for each of the 10 folds. Reported results are averages over the 10 folds.", "label": "We employ a 10-fold cross-validation (CV) setting. Data are randomly split into 10 disjoint parts and 10% is employed for testing and 90% for training, repeating this for each of the 10 folds. Reported results are averages over the 10 folds.", "shape": "textbox", "title": "We employ a 10-fold cross-validation (CV) setting. Data are randomly split into 10 disjoint parts and 10% is employed for testing and 90% for training, repeating this for each of the 10 folds. Reported results are averages over the 10 folds.", "value": 18000}, {"id": "Visual Genome data set", "label": "Visual Genome data set", "shape": "ellipse", "title": "Visual Genome data set", "value": 30000}, {"id": "We use the Visual Genome dataset ", "label": "We use the Visual Genome dataset ", "shape": "textbox", "title": "We use the Visual Genome dataset ", "value": 22000}, {"id": "dog, catches, frisbee boy, feeds, giraffe man, throws, frisbee cat, wears glasses ", "label": "dog, catches, frisbee boy, feeds, giraffe man, throws, frisbee cat, wears glasses ", "shape": "textbox", "title": "dog, catches, frisbee boy, feeds, giraffe man, throws, frisbee cat, wears glasses ", "value": 22000}, {"id": "Evaluation sets", "label": "Evaluation sets", "shape": "ellipse", "title": "Evaluation sets", "value": 30000}, {"id": "We consider the following subsets of the Visual Genome data to evaluate performance. (i) Raw data: Simply the unfiltered instances from the Visual Genome data (Sect. 4.1). This set contains a substantial proportion of meaningless (e.g., (nose, almost, touching)) and irrelevant (e.g., (sign, says, gate 2)) instances. (ii) Generalized Triplets: We pick at random 100 combinations (S, R, O) among the 1,000 most frequent implicit combinations in Visual Genome. This yields \u223c25K instances such as (person, holding, racket), (man, flying, kite), etc. 4 (iii) Generalized Words: We randomly choose 25 objects (e.g., \"woman\", \"apple\", etc.) 5 among the 100 most frequent objects in Visual Genome and take all the instances (\u223c130K) that contain any of these words. For example, since \"apple\" is in our list, e.g., (cat, sniffing, apple) is kept. ", "label": "We consider the following subsets of the Visual Genome data to evaluate performance. (i) Raw data: Simply the unfiltered instances from the Visual Genome data (Sect. 4.1). This set contains a substantial proportion of meaningless (e.g., (nose, almost, touching)) and irrelevant (e.g., (sign, says, gate 2)) instances. (ii) Generalized Triplets: We pick at random 100 combinations (S, R, O) among the 1,000 most frequent implicit combinations in Visual Genome. This yields \u223c25K instances such as (person, holding, racket), (man, flying, kite), etc. 4 (iii) Generalized Words: We randomly choose 25 objects (e.g., \"woman\", \"apple\", etc.) 5 among the 100 most frequent objects in Visual Genome and take all the instances (\u223c130K) that contain any of these words. For example, since \"apple\" is in our list, e.g., (cat, sniffing, apple) is kept. ", "shape": "textbox", "title": "We consider the following subsets of the Visual Genome data to evaluate performance. (i) Raw data: Simply the unfiltered instances from the Visual Genome data (Sect. 4.1). This set contains a substantial proportion of meaningless (e.g., (nose, almost, touching)) and irrelevant (e.g., (sign, says, gate 2)) instances. (ii) Generalized Triplets: We pick at random 100 combinations (S, R, O) among the 1,000 most frequent implicit combinations in Visual Genome. This yields \u223c25K instances such as (person, holding, racket), (man, flying, kite), etc. 4 (iii) Generalized Words: We randomly choose 25 objects (e.g., \"woman\", \"apple\", etc.) 5 among the 100 most frequent objects in Visual Genome and take all the instances (\u223c130K) that contain any of these words. For example, since \"apple\" is in our list, e.g., (cat, sniffing, apple) is kept. ", "value": 15000}, {"id": "When enforcing generalization conditions in our experiments, all combinations from sets (ii) and (iii) are removed from the training data to prevent the model from seeing them. 7 Even without imposing generalization conditions (or when testing with Raw data), reported results are always on unseen instances-yet the combinations (S, R, O) may have been seen during training (e.g., in different images). All sets above contain exclusively implicit spatial language, although an analogous version of the Raw set where R are explicit spatial prepositions is also considered in our experiments.", "label": "When enforcing generalization conditions in our experiments, all combinations from sets (ii) and (iii) are removed from the training data to prevent the model from seeing them. 7 Even without imposing generalization conditions (or when testing with Raw data), reported results are always on unseen instances-yet the combinations (S, R, O) may have been seen during training (e.g., in different images). All sets above contain exclusively implicit spatial language, although an analogous version of the Raw set where R are explicit spatial prepositions is also considered in our experiments.", "shape": "textbox", "title": "When enforcing generalization conditions in our experiments, all combinations from sets (ii) and (iii) are removed from the training data to prevent the model from seeing them. 7 Even without imposing generalization conditions (or when testing with Raw data), reported results are always on unseen instances-yet the combinations (S, R, O) may have been seen during training (e.g., in different images). All sets above contain exclusively implicit spatial language, although an analogous version of the Raw set where R are explicit spatial prepositions is also considered in our experiments.", "value": 15000}, {"id": "Data pre-processing", "label": "Data pre-processing", "shape": "ellipse", "title": "Data pre-processing", "value": 30000}, {"id": "The coordinates of bounding boxes in the images are normalized by the width and height of the image. Thus, S c , O c \u2208 [0, 1] 2 . Additionally, we notice that the distinction between left and right is arbitrary regarding the semantics of the image ", "label": "The coordinates of bounding boxes in the images are normalized by the width and height of the image. Thus, S c , O c \u2208 [0, 1] 2 . Additionally, we notice that the distinction between left and right is arbitrary regarding the semantics of the image ", "shape": "textbox", "title": "The coordinates of bounding boxes in the images are normalized by the width and height of the image. Thus, S c , O c \u2208 [0, 1] 2 . Additionally, we notice that the distinction between left and right is arbitrary regarding the semantics of the image ", "value": 19000}, {"id": "Evaluation metrics", "label": "Evaluation metrics", "shape": "ellipse", "title": "Evaluation metrics", "value": 30000}, {"id": "The REG model directly outputs Object coordinates, while PIX outputs 2D heatmaps. We however enable evaluating the PIX model with regression/classification metrics by taking the point of maximum activation (or their average, if there are many) as the Object center O c . The predicted Object size O b is not estimated. We use the following performance metrics.", "label": "The REG model directly outputs Object coordinates, while PIX outputs 2D heatmaps. We however enable evaluating the PIX model with regression/classification metrics by taking the point of maximum activation (or their average, if there are many) as the Object center O c . The predicted Object size O b is not estimated. We use the following performance metrics.", "shape": "textbox", "title": "The REG model directly outputs Object coordinates, while PIX outputs 2D heatmaps. We however enable evaluating the PIX model with regression/classification metrics by taking the point of maximum activation (or their average, if there are many) as the Object center O c . The predicted Object size O b is not estimated. We use the following performance metrics.", "value": 18000}, {"id": "(A) Intersection over Union ", "label": "(A) Intersection over Union ", "shape": "textbox", "title": "(A) Intersection over Union ", "value": 18000}, {"id": "where B O and B O are the predicted and ground truth Object bounding boxes, respectively. If the IoU is larger than 50%, the prediction is counted as correct. It must be noted that our setting is not comparable to object detection (nor our results) since we do not employ the image as input (but text) and thus we cannot leverage the pixels to predict the Object\u0027s location.", "label": "where B O and B O are the predicted and ground truth Object bounding boxes, respectively. If the IoU is larger than 50%, the prediction is counted as correct. It must be noted that our setting is not comparable to object detection (nor our results) since we do not employ the image as input (but text) and thus we cannot leverage the pixels to predict the Object\u0027s location.", "shape": "textbox", "title": "where B O and B O are the predicted and ground truth Object bounding boxes, respectively. If the IoU is larger than 50%, the prediction is counted as correct. It must be noted that our setting is not comparable to object detection (nor our results) since we do not employ the image as input (but text) and thus we cannot leverage the pixels to predict the Object\u0027s location.", "value": 18000}, {"id": "(B) Regression. We consider standard regression metrics. ", "label": "(B) Regression. We consider standard regression metrics. ", "shape": "textbox", "title": "(B) Regression. We consider standard regression metrics. ", "value": 18000}, {"id": "(D) Pixel (macro) accuracy. The IoU on pixels is equivalent to binary pixel accuracy. However, this is not a good measure here, where class 1 (Object box) comprises, on average, only 5% of the pixels. Thus, a constant prediction of zeros everywhere would obtain 95% accuracy. Hence, we consider macro-averaged pixel accuracy, a.k.a. mean IoU (mIoU) ", "label": "(D) Pixel (macro) accuracy. The IoU on pixels is equivalent to binary pixel accuracy. However, this is not a good measure here, where class 1 (Object box) comprises, on average, only 5% of the pixels. Thus, a constant prediction of zeros everywhere would obtain 95% accuracy. Hence, we consider macro-averaged pixel accuracy, a.k.a. mean IoU (mIoU) ", "shape": "textbox", "title": "(D) Pixel (macro) accuracy. The IoU on pixels is equivalent to binary pixel accuracy. However, this is not a good measure here, where class 1 (Object box) comprises, on average, only 5% of the pixels. Thus, a constant prediction of zeros everywhere would obtain 95% accuracy. Hence, we consider macro-averaged pixel accuracy, a.k.a. mean IoU (mIoU) ", "value": 18000}, {"id": "Word embeddings", "label": "Word embeddings", "shape": "ellipse", "title": "Word embeddings", "value": 30000}, {"id": "We use 300-dimensional GloVe word embeddings (Pennington, Socher, and Manning 2014) pre-trained on the Common Crawl corpus (consisting of 840B-tokens), which we obtain from the authors\u0027 website. 8", "label": "We use 300-dimensional GloVe word embeddings (Pennington, Socher, and Manning 2014) pre-trained on the Common Crawl corpus (consisting of 840B-tokens), which we obtain from the authors\u0027 website. 8", "shape": "textbox", "title": "We use 300-dimensional GloVe word embeddings (Pennington, Socher, and Manning 2014) pre-trained on the Common Crawl corpus (consisting of 840B-tokens), which we obtain from the authors\u0027 website. 8", "value": 15000}, {"id": "Model hyperparameters and implementation", "label": "Model hyperparameters and implementation", "shape": "ellipse", "title": "Model hyperparameters and implementation", "value": 30000}, {"id": "Our experiments are implemented in Python 2.7 and we use Keras deep learning framework for our models ", "label": "Our experiments are implemented in Python 2.7 and we use Keras deep learning framework for our models ", "shape": "textbox", "title": "Our experiments are implemented in Python 2.7 and we use Keras deep learning framework for our models ", "value": 40000}, {"id": "Results and discussion", "label": "Results and discussion", "shape": "ellipse", "title": "Results and discussion", "value": 30000}, {"id": "We consider the evaluation sets from Sect. 4.2 and the following variations of PIX and REG models (Sect. 3.2). The subindex EMB denotes a model that employs GloVe embeddings and RND a model with embeddings randomly drawn from a dimension-wise normal distribution of mean (\u03bc) and standard deviation (\u03c3) equal to those of the GloVe embeddings, preserving the original dimensionality (d=300). A third type employs one-hot vectors (1H). We additionally  consider a control method (ctrl) that outputs random normal predictions of \u03bc and \u03c3 equal to the dimension-wise mean and standard deviation of the training targets. We test statistical significance with a Friedman rank test and post hoc Nemenyi tests on the results of the 10 folds. We indicate with an asterisk * in the tables when a method is significantly better than the rest (p \u003c 0.01) within the same model (PIX or REG). ", "label": "We consider the evaluation sets from Sect. 4.2 and the following variations of PIX and REG models (Sect. 3.2). The subindex EMB denotes a model that employs GloVe embeddings and RND a model with embeddings randomly drawn from a dimension-wise normal distribution of mean (\u03bc) and standard deviation (\u03c3) equal to those of the GloVe embeddings, preserving the original dimensionality (d=300). A third type employs one-hot vectors (1H). We additionally  consider a control method (ctrl) that outputs random normal predictions of \u03bc and \u03c3 equal to the dimension-wise mean and standard deviation of the training targets. We test statistical significance with a Friedman rank test and post hoc Nemenyi tests on the results of the 10 folds. We indicate with an asterisk * in the tables when a method is significantly better than the rest (p \u003c 0.01) within the same model (PIX or REG). ", "shape": "textbox", "title": "We consider the evaluation sets from Sect. 4.2 and the following variations of PIX and REG models (Sect. 3.2). The subindex EMB denotes a model that employs GloVe embeddings and RND a model with embeddings randomly drawn from a dimension-wise normal distribution of mean (\u03bc) and standard deviation (\u03c3) equal to those of the GloVe embeddings, preserving the original dimensionality (d=300). A third type employs one-hot vectors (1H). We additionally  consider a control method (ctrl) that outputs random normal predictions of \u03bc and \u03c3 equal to the dimension-wise mean and standard deviation of the training targets. We test statistical significance with a Friedman rank test and post hoc Nemenyi tests on the results of the 10 folds. We indicate with an asterisk * in the tables when a method is significantly better than the rest (p \u003c 0.01) within the same model (PIX or REG). ", "value": 22000}, {"id": "Evaluation with raw data", "label": "Evaluation with raw data", "shape": "ellipse", "title": "Evaluation with raw data", "value": 30000}, {"id": "Generalized evaluations", "label": "Generalized evaluations", "shape": "ellipse", "title": "Generalized evaluations", "value": 30000}, {"id": "Qualitative evaluation (spatial templates)", "label": "Qualitative evaluation (spatial templates)", "shape": "ellipse", "title": "Qualitative evaluation (spatial templates)", "value": 30000}, {"id": "To ensure that model predictions are meaningful and interpretable, we further validate the quantitative results above with a qualitative evaluation. Notice that all plots in ", "label": "To ensure that model predictions are meaningful and interpretable, we further validate the quantitative results above with a qualitative evaluation. Notice that all plots in ", "shape": "textbox", "title": "To ensure that model predictions are meaningful and interpretable, we further validate the quantitative results above with a qualitative evaluation. Notice that all plots in ", "value": 42000}, {"id": "Interpretation of model weights", "label": "Interpretation of model weights", "shape": "ellipse", "title": "Interpretation of model weights", "value": 30000}, {"id": "We study how the weights of the model provide insight into the spatial properties of words. To obtain more interpretable weights, we learn a REG 9 model without hidden layers, resulting in only an embedding layer followed by a linear output layer\u0177 = W out u + b out , where", "label": "We study how the weights of the model provide insight into the spatial properties of words. To obtain more interpretable weights, we learn a REG 9 model without hidden layers, resulting in only an embedding layer followed by a linear output layer\u0177 = W out u + b out , where", "shape": "textbox", "title": "We study how the weights of the model provide insight into the spatial properties of words. To obtain more interpretable weights, we learn a REG 9 model without hidden layers, resulting in only an embedding layer followed by a linear output layer\u0177 = W out u + b out , where", "value": 31000}, {"id": "By using one-hot encodings w S , w R , w O , the concatenation layer u becomes of size |V S | + |V R | + |V O | + 2 + 2. E.g., if \"wearing\" has one-hot index j in the Relationships\u0027 vocabulary (V R ), its index in the concatenation layer is |V S | + j. The product W out u is a 4-dimensional vector, where its i-th component is the product of the i-th row of W out with the vector u. Thus, the component |V S |+j of the i-th row of W out gives us the influence of the j-th relationship (i.e., \"wearing\") on the i-th dimension of the output\u0177 =   ", "label": "By using one-hot encodings w S , w R , w O , the concatenation layer u becomes of size |V S | + |V R | + |V O | + 2 + 2. E.g., if \"wearing\" has one-hot index j in the Relationships\u0027 vocabulary (V R ), its index in the concatenation layer is |V S | + j. The product W out u is a 4-dimensional vector, where its i-th component is the product of the i-th row of W out with the vector u. Thus, the component |V S |+j of the i-th row of W out gives us the influence of the j-th relationship (i.e., \"wearing\") on the i-th dimension of the output\u0177 =   ", "shape": "textbox", "title": "By using one-hot encodings w S , w R , w O , the concatenation layer u becomes of size |V S | + |V R | + |V O | + 2 + 2. E.g., if \"wearing\" has one-hot index j in the Relationships\u0027 vocabulary (V R ), its index in the concatenation layer is |V S | + j. The product W out u is a 4-dimensional vector, where its i-th component is the product of the i-th row of W out with the vector u. Thus, the component |V S |+j of the i-th row of W out gives us the influence of the j-th relationship (i.e., \"wearing\") on the i-th dimension of the output\u0177 =   ", "value": 31000}, {"id": "Conclusions", "label": "Conclusions", "shape": "ellipse", "title": "Conclusions", "value": 30000}, {"id": "Overall, this paper provides insight into the fundamental differences between implicit and explicit spatial language and extends the concept of spatial templates to implicit spatial language, the understanding of which requires common sense spatial knowledge about objects and actions. We define the task of predicting relative spatial arrangements of two objects under a relationship and present two embedding-based neural models that attain promising performance, proving that spatial templates can be accurately predicted from implicit spatial language. Remarkably, our models generalize well, predicting correctly unseen (generalized) object-relationshipobject combinations. Furthermore, the acquired common sense spatial knowledge-aided with word embeddingsallows the model to correctly predict templates for unseen words. Finally, we show that the weights of the model provide great insight into the spatial connotations of words.", "label": "Overall, this paper provides insight into the fundamental differences between implicit and explicit spatial language and extends the concept of spatial templates to implicit spatial language, the understanding of which requires common sense spatial knowledge about objects and actions. We define the task of predicting relative spatial arrangements of two objects under a relationship and present two embedding-based neural models that attain promising performance, proving that spatial templates can be accurately predicted from implicit spatial language. Remarkably, our models generalize well, predicting correctly unseen (generalized) object-relationshipobject combinations. Furthermore, the acquired common sense spatial knowledge-aided with word embeddingsallows the model to correctly predict templates for unseen words. Finally, we show that the weights of the model provide great insight into the spatial connotations of words.", "shape": "textbox", "title": "Overall, this paper provides insight into the fundamental differences between implicit and explicit spatial language and extends the concept of spatial templates to implicit spatial language, the understanding of which requires common sense spatial knowledge about objects and actions. We define the task of predicting relative spatial arrangements of two objects under a relationship and present two embedding-based neural models that attain promising performance, proving that spatial templates can be accurately predicted from implicit spatial language. Remarkably, our models generalize well, predicting correctly unseen (generalized) object-relationshipobject combinations. Furthermore, the acquired common sense spatial knowledge-aided with word embeddingsallows the model to correctly predict templates for unseen words. Finally, we show that the weights of the model provide great insight into the spatial connotations of words.", "value": 11000}, {"id": "A first limitation of our approach is the fully supervised setting where the models are trained using images with detected ground truth objects and parsed text-which aims at keeping the design clean in this first study on implicit spatial templates. Notice however that methods to automatically parse images and text exist. In future work, we aim at implementing our approach in a weakly supervised setting. A second limitation is the 2D spatial treatment of the actual 3D world. It is worth noting however, that our models (PIX and REG) and setting trivially generalize to 3D if appropriate data are available.", "label": "A first limitation of our approach is the fully supervised setting where the models are trained using images with detected ground truth objects and parsed text-which aims at keeping the design clean in this first study on implicit spatial templates. Notice however that methods to automatically parse images and text exist. In future work, we aim at implementing our approach in a weakly supervised setting. A second limitation is the 2D spatial treatment of the actual 3D world. It is worth noting however, that our models (PIX and REG) and setting trivially generalize to 3D if appropriate data are available.", "shape": "textbox", "title": "A first limitation of our approach is the fully supervised setting where the models are trained using images with detected ground truth objects and parsed text-which aims at keeping the design clean in this first study on implicit spatial templates. Notice however that methods to automatically parse images and text exist. In future work, we aim at implementing our approach in a weakly supervised setting. A second limitation is the 2D spatial treatment of the actual 3D world. It is worth noting however, that our models (PIX and REG) and setting trivially generalize to 3D if appropriate data are available.", "value": 11000}]);
        edges = new vis.DataSet([{"arrow": true, "from": "Your Visual Research Paper", "length": 3000, "to": "Abstract", "weight": 0.97}, {"arrow": true, "from": "Abstract", "length": 5000, "to": "Spatial understanding is a fundamental problem with widereaching real-world applications. The representation of spatial knowledge is often modeled with spatial templates, i.e., regions of acceptability of two objects under an explicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with prior work that restricts spatial templates to explicit spatial prepositions (e.g., \"glass on table\"), here we extend this concept to implicit spatial language, i.e., those relationships (generally actions) for which the spatial arrangement of the objects is only implicitly implied (e.g., \"man riding horse\"). In contrast with explicit relationships, predicting spatial arrangements from implicit spatial language requires significant common sense spatial understanding. Here, we introduce the task of predicting spatial templates for two objects under a relationship, which can be seen as a spatial question-answering task with a (2D) continuous output (\"where is the man w.r.t. a horse when the man is walking the horse?\"). We present two simple neural-based models that leverage annotated images and structured text to learn this task. The good performance of these models reveals that spatial locations are to a large extent predictable from implicit spatial language. Crucially, the models attain similar performance in a challenging generalized setting, where the object-relation-object combinations (e.g.,\"man walking dog\") have never been seen before. Next, we go one step further by presenting the models with unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging word embeddings enables the models to output accurate spatial predictions, proving that the models acquire solid common sense spatial knowledge allowing for such generalization.", "weight": 0.87}, {"arrow": true, "from": "Your Visual Research Paper", "length": 3000, "to": "Introduction", "weight": 0.97}, {"arrow": true, "from": "Introduction", "length": 5000, "to": "To provide machines with common sense is one of the major long term goals of artificial intelligence. Common sense knowledge regards knowledge that humans have acquired through a lifetime of experiences. It is crucial in language understanding because a lot of content needed for correct understanding is not expressed explicitly but resides in the mind of communicator and audience. In addition, humans rely on their common sense knowledge when performing a variety of tasks including interpreting images, navigation and reasoning, to name a few. Representing and understanding spatial knowledge are in fact imperative for any agent (human, animal or robot) that navigates in a physical world. In this paper, we are interested in acquiring spatial commonsense knowledge from language paired with visual data.", "weight": 0.87}, {"arrow": true, "from": "Introduction", "length": 5000, "to": "Computational and cognitive models often handle spatial representations as spatial templates or regions of acceptability for two objects under an explicit (a.k.a. deictic) spatial preposition such as \"on\", \"below\" or \"left\" ", "weight": 0.87}, {"arrow": true, "from": "Introduction", "length": 5000, "to": "Predicting spatial templates for implicit relationships is notably more challenging than for explicit relationships. Firstly, whereas there are only a few tens of explicit spatial prepositions, there exist thousands of actions, entailing thus a drastic increase in the sparsity of (object 1 , relationship, object 2 ) combinations. Secondly, the complexity of the task radically increases in implicit language. More precisely, while explicit spatial prepositions 1 are highly deterministic about the spatial arrangements (e.g., (object 1 , below, object 2 ) unequivocally implies that object 1 is relatively lower than object 2 ), actions generally are not. E.g., the relative spatial configuration of \"man\" and the object is clearly distinct in (man, pulling, kite) than in (man, pulling, luggage) yet the action is the same. Contrarily, other relationships such as \"jumping\" are highly informative about the spatial template, i.e., in (object 1 , jumping, object 2 ), object 2 is in a lower position than object 1 . Hence, unlike explicit relationships, predicting spatial layouts from implicit spatial language requires spatial common sense knowledge about the objects, actions and their interaction, which suggests the need of learning to compose the triplet (Subject, Relationship, Object) as a whole instead of learning a template for each Relationship.", "weight": 0.87}, {"arrow": true, "from": "Introduction", "length": 5000, "to": "To systematically study these questions, we propose the task of predicting the relative spatial locations of two objects given a structured text input (Subject, Relationship, Object). We introduce two simple neural-based models trained from annotated images that successfully address the two challenges of implicit spatial language discussed above. Our quantitative evaluation reveals that spatial templates can be reliably predicted from implicit spatial language-as accurately as from explicit spatial language. We also show that our models generalize well to templates of unseen combinations, e.g., predicting (man, riding, elephant) without having been exposed to such scene before, tackling thus the challenge of sparsity. Furthermore, by leveraging word embeddings, the models can correctly generalize to spatial templates with unseen words, e.g., predicting (man, riding, elephant) without having ever seen an \"elephant\" before. Since word embeddings capture attributes of objects ", "weight": 0.87}, {"arrow": true, "from": "Introduction", "length": 5000, "to": "The rest of the paper is organized as follows. In Sect. 2 we review related research. In Sect. 3 we first introduce the task of predicting spatial templates and then present two simple neural models. Then, in Sect. 4, we describe our experimental setup. In Sect. 5 we present and discuss our results. Finally, in Sect. 6 we summarize the contributions of this article.", "weight": 0.87}, {"arrow": true, "from": "Your Visual Research Paper", "length": 3000, "to": "Related work", "weight": 0.97}, {"arrow": true, "from": "Related work", "length": 5000, "to": "Spatial processing has drawn significant attention from the cognitive ", "weight": 0.87}, {"arrow": true, "from": "Related work", "length": 5000, "to": "Spatial templates. Earlier approaches have predominantly considered rule-based spatial representations ", "weight": 0.87}, {"arrow": true, "from": "Related work", "length": 5000, "to": "Additionally, while they build a spatial template for each (explicit) Relationship, we build a template for each (Subject, Relationship, Object) combination, allowing the template to be determined by the interaction/composition of the Subject, Relationship and Object instead of the Relationship alone. Additionally, the model from ", "weight": 0.87}, {"arrow": true, "from": "Related work", "length": 5000, "to": "Leveraging spatial knowledge in tasks. It has been shown that knowledge of the spatial structure in images improves the task of image captioning ", "weight": 0.87}, {"arrow": true, "from": "Related work", "length": 5000, "to": "Common sense spatial knowledge. ", "weight": 0.87}, {"arrow": true, "from": "Related work", "length": 5000, "to": "Image generation. Although models that generate images from text exist (e.g., DRAW model ", "weight": 0.87}, {"arrow": true, "from": "Related work", "length": 5000, "to": "As discussed, spatial knowledge can improve a wide range of tasks ", "weight": 0.87}, {"arrow": true, "from": "Related work", "length": 5000, "to": "Obj.", "weight": 0.87}, {"arrow": true, "from": "Related work", "length": 5000, "to": "Subj.", "weight": 0.87}, {"arrow": true, "from": "Related work", "length": 5000, "to": "x y Elliott and Keller 2013). This suggests that the predictions of our models can be used as spatial common sense input for methods that rely on good spatial priors. Additionally, existing methods require the spatial information to be present in the data and lack the capacity to extrapolate/generalize. Thus, in this paper we focus, first, on showing that generalizing spatial arrangements to unseen objects and object-relation-object combinations is possible and, second, on ensuring that the predicted templates are accurate by performing several quantitative and qualitative evaluations.", "weight": 0.87}, {"arrow": true, "from": "Related work", "length": 5000, "to": "3 Proposed task and model", "weight": 0.87}, {"arrow": true, "from": "Your Visual Research Paper", "length": 3000, "to": "Proposed task", "weight": 0.97}, {"arrow": true, "from": "Proposed task", "length": 5000, "to": "To learn spatial templates, we propose the task of predicting the 2D relative spatial arrangement of two objects under a relationship given a structured text input of the form (Subject, Relationship, Object)-henceforth abbreviated as (S, R, O). Let us denote the 2D coordinates of the center (\"c\") of the Object\u0027s box as", "weight": 0.87}, {"arrow": true, "from": "Proposed task", "length": 5000, "to": "y \u2208 R are the horizontal and vertical components respectively. Let", "weight": 0.87}, {"arrow": true, "from": "Proposed task", "length": 5000, "to": "x , O b y ] \u2208 R 2 be half of the width (O b x ) and half of the height (O b y ) of the Object\u0027s box (\"b\"). We employ a similar notation for the Subject (S c , S b ), and model predictions are denoted with a hat O c , O b . The task consists in predicting the Object\u0027s location and size", "weight": 0.87}, {"arrow": true, "from": "Proposed task", "length": 5000, "to": "given the structured text input (S, R, O) and the location S c and size S b of the Subject ", "weight": 0.87}, {"arrow": true, "from": "Proposed task", "length": 5000, "to": "2 Crucially, we notice that knowing the Subject\u0027s coordinates is not a requirement for generating templates (but only for evaluating against the ground truth) since inputting arbitrary coordinates (e.g., S c =[0.5, 0.5]) still enables visualizing relative object locations. Additionally, we input the Subject\u0027s size in order to provide a reference size to the model. However, we find that without this input, the model still learns to predict an \"average size\" for each Object.", "weight": 0.87}, {"arrow": true, "from": "Proposed task", "length": 5000, "to": "Hence, the goal is to answer common sense spatial questions such as \"if a man is feeding a horse, where is the man relative to the horse?\" or \"where would a child wear her shoes?\"", "weight": 0.87}, {"arrow": true, "from": "Your Visual Research Paper", "length": 3000, "to": "Proposed models", "weight": 0.97}, {"arrow": true, "from": "Proposed models", "length": 5000, "to": "To build a mapping from the input to the output in our task (Sect. 3.1) we propose two simple neural models ( ", "weight": 0.87}, {"arrow": true, "from": "Proposed models", "length": 5000, "to": "are one-hot encodings of S, R, O (a.k.a. one-of-k encoding, i.e., a sparse vector with 0 everywhere except for a 1 at the position of the k-th word) and", "weight": 0.87}, {"arrow": true, "from": "Proposed models", "length": 5000, "to": "This layer represents objects and relationships as continuous features, enabling thus to introduce external knowledge of unseen objects as features. The embeddings are then concatenated with the Subject center S c and size", "weight": 0.87}, {"arrow": true, "from": "Proposed models", "length": 5000, "to": "which is inputted to a stack of hidden layers that compose S, R and O into a joint hidden representation z h :", "weight": 0.87}, {"arrow": true, "from": "Proposed models", "length": 5000, "to": "where f (\u2022) is an element-wise non-linear function and W h and b h are the weight matrix and bias respectively. 3 (ii) Output and loss. We consider two different possibilities for the output\u0177 that follows immediately after the last layer: ", "weight": 0.87}, {"arrow": true, "from": "Proposed models", "length": 5000, "to": "These models are conceptually different and have different capabilities. While the REG model outputs \"crisp\" pointwise predictions, PIX can model more diffuse spatial templates where the location of the object has more variability, e.g., in (man, flying, kite) the \"kite\" can easily move around. Notice that in contrast with convolutional neural networks (CNNs) our approach does not make use of the image pixels ( ", "weight": 0.87}, {"arrow": true, "from": "Your Visual Research Paper", "length": 3000, "to": "Experimental setup", "weight": 0.97}, {"arrow": true, "from": "Experimental setup", "length": 5000, "to": "We employ a 10-fold cross-validation (CV) setting. Data are randomly split into 10 disjoint parts and 10% is employed for testing and 90% for training, repeating this for each of the 10 folds. Reported results are averages over the 10 folds.", "weight": 0.87}, {"arrow": true, "from": "Your Visual Research Paper", "length": 3000, "to": "Visual Genome data set", "weight": 0.97}, {"arrow": true, "from": "Visual Genome data set", "length": 5000, "to": "We use the Visual Genome dataset ", "weight": 0.87}, {"arrow": true, "from": "Visual Genome data set", "length": 5000, "to": "dog, catches, frisbee boy, feeds, giraffe man, throws, frisbee cat, wears glasses ", "weight": 0.87}, {"arrow": true, "from": "Your Visual Research Paper", "length": 3000, "to": "Evaluation sets", "weight": 0.97}, {"arrow": true, "from": "Evaluation sets", "length": 5000, "to": "We consider the following subsets of the Visual Genome data to evaluate performance. (i) Raw data: Simply the unfiltered instances from the Visual Genome data (Sect. 4.1). This set contains a substantial proportion of meaningless (e.g., (nose, almost, touching)) and irrelevant (e.g., (sign, says, gate 2)) instances. (ii) Generalized Triplets: We pick at random 100 combinations (S, R, O) among the 1,000 most frequent implicit combinations in Visual Genome. This yields \u223c25K instances such as (person, holding, racket), (man, flying, kite), etc. 4 (iii) Generalized Words: We randomly choose 25 objects (e.g., \"woman\", \"apple\", etc.) 5 among the 100 most frequent objects in Visual Genome and take all the instances (\u223c130K) that contain any of these words. For example, since \"apple\" is in our list, e.g., (cat, sniffing, apple) is kept. ", "weight": 0.87}, {"arrow": true, "from": "Evaluation sets", "length": 5000, "to": "When enforcing generalization conditions in our experiments, all combinations from sets (ii) and (iii) are removed from the training data to prevent the model from seeing them. 7 Even without imposing generalization conditions (or when testing with Raw data), reported results are always on unseen instances-yet the combinations (S, R, O) may have been seen during training (e.g., in different images). All sets above contain exclusively implicit spatial language, although an analogous version of the Raw set where R are explicit spatial prepositions is also considered in our experiments.", "weight": 0.87}, {"arrow": true, "from": "Your Visual Research Paper", "length": 3000, "to": "Data pre-processing", "weight": 0.97}, {"arrow": true, "from": "Data pre-processing", "length": 5000, "to": "The coordinates of bounding boxes in the images are normalized by the width and height of the image. Thus, S c , O c \u2208 [0, 1] 2 . Additionally, we notice that the distinction between left and right is arbitrary regarding the semantics of the image ", "weight": 0.87}, {"arrow": true, "from": "Your Visual Research Paper", "length": 3000, "to": "Evaluation metrics", "weight": 0.97}, {"arrow": true, "from": "Evaluation metrics", "length": 5000, "to": "The REG model directly outputs Object coordinates, while PIX outputs 2D heatmaps. We however enable evaluating the PIX model with regression/classification metrics by taking the point of maximum activation (or their average, if there are many) as the Object center O c . The predicted Object size O b is not estimated. We use the following performance metrics.", "weight": 0.87}, {"arrow": true, "from": "Evaluation metrics", "length": 5000, "to": "(A) Intersection over Union ", "weight": 0.87}, {"arrow": true, "from": "Evaluation metrics", "length": 5000, "to": "where B O and B O are the predicted and ground truth Object bounding boxes, respectively. If the IoU is larger than 50%, the prediction is counted as correct. It must be noted that our setting is not comparable to object detection (nor our results) since we do not employ the image as input (but text) and thus we cannot leverage the pixels to predict the Object\u0027s location.", "weight": 0.87}, {"arrow": true, "from": "Evaluation metrics", "length": 5000, "to": "(B) Regression. We consider standard regression metrics. ", "weight": 0.87}, {"arrow": true, "from": "Evaluation metrics", "length": 5000, "to": "(D) Pixel (macro) accuracy. The IoU on pixels is equivalent to binary pixel accuracy. However, this is not a good measure here, where class 1 (Object box) comprises, on average, only 5% of the pixels. Thus, a constant prediction of zeros everywhere would obtain 95% accuracy. Hence, we consider macro-averaged pixel accuracy, a.k.a. mean IoU (mIoU) ", "weight": 0.87}, {"arrow": true, "from": "Your Visual Research Paper", "length": 3000, "to": "Word embeddings", "weight": 0.97}, {"arrow": true, "from": "Word embeddings", "length": 5000, "to": "We use 300-dimensional GloVe word embeddings (Pennington, Socher, and Manning 2014) pre-trained on the Common Crawl corpus (consisting of 840B-tokens), which we obtain from the authors\u0027 website. 8", "weight": 0.87}, {"arrow": true, "from": "Your Visual Research Paper", "length": 3000, "to": "Model hyperparameters and implementation", "weight": 0.97}, {"arrow": true, "from": "Model hyperparameters and implementation", "length": 5000, "to": "Our experiments are implemented in Python 2.7 and we use Keras deep learning framework for our models ", "weight": 0.87}, {"arrow": true, "from": "Your Visual Research Paper", "length": 3000, "to": "Results and discussion", "weight": 0.97}, {"arrow": true, "from": "Results and discussion", "length": 5000, "to": "We consider the evaluation sets from Sect. 4.2 and the following variations of PIX and REG models (Sect. 3.2). The subindex EMB denotes a model that employs GloVe embeddings and RND a model with embeddings randomly drawn from a dimension-wise normal distribution of mean (\u03bc) and standard deviation (\u03c3) equal to those of the GloVe embeddings, preserving the original dimensionality (d=300). A third type employs one-hot vectors (1H). We additionally  consider a control method (ctrl) that outputs random normal predictions of \u03bc and \u03c3 equal to the dimension-wise mean and standard deviation of the training targets. We test statistical significance with a Friedman rank test and post hoc Nemenyi tests on the results of the 10 folds. We indicate with an asterisk * in the tables when a method is significantly better than the rest (p \u003c 0.01) within the same model (PIX or REG). ", "weight": 0.87}, {"arrow": true, "from": "Your Visual Research Paper", "length": 3000, "to": "Evaluation with raw data", "weight": 0.97}, {"arrow": true, "from": "Your Visual Research Paper", "length": 3000, "to": "Generalized evaluations", "weight": 0.97}, {"arrow": true, "from": "Your Visual Research Paper", "length": 3000, "to": "Qualitative evaluation (spatial templates)", "weight": 0.97}, {"arrow": true, "from": "Qualitative evaluation (spatial templates)", "length": 5000, "to": "To ensure that model predictions are meaningful and interpretable, we further validate the quantitative results above with a qualitative evaluation. Notice that all plots in ", "weight": 0.87}, {"arrow": true, "from": "Your Visual Research Paper", "length": 3000, "to": "Interpretation of model weights", "weight": 0.97}, {"arrow": true, "from": "Interpretation of model weights", "length": 5000, "to": "We study how the weights of the model provide insight into the spatial properties of words. To obtain more interpretable weights, we learn a REG 9 model without hidden layers, resulting in only an embedding layer followed by a linear output layer\u0177 = W out u + b out , where", "weight": 0.87}, {"arrow": true, "from": "Interpretation of model weights", "length": 5000, "to": "By using one-hot encodings w S , w R , w O , the concatenation layer u becomes of size |V S | + |V R | + |V O | + 2 + 2. E.g., if \"wearing\" has one-hot index j in the Relationships\u0027 vocabulary (V R ), its index in the concatenation layer is |V S | + j. The product W out u is a 4-dimensional vector, where its i-th component is the product of the i-th row of W out with the vector u. Thus, the component |V S |+j of the i-th row of W out gives us the influence of the j-th relationship (i.e., \"wearing\") on the i-th dimension of the output\u0177 =   ", "weight": 0.87}, {"arrow": true, "from": "Your Visual Research Paper", "length": 3000, "to": "Conclusions", "weight": 0.97}, {"arrow": true, "from": "Conclusions", "length": 5000, "to": "Overall, this paper provides insight into the fundamental differences between implicit and explicit spatial language and extends the concept of spatial templates to implicit spatial language, the understanding of which requires common sense spatial knowledge about objects and actions. We define the task of predicting relative spatial arrangements of two objects under a relationship and present two embedding-based neural models that attain promising performance, proving that spatial templates can be accurately predicted from implicit spatial language. Remarkably, our models generalize well, predicting correctly unseen (generalized) object-relationshipobject combinations. Furthermore, the acquired common sense spatial knowledge-aided with word embeddingsallows the model to correctly predict templates for unseen words. Finally, we show that the weights of the model provide great insight into the spatial connotations of words.", "weight": 0.87}, {"arrow": true, "from": "Conclusions", "length": 5000, "to": "A first limitation of our approach is the fully supervised setting where the models are trained using images with detected ground truth objects and parsed text-which aims at keeping the design clean in this first study on implicit spatial templates. Notice however that methods to automatically parse images and text exist. In future work, we aim at implementing our approach in a weakly supervised setting. A second limitation is the 2D spatial treatment of the actual 3D world. It is worth noting however, that our models (PIX and REG) and setting trivially generalize to 3D if appropriate data are available.", "weight": 0.87}]);

        // adding nodes and edges to the graph
        data = {nodes: nodes, edges: edges};

        var options = {
    "configure": {
        "enabled": true,
        "filter": [
            "physics"
        ]
    },
    "edges": {
        "color": {
            "inherit": true
        },
        "smooth": {
            "enabled": false,
            "type": "continuous"
        }
    },
    "interaction": {
        "dragNodes": true,
        "hideEdgesOnDrag": false,
        "hideNodesOnDrag": false
    },
    "physics": {
        "enabled": true,
        "stabilization": {
            "enabled": true,
            "fit": true,
            "iterations": 1000,
            "onlyDynamicEdges": false,
            "updateInterval": 50
        }
    }
};
        
        

        
        // if this network requires displaying the configure window,
        // put it in its div
        options.configure["container"] = document.getElementById("config");
        

        network = new vis.Network(container, data, options);

        


        

        return network;

    }

    drawGraph();

</script>
</body>
</html>