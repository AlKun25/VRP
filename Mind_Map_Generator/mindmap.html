<html>
<head>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis/4.16.1/vis.css" type="text/css" />
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/vis/4.16.1/vis-network.min.js"> </script>
<center>
<h1>None</h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->

<style type="text/css">

        #mynetwork {
            width: 500px;
            height: The Visual Research Paper;
            background-color: #ffffff;
            border: 1px solid lightgray;
            position: relative;
            float: left;
        }

        

        
        #config {
            float: left;
            width: 400px;
            height: 600px;
        }
        

        
</style>

</head>

<body>
<div id = "mynetwork"></div>


<div id = "config"></div>

<script type="text/javascript">

    // initialize global variables.
    var edges;
    var nodes;
    var network; 
    var container;
    var options, data;

    
    // This method is responsible for drawing the graph, returns the drawn network
    function drawGraph() {
        var container = document.getElementById('mynetwork');
        
        

        // parsing and collecting nodes and edges from the python
        nodes = new vis.DataSet([{"color": "ffb3bf", "fill": "pink", "id": "Your Visual Research Paper", "label": "Your Visual Research Paper", "shape": "circle", "title": "Your Visual Research Paper", "value": 5000}, {"color": "b37d8b", "fill": "red", "id": "Abstract", "label": "Abstract", "shape": "ellipse", "title": "Abstract", "value": 3000}, {"color": "ffecf1", "id": "Spatial understanding is a fundamental problem with widereaching real-world applications. The representation of spatial knowledge is often modeled with spatial templates, i.e., regions of acceptability of two objects under an explicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with prior work that restricts spatial templates to explicit spatial prepositions (e.g., \"glass on table\"), here we extend this concept to implicit spatial language, i.e., those relationships (generally actions) for which the spatial arrangement of the objects is only implicitly implied (e.g., \"man riding horse\"). In contrast with explicit relationships, predicting spatial arrangements from implicit spatial language requires significant common sense spatial understanding. Here, we introduce the task of predicting spatial templates for two objects under a relationship, which can be seen as a spatial question-answering task with a (2D) continuous output (\"where is the man w.r.t. a horse when the man is walking the horse?\"). We present two simple neural-based models that leverage annotated images and structured text to learn this task. The good performance of these models reveals that spatial locations are to a large extent predictable from implicit spatial language. Crucially, the models attain similar performance in a challenging generalized setting, where the object-relation-object combinations (e.g.,\"man walking dog\") have never been seen before. Next, we go one step further by presenting the models with unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging word embeddings enables the models to output accurate spatial predictions, proving that the models acquire solid common sense spatial knowledge allowing for such generalization.", "label": "Spatial understanding is a fundamental problem with widereaching real-world applications. The representation of spatial knowledge is often modeled with spatial templates, i.e., regions of acceptability of two objects under an explicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with prior work that restricts spatial templates to explicit spatial prepositions (e.g., \"glass on table\"), here we extend this concept to implicit spatial language, i.e., those relationships (generally actions) for which the spatial arrangement of the objects is only implicitly implied (e.g., \"man riding horse\"). In contrast with explicit relationships, predicting spatial arrangements from implicit spatial language requires significant common sense spatial understanding. Here, we introduce the task of predicting spatial templates for two objects under a relationship, which can be seen as a spatial question-answering task with a (2D) continuous output (\"where is the man w.r.t. a horse when the man is walking the horse?\"). We present two simple neural-based models that leverage annotated images and structured text to learn this task. The good performance of these models reveals that spatial locations are to a large extent predictable from implicit spatial language. Crucially, the models attain similar performance in a challenging generalized setting, where the object-relation-object combinations (e.g.,\"man walking dog\") have never been seen before. Next, we go one step further by presenting the models with unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging word embeddings enables the models to output accurate spatial predictions, proving that the models acquire solid common sense spatial knowledge allowing for such generalization.", "shape": "textbox", "title": "Spatial understanding is a fundamental problem with widereaching real-world applications. The representation of spatial knowledge is often modeled with spatial templates, i.e., regions of acceptability of two objects under an explicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with prior work that restricts spatial templates to explicit spatial prepositions (e.g., \"glass on table\"), here we extend this concept to implicit spatial language, i.e., those relationships (generally actions) for which the spatial arrangement of the objects is only implicitly implied (e.g., \"man riding horse\"). In contrast with explicit relationships, predicting spatial arrangements from implicit spatial language requires significant common sense spatial understanding. Here, we introduce the task of predicting spatial templates for two objects under a relationship, which can be seen as a spatial question-answering task with a (2D) continuous output (\"where is the man w.r.t. a horse when the man is walking the horse?\"). We present two simple neural-based models that leverage annotated images and structured text to learn this task. The good performance of these models reveals that spatial locations are to a large extent predictable from implicit spatial language. Crucially, the models attain similar performance in a challenging generalized setting, where the object-relation-object combinations (e.g.,\"man walking dog\") have never been seen before. Next, we go one step further by presenting the models with unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging word embeddings enables the models to output accurate spatial predictions, proving that the models acquire solid common sense spatial knowledge allowing for such generalization.", "value": 800}, {"color": "ffecf1", "id": "To provide machines with common sense is one of the major long term goals of artificial intelligence. Common sense knowledge regards knowledge that humans have acquired through a lifetime of experiences. It is crucial in language understanding because a lot of content needed for correct understanding is not expressed explicitly but resides in the mind of communicator and audience. In addition, humans rely on their common sense knowledge when performing a variety of tasks including interpreting images, navigation and reasoning, to name a few. Representing and understanding spatial knowledge are in fact imperative for any agent (human, animal or robot) that navigates in a physical world. In this paper, we are interested in acquiring spatial commonsense knowledge from language paired with visual data.", "label": "To provide machines with common sense is one of the major long term goals of artificial intelligence. Common sense knowledge regards knowledge that humans have acquired through a lifetime of experiences. It is crucial in language understanding because a lot of content needed for correct understanding is not expressed explicitly but resides in the mind of communicator and audience. In addition, humans rely on their common sense knowledge when performing a variety of tasks including interpreting images, navigation and reasoning, to name a few. Representing and understanding spatial knowledge are in fact imperative for any agent (human, animal or robot) that navigates in a physical world. In this paper, we are interested in acquiring spatial commonsense knowledge from language paired with visual data.", "shape": "textbox", "title": "To provide machines with common sense is one of the major long term goals of artificial intelligence. Common sense knowledge regards knowledge that humans have acquired through a lifetime of experiences. It is crucial in language understanding because a lot of content needed for correct understanding is not expressed explicitly but resides in the mind of communicator and audience. In addition, humans rely on their common sense knowledge when performing a variety of tasks including interpreting images, navigation and reasoning, to name a few. Representing and understanding spatial knowledge are in fact imperative for any agent (human, animal or robot) that navigates in a physical world. In this paper, we are interested in acquiring spatial commonsense knowledge from language paired with visual data.", "value": 800}, {"color": "ffecf1", "id": "Computational and cognitive models often handle spatial representations as spatial templates or regions of acceptability for two objects under an explicit (a.k.a. deictic) spatial preposition such as \"on\", \"below\" or \"left\" ", "label": "Computational and cognitive models often handle spatial representations as spatial templates or regions of acceptability for two objects under an explicit (a.k.a. deictic) spatial preposition such as \"on\", \"below\" or \"left\" ", "shape": "textbox", "title": "Computational and cognitive models often handle spatial representations as spatial templates or regions of acceptability for two objects under an explicit (a.k.a. deictic) spatial preposition such as \"on\", \"below\" or \"left\" ", "value": 800}, {"color": "ffecf1", "id": "Predicting spatial templates for implicit relationships is notably more challenging than for explicit relationships. Firstly, whereas there are only a few tens of explicit spatial prepositions, there exist thousands of actions, entailing thus a drastic increase in the sparsity of (object 1 , relationship, object 2 ) combinations. Secondly, the complexity of the task radically increases in implicit language. More precisely, while explicit spatial prepositions 1 are highly deterministic about the spatial arrangements (e.g., (object 1 , below, object 2 ) unequivocally implies that object 1 is relatively lower than object 2 ), actions generally are not. E.g., the relative spatial configuration of \"man\" and the object is clearly distinct in (man, pulling, kite) than in (man, pulling, luggage) yet the action is the same. Contrarily, other relationships such as \"jumping\" are highly informative about the spatial template, i.e., in (object 1 , jumping, object 2 ), object 2 is in a lower position than object 1 . Hence, unlike explicit relationships, predicting spatial layouts from implicit spatial language requires spatial common sense knowledge about the objects, actions and their interaction, which suggests the need of learning to compose the triplet (Subject, Relationship, Object) as a whole instead of learning a template for each Relationship.", "label": "Predicting spatial templates for implicit relationships is notably more challenging than for explicit relationships. Firstly, whereas there are only a few tens of explicit spatial prepositions, there exist thousands of actions, entailing thus a drastic increase in the sparsity of (object 1 , relationship, object 2 ) combinations. Secondly, the complexity of the task radically increases in implicit language. More precisely, while explicit spatial prepositions 1 are highly deterministic about the spatial arrangements (e.g., (object 1 , below, object 2 ) unequivocally implies that object 1 is relatively lower than object 2 ), actions generally are not. E.g., the relative spatial configuration of \"man\" and the object is clearly distinct in (man, pulling, kite) than in (man, pulling, luggage) yet the action is the same. Contrarily, other relationships such as \"jumping\" are highly informative about the spatial template, i.e., in (object 1 , jumping, object 2 ), object 2 is in a lower position than object 1 . Hence, unlike explicit relationships, predicting spatial layouts from implicit spatial language requires spatial common sense knowledge about the objects, actions and their interaction, which suggests the need of learning to compose the triplet (Subject, Relationship, Object) as a whole instead of learning a template for each Relationship.", "shape": "textbox", "title": "Predicting spatial templates for implicit relationships is notably more challenging than for explicit relationships. Firstly, whereas there are only a few tens of explicit spatial prepositions, there exist thousands of actions, entailing thus a drastic increase in the sparsity of (object 1 , relationship, object 2 ) combinations. Secondly, the complexity of the task radically increases in implicit language. More precisely, while explicit spatial prepositions 1 are highly deterministic about the spatial arrangements (e.g., (object 1 , below, object 2 ) unequivocally implies that object 1 is relatively lower than object 2 ), actions generally are not. E.g., the relative spatial configuration of \"man\" and the object is clearly distinct in (man, pulling, kite) than in (man, pulling, luggage) yet the action is the same. Contrarily, other relationships such as \"jumping\" are highly informative about the spatial template, i.e., in (object 1 , jumping, object 2 ), object 2 is in a lower position than object 1 . Hence, unlike explicit relationships, predicting spatial layouts from implicit spatial language requires spatial common sense knowledge about the objects, actions and their interaction, which suggests the need of learning to compose the triplet (Subject, Relationship, Object) as a whole instead of learning a template for each Relationship.", "value": 800}, {"color": "ffecf1", "id": "To systematically study these questions, we propose the task of predicting the relative spatial locations of two objects given a structured text input (Subject, Relationship, Object). We introduce two simple neural-based models trained from annotated images that successfully address the two challenges of implicit spatial language discussed above. Our quantitative evaluation reveals that spatial templates can be reliably predicted from implicit spatial language-as accurately as from explicit spatial language. We also show that our models generalize well to templates of unseen combinations, e.g., predicting (man, riding, elephant) without having been exposed to such scene before, tackling thus the challenge of sparsity. Furthermore, by leveraging word embeddings, the models can correctly generalize to spatial templates with unseen words, e.g., predicting (man, riding, elephant) without having ever seen an \"elephant\" before. Since word embeddings capture attributes of objects ", "label": "To systematically study these questions, we propose the task of predicting the relative spatial locations of two objects given a structured text input (Subject, Relationship, Object). We introduce two simple neural-based models trained from annotated images that successfully address the two challenges of implicit spatial language discussed above. Our quantitative evaluation reveals that spatial templates can be reliably predicted from implicit spatial language-as accurately as from explicit spatial language. We also show that our models generalize well to templates of unseen combinations, e.g., predicting (man, riding, elephant) without having been exposed to such scene before, tackling thus the challenge of sparsity. Furthermore, by leveraging word embeddings, the models can correctly generalize to spatial templates with unseen words, e.g., predicting (man, riding, elephant) without having ever seen an \"elephant\" before. Since word embeddings capture attributes of objects ", "shape": "textbox", "title": "To systematically study these questions, we propose the task of predicting the relative spatial locations of two objects given a structured text input (Subject, Relationship, Object). We introduce two simple neural-based models trained from annotated images that successfully address the two challenges of implicit spatial language discussed above. Our quantitative evaluation reveals that spatial templates can be reliably predicted from implicit spatial language-as accurately as from explicit spatial language. We also show that our models generalize well to templates of unseen combinations, e.g., predicting (man, riding, elephant) without having been exposed to such scene before, tackling thus the challenge of sparsity. Furthermore, by leveraging word embeddings, the models can correctly generalize to spatial templates with unseen words, e.g., predicting (man, riding, elephant) without having ever seen an \"elephant\" before. Since word embeddings capture attributes of objects ", "value": 800}, {"color": "ffecf1", "id": "The rest of the paper is organized as follows. In Sect. 2 we review related research. In Sect. 3 we first introduce the task of predicting spatial templates and then present two simple neural models. Then, in Sect. 4, we describe our experimental setup. In Sect. 5 we present and discuss our results. Finally, in Sect. 6 we summarize the contributions of this article.", "label": "The rest of the paper is organized as follows. In Sect. 2 we review related research. In Sect. 3 we first introduce the task of predicting spatial templates and then present two simple neural models. Then, in Sect. 4, we describe our experimental setup. In Sect. 5 we present and discuss our results. Finally, in Sect. 6 we summarize the contributions of this article.", "shape": "textbox", "title": "The rest of the paper is organized as follows. In Sect. 2 we review related research. In Sect. 3 we first introduce the task of predicting spatial templates and then present two simple neural models. Then, in Sect. 4, we describe our experimental setup. In Sect. 5 we present and discuss our results. Finally, in Sect. 6 we summarize the contributions of this article.", "value": 800}, {"color": "ffecf1", "id": "Spatial processing has drawn significant attention from the cognitive ", "label": "Spatial processing has drawn significant attention from the cognitive ", "shape": "textbox", "title": "Spatial processing has drawn significant attention from the cognitive ", "value": 800}, {"color": "ffecf1", "id": "Spatial templates. Earlier approaches have predominantly considered rule-based spatial representations ", "label": "Spatial templates. Earlier approaches have predominantly considered rule-based spatial representations ", "shape": "textbox", "title": "Spatial templates. Earlier approaches have predominantly considered rule-based spatial representations ", "value": 800}, {"color": "ffecf1", "id": "Additionally, while they build a spatial template for each (explicit) Relationship, we build a template for each (Subject, Relationship, Object) combination, allowing the template to be determined by the interaction/composition of the Subject, Relationship and Object instead of the Relationship alone. Additionally, the model from ", "label": "Additionally, while they build a spatial template for each (explicit) Relationship, we build a template for each (Subject, Relationship, Object) combination, allowing the template to be determined by the interaction/composition of the Subject, Relationship and Object instead of the Relationship alone. Additionally, the model from ", "shape": "textbox", "title": "Additionally, while they build a spatial template for each (explicit) Relationship, we build a template for each (Subject, Relationship, Object) combination, allowing the template to be determined by the interaction/composition of the Subject, Relationship and Object instead of the Relationship alone. Additionally, the model from ", "value": 800}, {"color": "ffecf1", "id": "Leveraging spatial knowledge in tasks. It has been shown that knowledge of the spatial structure in images improves the task of image captioning ", "label": "Leveraging spatial knowledge in tasks. It has been shown that knowledge of the spatial structure in images improves the task of image captioning ", "shape": "textbox", "title": "Leveraging spatial knowledge in tasks. It has been shown that knowledge of the spatial structure in images improves the task of image captioning ", "value": 800}, {"color": "ffecf1", "id": "Common sense spatial knowledge. ", "label": "Common sense spatial knowledge. ", "shape": "textbox", "title": "Common sense spatial knowledge. ", "value": 800}, {"color": "ffecf1", "id": "Image generation. Although models that generate images from text exist (e.g., DRAW model ", "label": "Image generation. Although models that generate images from text exist (e.g., DRAW model ", "shape": "textbox", "title": "Image generation. Although models that generate images from text exist (e.g., DRAW model ", "value": 800}, {"color": "ffecf1", "id": "As discussed, spatial knowledge can improve a wide range of tasks ", "label": "As discussed, spatial knowledge can improve a wide range of tasks ", "shape": "textbox", "title": "As discussed, spatial knowledge can improve a wide range of tasks ", "value": 800}, {"color": "ffecf1", "id": "Obj.", "label": "Obj.", "shape": "textbox", "title": "Obj.", "value": 800}, {"color": "ffecf1", "id": "Subj.", "label": "Subj.", "shape": "textbox", "title": "Subj.", "value": 800}, {"color": "ffecf1", "id": "x y Elliott and Keller 2013). This suggests that the predictions of our models can be used as spatial common sense input for methods that rely on good spatial priors. Additionally, existing methods require the spatial information to be present in the data and lack the capacity to extrapolate/generalize. Thus, in this paper we focus, first, on showing that generalizing spatial arrangements to unseen objects and object-relation-object combinations is possible and, second, on ensuring that the predicted templates are accurate by performing several quantitative and qualitative evaluations.", "label": "x y Elliott and Keller 2013). This suggests that the predictions of our models can be used as spatial common sense input for methods that rely on good spatial priors. Additionally, existing methods require the spatial information to be present in the data and lack the capacity to extrapolate/generalize. Thus, in this paper we focus, first, on showing that generalizing spatial arrangements to unseen objects and object-relation-object combinations is possible and, second, on ensuring that the predicted templates are accurate by performing several quantitative and qualitative evaluations.", "shape": "textbox", "title": "x y Elliott and Keller 2013). This suggests that the predictions of our models can be used as spatial common sense input for methods that rely on good spatial priors. Additionally, existing methods require the spatial information to be present in the data and lack the capacity to extrapolate/generalize. Thus, in this paper we focus, first, on showing that generalizing spatial arrangements to unseen objects and object-relation-object combinations is possible and, second, on ensuring that the predicted templates are accurate by performing several quantitative and qualitative evaluations.", "value": 800}, {"color": "ffecf1", "id": "3 Proposed task and model", "label": "3 Proposed task and model", "shape": "textbox", "title": "3 Proposed task and model", "value": 800}, {"color": "ffecf1", "id": "To learn spatial templates, we propose the task of predicting the 2D relative spatial arrangement of two objects under a relationship given a structured text input of the form (Subject, Relationship, Object)-henceforth abbreviated as (S, R, O). Let us denote the 2D coordinates of the center (\"c\") of the Object\u0027s box as", "label": "To learn spatial templates, we propose the task of predicting the 2D relative spatial arrangement of two objects under a relationship given a structured text input of the form (Subject, Relationship, Object)-henceforth abbreviated as (S, R, O). Let us denote the 2D coordinates of the center (\"c\") of the Object\u0027s box as", "shape": "textbox", "title": "To learn spatial templates, we propose the task of predicting the 2D relative spatial arrangement of two objects under a relationship given a structured text input of the form (Subject, Relationship, Object)-henceforth abbreviated as (S, R, O). Let us denote the 2D coordinates of the center (\"c\") of the Object\u0027s box as", "value": 800}, {"color": "ffecf1", "id": "y \u2208 R are the horizontal and vertical components respectively. Let", "label": "y \u2208 R are the horizontal and vertical components respectively. Let", "shape": "textbox", "title": "y \u2208 R are the horizontal and vertical components respectively. Let", "value": 800}, {"color": "ffecf1", "id": "x , O b y ] \u2208 R 2 be half of the width (O b x ) and half of the height (O b y ) of the Object\u0027s box (\"b\"). We employ a similar notation for the Subject (S c , S b ), and model predictions are denoted with a hat O c , O b . The task consists in predicting the Object\u0027s location and size", "label": "x , O b y ] \u2208 R 2 be half of the width (O b x ) and half of the height (O b y ) of the Object\u0027s box (\"b\"). We employ a similar notation for the Subject (S c , S b ), and model predictions are denoted with a hat O c , O b . The task consists in predicting the Object\u0027s location and size", "shape": "textbox", "title": "x , O b y ] \u2208 R 2 be half of the width (O b x ) and half of the height (O b y ) of the Object\u0027s box (\"b\"). We employ a similar notation for the Subject (S c , S b ), and model predictions are denoted with a hat O c , O b . The task consists in predicting the Object\u0027s location and size", "value": 800}, {"color": "ffecf1", "id": "given the structured text input (S, R, O) and the location S c and size S b of the Subject ", "label": "given the structured text input (S, R, O) and the location S c and size S b of the Subject ", "shape": "textbox", "title": "given the structured text input (S, R, O) and the location S c and size S b of the Subject ", "value": 800}, {"color": "ffecf1", "id": "2 Crucially, we notice that knowing the Subject\u0027s coordinates is not a requirement for generating templates (but only for evaluating against the ground truth) since inputting arbitrary coordinates (e.g., S c =[0.5, 0.5]) still enables visualizing relative object locations. Additionally, we input the Subject\u0027s size in order to provide a reference size to the model. However, we find that without this input, the model still learns to predict an \"average size\" for each Object.", "label": "2 Crucially, we notice that knowing the Subject\u0027s coordinates is not a requirement for generating templates (but only for evaluating against the ground truth) since inputting arbitrary coordinates (e.g., S c =[0.5, 0.5]) still enables visualizing relative object locations. Additionally, we input the Subject\u0027s size in order to provide a reference size to the model. However, we find that without this input, the model still learns to predict an \"average size\" for each Object.", "shape": "textbox", "title": "2 Crucially, we notice that knowing the Subject\u0027s coordinates is not a requirement for generating templates (but only for evaluating against the ground truth) since inputting arbitrary coordinates (e.g., S c =[0.5, 0.5]) still enables visualizing relative object locations. Additionally, we input the Subject\u0027s size in order to provide a reference size to the model. However, we find that without this input, the model still learns to predict an \"average size\" for each Object.", "value": 800}, {"color": "ffecf1", "id": "Hence, the goal is to answer common sense spatial questions such as \"if a man is feeding a horse, where is the man relative to the horse?\" or \"where would a child wear her shoes?\"", "label": "Hence, the goal is to answer common sense spatial questions such as \"if a man is feeding a horse, where is the man relative to the horse?\" or \"where would a child wear her shoes?\"", "shape": "textbox", "title": "Hence, the goal is to answer common sense spatial questions such as \"if a man is feeding a horse, where is the man relative to the horse?\" or \"where would a child wear her shoes?\"", "value": 800}, {"color": "ffecf1", "id": "To build a mapping from the input to the output in our task (Sect. 3.1) we propose two simple neural models ( ", "label": "To build a mapping from the input to the output in our task (Sect. 3.1) we propose two simple neural models ( ", "shape": "textbox", "title": "To build a mapping from the input to the output in our task (Sect. 3.1) we propose two simple neural models ( ", "value": 800}, {"color": "ffecf1", "id": "are one-hot encodings of S, R, O (a.k.a. one-of-k encoding, i.e., a sparse vector with 0 everywhere except for a 1 at the position of the k-th word) and", "label": "are one-hot encodings of S, R, O (a.k.a. one-of-k encoding, i.e., a sparse vector with 0 everywhere except for a 1 at the position of the k-th word) and", "shape": "textbox", "title": "are one-hot encodings of S, R, O (a.k.a. one-of-k encoding, i.e., a sparse vector with 0 everywhere except for a 1 at the position of the k-th word) and", "value": 800}, {"color": "ffecf1", "id": "This layer represents objects and relationships as continuous features, enabling thus to introduce external knowledge of unseen objects as features. The embeddings are then concatenated with the Subject center S c and size", "label": "This layer represents objects and relationships as continuous features, enabling thus to introduce external knowledge of unseen objects as features. The embeddings are then concatenated with the Subject center S c and size", "shape": "textbox", "title": "This layer represents objects and relationships as continuous features, enabling thus to introduce external knowledge of unseen objects as features. The embeddings are then concatenated with the Subject center S c and size", "value": 800}, {"color": "ffecf1", "id": "which is inputted to a stack of hidden layers that compose S, R and O into a joint hidden representation z h :", "label": "which is inputted to a stack of hidden layers that compose S, R and O into a joint hidden representation z h :", "shape": "textbox", "title": "which is inputted to a stack of hidden layers that compose S, R and O into a joint hidden representation z h :", "value": 800}, {"color": "ffecf1", "id": "where f (\u2022) is an element-wise non-linear function and W h and b h are the weight matrix and bias respectively. 3 (ii) Output and loss. We consider two different possibilities for the output\u0177 that follows immediately after the last layer: ", "label": "where f (\u2022) is an element-wise non-linear function and W h and b h are the weight matrix and bias respectively. 3 (ii) Output and loss. We consider two different possibilities for the output\u0177 that follows immediately after the last layer: ", "shape": "textbox", "title": "where f (\u2022) is an element-wise non-linear function and W h and b h are the weight matrix and bias respectively. 3 (ii) Output and loss. We consider two different possibilities for the output\u0177 that follows immediately after the last layer: ", "value": 800}, {"color": "ffecf1", "id": "These models are conceptually different and have different capabilities. While the REG model outputs \"crisp\" pointwise predictions, PIX can model more diffuse spatial templates where the location of the object has more variability, e.g., in (man, flying, kite) the \"kite\" can easily move around. Notice that in contrast with convolutional neural networks (CNNs) our approach does not make use of the image pixels ( ", "label": "These models are conceptually different and have different capabilities. While the REG model outputs \"crisp\" pointwise predictions, PIX can model more diffuse spatial templates where the location of the object has more variability, e.g., in (man, flying, kite) the \"kite\" can easily move around. Notice that in contrast with convolutional neural networks (CNNs) our approach does not make use of the image pixels ( ", "shape": "textbox", "title": "These models are conceptually different and have different capabilities. While the REG model outputs \"crisp\" pointwise predictions, PIX can model more diffuse spatial templates where the location of the object has more variability, e.g., in (man, flying, kite) the \"kite\" can easily move around. Notice that in contrast with convolutional neural networks (CNNs) our approach does not make use of the image pixels ( ", "value": 800}, {"color": "ffecf1", "id": "We employ a 10-fold cross-validation (CV) setting. Data are randomly split into 10 disjoint parts and 10% is employed for testing and 90% for training, repeating this for each of the 10 folds. Reported results are averages over the 10 folds.", "label": "We employ a 10-fold cross-validation (CV) setting. Data are randomly split into 10 disjoint parts and 10% is employed for testing and 90% for training, repeating this for each of the 10 folds. Reported results are averages over the 10 folds.", "shape": "textbox", "title": "We employ a 10-fold cross-validation (CV) setting. Data are randomly split into 10 disjoint parts and 10% is employed for testing and 90% for training, repeating this for each of the 10 folds. Reported results are averages over the 10 folds.", "value": 800}, {"color": "ffecf1", "id": "We use the Visual Genome dataset ", "label": "We use the Visual Genome dataset ", "shape": "textbox", "title": "We use the Visual Genome dataset ", "value": 800}, {"color": "ffecf1", "id": "dog, catches, frisbee boy, feeds, giraffe man, throws, frisbee cat, wears glasses ", "label": "dog, catches, frisbee boy, feeds, giraffe man, throws, frisbee cat, wears glasses ", "shape": "textbox", "title": "dog, catches, frisbee boy, feeds, giraffe man, throws, frisbee cat, wears glasses ", "value": 800}, {"color": "ffecf1", "id": "We consider the following subsets of the Visual Genome data to evaluate performance. (i) Raw data: Simply the unfiltered instances from the Visual Genome data (Sect. 4.1). This set contains a substantial proportion of meaningless (e.g., (nose, almost, touching)) and irrelevant (e.g., (sign, says, gate 2)) instances. (ii) Generalized Triplets: We pick at random 100 combinations (S, R, O) among the 1,000 most frequent implicit combinations in Visual Genome. This yields \u223c25K instances such as (person, holding, racket), (man, flying, kite), etc. 4 (iii) Generalized Words: We randomly choose 25 objects (e.g., \"woman\", \"apple\", etc.) 5 among the 100 most frequent objects in Visual Genome and take all the instances (\u223c130K) that contain any of these words. For example, since \"apple\" is in our list, e.g., (cat, sniffing, apple) is kept. ", "label": "We consider the following subsets of the Visual Genome data to evaluate performance. (i) Raw data: Simply the unfiltered instances from the Visual Genome data (Sect. 4.1). This set contains a substantial proportion of meaningless (e.g., (nose, almost, touching)) and irrelevant (e.g., (sign, says, gate 2)) instances. (ii) Generalized Triplets: We pick at random 100 combinations (S, R, O) among the 1,000 most frequent implicit combinations in Visual Genome. This yields \u223c25K instances such as (person, holding, racket), (man, flying, kite), etc. 4 (iii) Generalized Words: We randomly choose 25 objects (e.g., \"woman\", \"apple\", etc.) 5 among the 100 most frequent objects in Visual Genome and take all the instances (\u223c130K) that contain any of these words. For example, since \"apple\" is in our list, e.g., (cat, sniffing, apple) is kept. ", "shape": "textbox", "title": "We consider the following subsets of the Visual Genome data to evaluate performance. (i) Raw data: Simply the unfiltered instances from the Visual Genome data (Sect. 4.1). This set contains a substantial proportion of meaningless (e.g., (nose, almost, touching)) and irrelevant (e.g., (sign, says, gate 2)) instances. (ii) Generalized Triplets: We pick at random 100 combinations (S, R, O) among the 1,000 most frequent implicit combinations in Visual Genome. This yields \u223c25K instances such as (person, holding, racket), (man, flying, kite), etc. 4 (iii) Generalized Words: We randomly choose 25 objects (e.g., \"woman\", \"apple\", etc.) 5 among the 100 most frequent objects in Visual Genome and take all the instances (\u223c130K) that contain any of these words. For example, since \"apple\" is in our list, e.g., (cat, sniffing, apple) is kept. ", "value": 800}, {"color": "ffecf1", "id": "When enforcing generalization conditions in our experiments, all combinations from sets (ii) and (iii) are removed from the training data to prevent the model from seeing them. 7 Even without imposing generalization conditions (or when testing with Raw data), reported results are always on unseen instances-yet the combinations (S, R, O) may have been seen during training (e.g., in different images). All sets above contain exclusively implicit spatial language, although an analogous version of the Raw set where R are explicit spatial prepositions is also considered in our experiments.", "label": "When enforcing generalization conditions in our experiments, all combinations from sets (ii) and (iii) are removed from the training data to prevent the model from seeing them. 7 Even without imposing generalization conditions (or when testing with Raw data), reported results are always on unseen instances-yet the combinations (S, R, O) may have been seen during training (e.g., in different images). All sets above contain exclusively implicit spatial language, although an analogous version of the Raw set where R are explicit spatial prepositions is also considered in our experiments.", "shape": "textbox", "title": "When enforcing generalization conditions in our experiments, all combinations from sets (ii) and (iii) are removed from the training data to prevent the model from seeing them. 7 Even without imposing generalization conditions (or when testing with Raw data), reported results are always on unseen instances-yet the combinations (S, R, O) may have been seen during training (e.g., in different images). All sets above contain exclusively implicit spatial language, although an analogous version of the Raw set where R are explicit spatial prepositions is also considered in our experiments.", "value": 800}, {"color": "ffecf1", "id": "The coordinates of bounding boxes in the images are normalized by the width and height of the image. Thus, S c , O c \u2208 [0, 1] 2 . Additionally, we notice that the distinction between left and right is arbitrary regarding the semantics of the image ", "label": "The coordinates of bounding boxes in the images are normalized by the width and height of the image. Thus, S c , O c \u2208 [0, 1] 2 . Additionally, we notice that the distinction between left and right is arbitrary regarding the semantics of the image ", "shape": "textbox", "title": "The coordinates of bounding boxes in the images are normalized by the width and height of the image. Thus, S c , O c \u2208 [0, 1] 2 . Additionally, we notice that the distinction between left and right is arbitrary regarding the semantics of the image ", "value": 800}, {"color": "ffecf1", "id": "The REG model directly outputs Object coordinates, while PIX outputs 2D heatmaps. We however enable evaluating the PIX model with regression/classification metrics by taking the point of maximum activation (or their average, if there are many) as the Object center O c . The predicted Object size O b is not estimated. We use the following performance metrics.", "label": "The REG model directly outputs Object coordinates, while PIX outputs 2D heatmaps. We however enable evaluating the PIX model with regression/classification metrics by taking the point of maximum activation (or their average, if there are many) as the Object center O c . The predicted Object size O b is not estimated. We use the following performance metrics.", "shape": "textbox", "title": "The REG model directly outputs Object coordinates, while PIX outputs 2D heatmaps. We however enable evaluating the PIX model with regression/classification metrics by taking the point of maximum activation (or their average, if there are many) as the Object center O c . The predicted Object size O b is not estimated. We use the following performance metrics.", "value": 800}, {"color": "ffecf1", "id": "(A) Intersection over Union ", "label": "(A) Intersection over Union ", "shape": "textbox", "title": "(A) Intersection over Union ", "value": 800}, {"color": "ffecf1", "id": "where B O and B O are the predicted and ground truth Object bounding boxes, respectively. If the IoU is larger than 50%, the prediction is counted as correct. It must be noted that our setting is not comparable to object detection (nor our results) since we do not employ the image as input (but text) and thus we cannot leverage the pixels to predict the Object\u0027s location.", "label": "where B O and B O are the predicted and ground truth Object bounding boxes, respectively. If the IoU is larger than 50%, the prediction is counted as correct. It must be noted that our setting is not comparable to object detection (nor our results) since we do not employ the image as input (but text) and thus we cannot leverage the pixels to predict the Object\u0027s location.", "shape": "textbox", "title": "where B O and B O are the predicted and ground truth Object bounding boxes, respectively. If the IoU is larger than 50%, the prediction is counted as correct. It must be noted that our setting is not comparable to object detection (nor our results) since we do not employ the image as input (but text) and thus we cannot leverage the pixels to predict the Object\u0027s location.", "value": 800}, {"color": "ffecf1", "id": "(B) Regression. We consider standard regression metrics. ", "label": "(B) Regression. We consider standard regression metrics. ", "shape": "textbox", "title": "(B) Regression. We consider standard regression metrics. ", "value": 800}, {"color": "ffecf1", "id": "(D) Pixel (macro) accuracy. The IoU on pixels is equivalent to binary pixel accuracy. However, this is not a good measure here, where class 1 (Object box) comprises, on average, only 5% of the pixels. Thus, a constant prediction of zeros everywhere would obtain 95% accuracy. Hence, we consider macro-averaged pixel accuracy, a.k.a. mean IoU (mIoU) ", "label": "(D) Pixel (macro) accuracy. The IoU on pixels is equivalent to binary pixel accuracy. However, this is not a good measure here, where class 1 (Object box) comprises, on average, only 5% of the pixels. Thus, a constant prediction of zeros everywhere would obtain 95% accuracy. Hence, we consider macro-averaged pixel accuracy, a.k.a. mean IoU (mIoU) ", "shape": "textbox", "title": "(D) Pixel (macro) accuracy. The IoU on pixels is equivalent to binary pixel accuracy. However, this is not a good measure here, where class 1 (Object box) comprises, on average, only 5% of the pixels. Thus, a constant prediction of zeros everywhere would obtain 95% accuracy. Hence, we consider macro-averaged pixel accuracy, a.k.a. mean IoU (mIoU) ", "value": 800}, {"color": "ffecf1", "id": "We use 300-dimensional GloVe word embeddings (Pennington, Socher, and Manning 2014) pre-trained on the Common Crawl corpus (consisting of 840B-tokens), which we obtain from the authors\u0027 website. 8", "label": "We use 300-dimensional GloVe word embeddings (Pennington, Socher, and Manning 2014) pre-trained on the Common Crawl corpus (consisting of 840B-tokens), which we obtain from the authors\u0027 website. 8", "shape": "textbox", "title": "We use 300-dimensional GloVe word embeddings (Pennington, Socher, and Manning 2014) pre-trained on the Common Crawl corpus (consisting of 840B-tokens), which we obtain from the authors\u0027 website. 8", "value": 800}, {"color": "ffecf1", "id": "Our experiments are implemented in Python 2.7 and we use Keras deep learning framework for our models ", "label": "Our experiments are implemented in Python 2.7 and we use Keras deep learning framework for our models ", "shape": "textbox", "title": "Our experiments are implemented in Python 2.7 and we use Keras deep learning framework for our models ", "value": 800}, {"color": "ffecf1", "id": "We consider the evaluation sets from Sect. 4.2 and the following variations of PIX and REG models (Sect. 3.2). The subindex EMB denotes a model that employs GloVe embeddings and RND a model with embeddings randomly drawn from a dimension-wise normal distribution of mean (\u03bc) and standard deviation (\u03c3) equal to those of the GloVe embeddings, preserving the original dimensionality (d=300). A third type employs one-hot vectors (1H). We additionally  consider a control method (ctrl) that outputs random normal predictions of \u03bc and \u03c3 equal to the dimension-wise mean and standard deviation of the training targets. We test statistical significance with a Friedman rank test and post hoc Nemenyi tests on the results of the 10 folds. We indicate with an asterisk * in the tables when a method is significantly better than the rest (p \u003c 0.01) within the same model (PIX or REG). ", "label": "We consider the evaluation sets from Sect. 4.2 and the following variations of PIX and REG models (Sect. 3.2). The subindex EMB denotes a model that employs GloVe embeddings and RND a model with embeddings randomly drawn from a dimension-wise normal distribution of mean (\u03bc) and standard deviation (\u03c3) equal to those of the GloVe embeddings, preserving the original dimensionality (d=300). A third type employs one-hot vectors (1H). We additionally  consider a control method (ctrl) that outputs random normal predictions of \u03bc and \u03c3 equal to the dimension-wise mean and standard deviation of the training targets. We test statistical significance with a Friedman rank test and post hoc Nemenyi tests on the results of the 10 folds. We indicate with an asterisk * in the tables when a method is significantly better than the rest (p \u003c 0.01) within the same model (PIX or REG). ", "shape": "textbox", "title": "We consider the evaluation sets from Sect. 4.2 and the following variations of PIX and REG models (Sect. 3.2). The subindex EMB denotes a model that employs GloVe embeddings and RND a model with embeddings randomly drawn from a dimension-wise normal distribution of mean (\u03bc) and standard deviation (\u03c3) equal to those of the GloVe embeddings, preserving the original dimensionality (d=300). A third type employs one-hot vectors (1H). We additionally  consider a control method (ctrl) that outputs random normal predictions of \u03bc and \u03c3 equal to the dimension-wise mean and standard deviation of the training targets. We test statistical significance with a Friedman rank test and post hoc Nemenyi tests on the results of the 10 folds. We indicate with an asterisk * in the tables when a method is significantly better than the rest (p \u003c 0.01) within the same model (PIX or REG). ", "value": 800}, {"color": "ffecf1", "id": "To ensure that model predictions are meaningful and interpretable, we further validate the quantitative results above with a qualitative evaluation. Notice that all plots in ", "label": "To ensure that model predictions are meaningful and interpretable, we further validate the quantitative results above with a qualitative evaluation. Notice that all plots in ", "shape": "textbox", "title": "To ensure that model predictions are meaningful and interpretable, we further validate the quantitative results above with a qualitative evaluation. Notice that all plots in ", "value": 800}, {"color": "ffecf1", "id": "We study how the weights of the model provide insight into the spatial properties of words. To obtain more interpretable weights, we learn a REG 9 model without hidden layers, resulting in only an embedding layer followed by a linear output layer\u0177 = W out u + b out , where", "label": "We study how the weights of the model provide insight into the spatial properties of words. To obtain more interpretable weights, we learn a REG 9 model without hidden layers, resulting in only an embedding layer followed by a linear output layer\u0177 = W out u + b out , where", "shape": "textbox", "title": "We study how the weights of the model provide insight into the spatial properties of words. To obtain more interpretable weights, we learn a REG 9 model without hidden layers, resulting in only an embedding layer followed by a linear output layer\u0177 = W out u + b out , where", "value": 800}, {"color": "ffecf1", "id": "By using one-hot encodings w S , w R , w O , the concatenation layer u becomes of size |V S | + |V R | + |V O | + 2 + 2. E.g., if \"wearing\" has one-hot index j in the Relationships\u0027 vocabulary (V R ), its index in the concatenation layer is |V S | + j. The product W out u is a 4-dimensional vector, where its i-th component is the product of the i-th row of W out with the vector u. Thus, the component |V S |+j of the i-th row of W out gives us the influence of the j-th relationship (i.e., \"wearing\") on the i-th dimension of the output\u0177 =   ", "label": "By using one-hot encodings w S , w R , w O , the concatenation layer u becomes of size |V S | + |V R | + |V O | + 2 + 2. E.g., if \"wearing\" has one-hot index j in the Relationships\u0027 vocabulary (V R ), its index in the concatenation layer is |V S | + j. The product W out u is a 4-dimensional vector, where its i-th component is the product of the i-th row of W out with the vector u. Thus, the component |V S |+j of the i-th row of W out gives us the influence of the j-th relationship (i.e., \"wearing\") on the i-th dimension of the output\u0177 =   ", "shape": "textbox", "title": "By using one-hot encodings w S , w R , w O , the concatenation layer u becomes of size |V S | + |V R | + |V O | + 2 + 2. E.g., if \"wearing\" has one-hot index j in the Relationships\u0027 vocabulary (V R ), its index in the concatenation layer is |V S | + j. The product W out u is a 4-dimensional vector, where its i-th component is the product of the i-th row of W out with the vector u. Thus, the component |V S |+j of the i-th row of W out gives us the influence of the j-th relationship (i.e., \"wearing\") on the i-th dimension of the output\u0177 =   ", "value": 800}, {"color": "ffecf1", "id": "Overall, this paper provides insight into the fundamental differences between implicit and explicit spatial language and extends the concept of spatial templates to implicit spatial language, the understanding of which requires common sense spatial knowledge about objects and actions. We define the task of predicting relative spatial arrangements of two objects under a relationship and present two embedding-based neural models that attain promising performance, proving that spatial templates can be accurately predicted from implicit spatial language. Remarkably, our models generalize well, predicting correctly unseen (generalized) object-relationshipobject combinations. Furthermore, the acquired common sense spatial knowledge-aided with word embeddingsallows the model to correctly predict templates for unseen words. Finally, we show that the weights of the model provide great insight into the spatial connotations of words.", "label": "Overall, this paper provides insight into the fundamental differences between implicit and explicit spatial language and extends the concept of spatial templates to implicit spatial language, the understanding of which requires common sense spatial knowledge about objects and actions. We define the task of predicting relative spatial arrangements of two objects under a relationship and present two embedding-based neural models that attain promising performance, proving that spatial templates can be accurately predicted from implicit spatial language. Remarkably, our models generalize well, predicting correctly unseen (generalized) object-relationshipobject combinations. Furthermore, the acquired common sense spatial knowledge-aided with word embeddingsallows the model to correctly predict templates for unseen words. Finally, we show that the weights of the model provide great insight into the spatial connotations of words.", "shape": "textbox", "title": "Overall, this paper provides insight into the fundamental differences between implicit and explicit spatial language and extends the concept of spatial templates to implicit spatial language, the understanding of which requires common sense spatial knowledge about objects and actions. We define the task of predicting relative spatial arrangements of two objects under a relationship and present two embedding-based neural models that attain promising performance, proving that spatial templates can be accurately predicted from implicit spatial language. Remarkably, our models generalize well, predicting correctly unseen (generalized) object-relationshipobject combinations. Furthermore, the acquired common sense spatial knowledge-aided with word embeddingsallows the model to correctly predict templates for unseen words. Finally, we show that the weights of the model provide great insight into the spatial connotations of words.", "value": 800}, {"color": "ffecf1", "id": "A first limitation of our approach is the fully supervised setting where the models are trained using images with detected ground truth objects and parsed text-which aims at keeping the design clean in this first study on implicit spatial templates. Notice however that methods to automatically parse images and text exist. In future work, we aim at implementing our approach in a weakly supervised setting. A second limitation is the 2D spatial treatment of the actual 3D world. It is worth noting however, that our models (PIX and REG) and setting trivially generalize to 3D if appropriate data are available.", "label": "A first limitation of our approach is the fully supervised setting where the models are trained using images with detected ground truth objects and parsed text-which aims at keeping the design clean in this first study on implicit spatial templates. Notice however that methods to automatically parse images and text exist. In future work, we aim at implementing our approach in a weakly supervised setting. A second limitation is the 2D spatial treatment of the actual 3D world. It is worth noting however, that our models (PIX and REG) and setting trivially generalize to 3D if appropriate data are available.", "shape": "textbox", "title": "A first limitation of our approach is the fully supervised setting where the models are trained using images with detected ground truth objects and parsed text-which aims at keeping the design clean in this first study on implicit spatial templates. Notice however that methods to automatically parse images and text exist. In future work, we aim at implementing our approach in a weakly supervised setting. A second limitation is the 2D spatial treatment of the actual 3D world. It is worth noting however, that our models (PIX and REG) and setting trivially generalize to 3D if appropriate data are available.", "value": 800}, {"color": "b37d8b", "fill": "red", "id": "Introduction", "label": "Introduction", "shape": "ellipse", "title": "Introduction", "value": 3000}, {"color": "b37d8b", "fill": "red", "id": "Related work", "label": "Related work", "shape": "ellipse", "title": "Related work", "value": 3000}, {"color": "b37d8b", "fill": "red", "id": "Proposed task", "label": "Proposed task", "shape": "ellipse", "title": "Proposed task", "value": 3000}, {"color": "b37d8b", "fill": "red", "id": "Proposed models", "label": "Proposed models", "shape": "ellipse", "title": "Proposed models", "value": 3000}, {"color": "b37d8b", "fill": "red", "id": "Experimental setup", "label": "Experimental setup", "shape": "ellipse", "title": "Experimental setup", "value": 3000}, {"color": "b37d8b", "fill": "red", "id": "Visual Genome data set", "label": "Visual Genome data set", "shape": "ellipse", "title": "Visual Genome data set", "value": 3000}, {"color": "b37d8b", "fill": "red", "id": "Evaluation sets", "label": "Evaluation sets", "shape": "ellipse", "title": "Evaluation sets", "value": 3000}, {"color": "b37d8b", "fill": "red", "id": "Data pre-processing", "label": "Data pre-processing", "shape": "ellipse", "title": "Data pre-processing", "value": 3000}, {"color": "b37d8b", "fill": "red", "id": "Evaluation metrics", "label": "Evaluation metrics", "shape": "ellipse", "title": "Evaluation metrics", "value": 3000}, {"color": "b37d8b", "fill": "red", "id": "Word embeddings", "label": "Word embeddings", "shape": "ellipse", "title": "Word embeddings", "value": 3000}, {"color": "b37d8b", "fill": "red", "id": "Model hyperparameters and implementation", "label": "Model hyperparameters and implementation", "shape": "ellipse", "title": "Model hyperparameters and implementation", "value": 3000}, {"color": "b37d8b", "fill": "red", "id": "Results and discussion", "label": "Results and discussion", "shape": "ellipse", "title": "Results and discussion", "value": 3000}, {"color": "b37d8b", "fill": "red", "id": "Evaluation with raw data", "label": "Evaluation with raw data", "shape": "ellipse", "title": "Evaluation with raw data", "value": 3000}, {"color": "b37d8b", "fill": "red", "id": "Generalized evaluations", "label": "Generalized evaluations", "shape": "ellipse", "title": "Generalized evaluations", "value": 3000}, {"color": "b37d8b", "fill": "red", "id": "Qualitative evaluation (spatial templates)", "label": "Qualitative evaluation (spatial templates)", "shape": "ellipse", "title": "Qualitative evaluation (spatial templates)", "value": 3000}, {"color": "b37d8b", "fill": "red", "id": "Interpretation of model weights", "label": "Interpretation of model weights", "shape": "ellipse", "title": "Interpretation of model weights", "value": 3000}, {"color": "b37d8b", "fill": "red", "id": "Conclusions", "label": "Conclusions", "shape": "ellipse", "title": "Conclusions", "value": 3000}]);
        edges = new vis.DataSet([{"from": "Your Visual Research Paper", "to": "Abstract", "weight": 0.97}, {"from": "Abstract", "to": "Spatial understanding is a fundamental problem with widereaching real-world applications. The representation of spatial knowledge is often modeled with spatial templates, i.e., regions of acceptability of two objects under an explicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with prior work that restricts spatial templates to explicit spatial prepositions (e.g., \"glass on table\"), here we extend this concept to implicit spatial language, i.e., those relationships (generally actions) for which the spatial arrangement of the objects is only implicitly implied (e.g., \"man riding horse\"). In contrast with explicit relationships, predicting spatial arrangements from implicit spatial language requires significant common sense spatial understanding. Here, we introduce the task of predicting spatial templates for two objects under a relationship, which can be seen as a spatial question-answering task with a (2D) continuous output (\"where is the man w.r.t. a horse when the man is walking the horse?\"). We present two simple neural-based models that leverage annotated images and structured text to learn this task. The good performance of these models reveals that spatial locations are to a large extent predictable from implicit spatial language. Crucially, the models attain similar performance in a challenging generalized setting, where the object-relation-object combinations (e.g.,\"man walking dog\") have never been seen before. Next, we go one step further by presenting the models with unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging word embeddings enables the models to output accurate spatial predictions, proving that the models acquire solid common sense spatial knowledge allowing for such generalization.", "weight": 0.87}, {"from": "Abstract", "to": "To provide machines with common sense is one of the major long term goals of artificial intelligence. Common sense knowledge regards knowledge that humans have acquired through a lifetime of experiences. It is crucial in language understanding because a lot of content needed for correct understanding is not expressed explicitly but resides in the mind of communicator and audience. In addition, humans rely on their common sense knowledge when performing a variety of tasks including interpreting images, navigation and reasoning, to name a few. Representing and understanding spatial knowledge are in fact imperative for any agent (human, animal or robot) that navigates in a physical world. In this paper, we are interested in acquiring spatial commonsense knowledge from language paired with visual data.", "weight": 0.87}, {"from": "Abstract", "to": "Computational and cognitive models often handle spatial representations as spatial templates or regions of acceptability for two objects under an explicit (a.k.a. deictic) spatial preposition such as \"on\", \"below\" or \"left\" ", "weight": 0.87}, {"from": "Abstract", "to": "Predicting spatial templates for implicit relationships is notably more challenging than for explicit relationships. Firstly, whereas there are only a few tens of explicit spatial prepositions, there exist thousands of actions, entailing thus a drastic increase in the sparsity of (object 1 , relationship, object 2 ) combinations. Secondly, the complexity of the task radically increases in implicit language. More precisely, while explicit spatial prepositions 1 are highly deterministic about the spatial arrangements (e.g., (object 1 , below, object 2 ) unequivocally implies that object 1 is relatively lower than object 2 ), actions generally are not. E.g., the relative spatial configuration of \"man\" and the object is clearly distinct in (man, pulling, kite) than in (man, pulling, luggage) yet the action is the same. Contrarily, other relationships such as \"jumping\" are highly informative about the spatial template, i.e., in (object 1 , jumping, object 2 ), object 2 is in a lower position than object 1 . Hence, unlike explicit relationships, predicting spatial layouts from implicit spatial language requires spatial common sense knowledge about the objects, actions and their interaction, which suggests the need of learning to compose the triplet (Subject, Relationship, Object) as a whole instead of learning a template for each Relationship.", "weight": 0.87}, {"from": "Abstract", "to": "To systematically study these questions, we propose the task of predicting the relative spatial locations of two objects given a structured text input (Subject, Relationship, Object). We introduce two simple neural-based models trained from annotated images that successfully address the two challenges of implicit spatial language discussed above. Our quantitative evaluation reveals that spatial templates can be reliably predicted from implicit spatial language-as accurately as from explicit spatial language. We also show that our models generalize well to templates of unseen combinations, e.g., predicting (man, riding, elephant) without having been exposed to such scene before, tackling thus the challenge of sparsity. Furthermore, by leveraging word embeddings, the models can correctly generalize to spatial templates with unseen words, e.g., predicting (man, riding, elephant) without having ever seen an \"elephant\" before. Since word embeddings capture attributes of objects ", "weight": 0.87}, {"from": "Abstract", "to": "The rest of the paper is organized as follows. In Sect. 2 we review related research. In Sect. 3 we first introduce the task of predicting spatial templates and then present two simple neural models. Then, in Sect. 4, we describe our experimental setup. In Sect. 5 we present and discuss our results. Finally, in Sect. 6 we summarize the contributions of this article.", "weight": 0.87}, {"from": "Abstract", "to": "Spatial processing has drawn significant attention from the cognitive ", "weight": 0.87}, {"from": "Abstract", "to": "Spatial templates. Earlier approaches have predominantly considered rule-based spatial representations ", "weight": 0.87}, {"from": "Abstract", "to": "Additionally, while they build a spatial template for each (explicit) Relationship, we build a template for each (Subject, Relationship, Object) combination, allowing the template to be determined by the interaction/composition of the Subject, Relationship and Object instead of the Relationship alone. Additionally, the model from ", "weight": 0.87}, {"from": "Abstract", "to": "Leveraging spatial knowledge in tasks. It has been shown that knowledge of the spatial structure in images improves the task of image captioning ", "weight": 0.87}, {"from": "Abstract", "to": "Common sense spatial knowledge. ", "weight": 0.87}, {"from": "Abstract", "to": "Image generation. Although models that generate images from text exist (e.g., DRAW model ", "weight": 0.87}, {"from": "Abstract", "to": "As discussed, spatial knowledge can improve a wide range of tasks ", "weight": 0.87}, {"from": "Abstract", "to": "Obj.", "weight": 0.87}, {"from": "Abstract", "to": "Subj.", "weight": 0.87}, {"from": "Abstract", "to": "x y Elliott and Keller 2013). This suggests that the predictions of our models can be used as spatial common sense input for methods that rely on good spatial priors. Additionally, existing methods require the spatial information to be present in the data and lack the capacity to extrapolate/generalize. Thus, in this paper we focus, first, on showing that generalizing spatial arrangements to unseen objects and object-relation-object combinations is possible and, second, on ensuring that the predicted templates are accurate by performing several quantitative and qualitative evaluations.", "weight": 0.87}, {"from": "Abstract", "to": "3 Proposed task and model", "weight": 0.87}, {"from": "Abstract", "to": "To learn spatial templates, we propose the task of predicting the 2D relative spatial arrangement of two objects under a relationship given a structured text input of the form (Subject, Relationship, Object)-henceforth abbreviated as (S, R, O). Let us denote the 2D coordinates of the center (\"c\") of the Object\u0027s box as", "weight": 0.87}, {"from": "Abstract", "to": "y \u2208 R are the horizontal and vertical components respectively. Let", "weight": 0.87}, {"from": "Abstract", "to": "x , O b y ] \u2208 R 2 be half of the width (O b x ) and half of the height (O b y ) of the Object\u0027s box (\"b\"). We employ a similar notation for the Subject (S c , S b ), and model predictions are denoted with a hat O c , O b . The task consists in predicting the Object\u0027s location and size", "weight": 0.87}, {"from": "Abstract", "to": "given the structured text input (S, R, O) and the location S c and size S b of the Subject ", "weight": 0.87}, {"from": "Abstract", "to": "2 Crucially, we notice that knowing the Subject\u0027s coordinates is not a requirement for generating templates (but only for evaluating against the ground truth) since inputting arbitrary coordinates (e.g., S c =[0.5, 0.5]) still enables visualizing relative object locations. Additionally, we input the Subject\u0027s size in order to provide a reference size to the model. However, we find that without this input, the model still learns to predict an \"average size\" for each Object.", "weight": 0.87}, {"from": "Abstract", "to": "Hence, the goal is to answer common sense spatial questions such as \"if a man is feeding a horse, where is the man relative to the horse?\" or \"where would a child wear her shoes?\"", "weight": 0.87}, {"from": "Abstract", "to": "To build a mapping from the input to the output in our task (Sect. 3.1) we propose two simple neural models ( ", "weight": 0.87}, {"from": "Abstract", "to": "are one-hot encodings of S, R, O (a.k.a. one-of-k encoding, i.e., a sparse vector with 0 everywhere except for a 1 at the position of the k-th word) and", "weight": 0.87}, {"from": "Abstract", "to": "This layer represents objects and relationships as continuous features, enabling thus to introduce external knowledge of unseen objects as features. The embeddings are then concatenated with the Subject center S c and size", "weight": 0.87}, {"from": "Abstract", "to": "which is inputted to a stack of hidden layers that compose S, R and O into a joint hidden representation z h :", "weight": 0.87}, {"from": "Abstract", "to": "where f (\u2022) is an element-wise non-linear function and W h and b h are the weight matrix and bias respectively. 3 (ii) Output and loss. We consider two different possibilities for the output\u0177 that follows immediately after the last layer: ", "weight": 0.87}, {"from": "Abstract", "to": "These models are conceptually different and have different capabilities. While the REG model outputs \"crisp\" pointwise predictions, PIX can model more diffuse spatial templates where the location of the object has more variability, e.g., in (man, flying, kite) the \"kite\" can easily move around. Notice that in contrast with convolutional neural networks (CNNs) our approach does not make use of the image pixels ( ", "weight": 0.87}, {"from": "Abstract", "to": "We employ a 10-fold cross-validation (CV) setting. Data are randomly split into 10 disjoint parts and 10% is employed for testing and 90% for training, repeating this for each of the 10 folds. Reported results are averages over the 10 folds.", "weight": 0.87}, {"from": "Abstract", "to": "We use the Visual Genome dataset ", "weight": 0.87}, {"from": "Abstract", "to": "dog, catches, frisbee boy, feeds, giraffe man, throws, frisbee cat, wears glasses ", "weight": 0.87}, {"from": "Abstract", "to": "We consider the following subsets of the Visual Genome data to evaluate performance. (i) Raw data: Simply the unfiltered instances from the Visual Genome data (Sect. 4.1). This set contains a substantial proportion of meaningless (e.g., (nose, almost, touching)) and irrelevant (e.g., (sign, says, gate 2)) instances. (ii) Generalized Triplets: We pick at random 100 combinations (S, R, O) among the 1,000 most frequent implicit combinations in Visual Genome. This yields \u223c25K instances such as (person, holding, racket), (man, flying, kite), etc. 4 (iii) Generalized Words: We randomly choose 25 objects (e.g., \"woman\", \"apple\", etc.) 5 among the 100 most frequent objects in Visual Genome and take all the instances (\u223c130K) that contain any of these words. For example, since \"apple\" is in our list, e.g., (cat, sniffing, apple) is kept. ", "weight": 0.87}, {"from": "Abstract", "to": "When enforcing generalization conditions in our experiments, all combinations from sets (ii) and (iii) are removed from the training data to prevent the model from seeing them. 7 Even without imposing generalization conditions (or when testing with Raw data), reported results are always on unseen instances-yet the combinations (S, R, O) may have been seen during training (e.g., in different images). All sets above contain exclusively implicit spatial language, although an analogous version of the Raw set where R are explicit spatial prepositions is also considered in our experiments.", "weight": 0.87}, {"from": "Abstract", "to": "The coordinates of bounding boxes in the images are normalized by the width and height of the image. Thus, S c , O c \u2208 [0, 1] 2 . Additionally, we notice that the distinction between left and right is arbitrary regarding the semantics of the image ", "weight": 0.87}, {"from": "Abstract", "to": "The REG model directly outputs Object coordinates, while PIX outputs 2D heatmaps. We however enable evaluating the PIX model with regression/classification metrics by taking the point of maximum activation (or their average, if there are many) as the Object center O c . The predicted Object size O b is not estimated. We use the following performance metrics.", "weight": 0.87}, {"from": "Abstract", "to": "(A) Intersection over Union ", "weight": 0.87}, {"from": "Abstract", "to": "where B O and B O are the predicted and ground truth Object bounding boxes, respectively. If the IoU is larger than 50%, the prediction is counted as correct. It must be noted that our setting is not comparable to object detection (nor our results) since we do not employ the image as input (but text) and thus we cannot leverage the pixels to predict the Object\u0027s location.", "weight": 0.87}, {"from": "Abstract", "to": "(B) Regression. We consider standard regression metrics. ", "weight": 0.87}, {"from": "Abstract", "to": "(D) Pixel (macro) accuracy. The IoU on pixels is equivalent to binary pixel accuracy. However, this is not a good measure here, where class 1 (Object box) comprises, on average, only 5% of the pixels. Thus, a constant prediction of zeros everywhere would obtain 95% accuracy. Hence, we consider macro-averaged pixel accuracy, a.k.a. mean IoU (mIoU) ", "weight": 0.87}, {"from": "Abstract", "to": "We use 300-dimensional GloVe word embeddings (Pennington, Socher, and Manning 2014) pre-trained on the Common Crawl corpus (consisting of 840B-tokens), which we obtain from the authors\u0027 website. 8", "weight": 0.87}, {"from": "Abstract", "to": "Our experiments are implemented in Python 2.7 and we use Keras deep learning framework for our models ", "weight": 0.87}, {"from": "Abstract", "to": "We consider the evaluation sets from Sect. 4.2 and the following variations of PIX and REG models (Sect. 3.2). The subindex EMB denotes a model that employs GloVe embeddings and RND a model with embeddings randomly drawn from a dimension-wise normal distribution of mean (\u03bc) and standard deviation (\u03c3) equal to those of the GloVe embeddings, preserving the original dimensionality (d=300). A third type employs one-hot vectors (1H). We additionally  consider a control method (ctrl) that outputs random normal predictions of \u03bc and \u03c3 equal to the dimension-wise mean and standard deviation of the training targets. We test statistical significance with a Friedman rank test and post hoc Nemenyi tests on the results of the 10 folds. We indicate with an asterisk * in the tables when a method is significantly better than the rest (p \u003c 0.01) within the same model (PIX or REG). ", "weight": 0.87}, {"from": "Abstract", "to": "To ensure that model predictions are meaningful and interpretable, we further validate the quantitative results above with a qualitative evaluation. Notice that all plots in ", "weight": 0.87}, {"from": "Abstract", "to": "We study how the weights of the model provide insight into the spatial properties of words. To obtain more interpretable weights, we learn a REG 9 model without hidden layers, resulting in only an embedding layer followed by a linear output layer\u0177 = W out u + b out , where", "weight": 0.87}, {"from": "Abstract", "to": "By using one-hot encodings w S , w R , w O , the concatenation layer u becomes of size |V S | + |V R | + |V O | + 2 + 2. E.g., if \"wearing\" has one-hot index j in the Relationships\u0027 vocabulary (V R ), its index in the concatenation layer is |V S | + j. The product W out u is a 4-dimensional vector, where its i-th component is the product of the i-th row of W out with the vector u. Thus, the component |V S |+j of the i-th row of W out gives us the influence of the j-th relationship (i.e., \"wearing\") on the i-th dimension of the output\u0177 =   ", "weight": 0.87}, {"from": "Abstract", "to": "Overall, this paper provides insight into the fundamental differences between implicit and explicit spatial language and extends the concept of spatial templates to implicit spatial language, the understanding of which requires common sense spatial knowledge about objects and actions. We define the task of predicting relative spatial arrangements of two objects under a relationship and present two embedding-based neural models that attain promising performance, proving that spatial templates can be accurately predicted from implicit spatial language. Remarkably, our models generalize well, predicting correctly unseen (generalized) object-relationshipobject combinations. Furthermore, the acquired common sense spatial knowledge-aided with word embeddingsallows the model to correctly predict templates for unseen words. Finally, we show that the weights of the model provide great insight into the spatial connotations of words.", "weight": 0.87}, {"from": "Abstract", "to": "A first limitation of our approach is the fully supervised setting where the models are trained using images with detected ground truth objects and parsed text-which aims at keeping the design clean in this first study on implicit spatial templates. Notice however that methods to automatically parse images and text exist. In future work, we aim at implementing our approach in a weakly supervised setting. A second limitation is the 2D spatial treatment of the actual 3D world. It is worth noting however, that our models (PIX and REG) and setting trivially generalize to 3D if appropriate data are available.", "weight": 0.87}, {"from": "Your Visual Research Paper", "to": "Introduction", "weight": 0.97}, {"from": "Introduction", "to": "Spatial understanding is a fundamental problem with widereaching real-world applications. The representation of spatial knowledge is often modeled with spatial templates, i.e., regions of acceptability of two objects under an explicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with prior work that restricts spatial templates to explicit spatial prepositions (e.g., \"glass on table\"), here we extend this concept to implicit spatial language, i.e., those relationships (generally actions) for which the spatial arrangement of the objects is only implicitly implied (e.g., \"man riding horse\"). In contrast with explicit relationships, predicting spatial arrangements from implicit spatial language requires significant common sense spatial understanding. Here, we introduce the task of predicting spatial templates for two objects under a relationship, which can be seen as a spatial question-answering task with a (2D) continuous output (\"where is the man w.r.t. a horse when the man is walking the horse?\"). We present two simple neural-based models that leverage annotated images and structured text to learn this task. The good performance of these models reveals that spatial locations are to a large extent predictable from implicit spatial language. Crucially, the models attain similar performance in a challenging generalized setting, where the object-relation-object combinations (e.g.,\"man walking dog\") have never been seen before. Next, we go one step further by presenting the models with unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging word embeddings enables the models to output accurate spatial predictions, proving that the models acquire solid common sense spatial knowledge allowing for such generalization.", "weight": 0.87}, {"from": "Introduction", "to": "To provide machines with common sense is one of the major long term goals of artificial intelligence. Common sense knowledge regards knowledge that humans have acquired through a lifetime of experiences. It is crucial in language understanding because a lot of content needed for correct understanding is not expressed explicitly but resides in the mind of communicator and audience. In addition, humans rely on their common sense knowledge when performing a variety of tasks including interpreting images, navigation and reasoning, to name a few. Representing and understanding spatial knowledge are in fact imperative for any agent (human, animal or robot) that navigates in a physical world. In this paper, we are interested in acquiring spatial commonsense knowledge from language paired with visual data.", "weight": 0.87}, {"from": "Introduction", "to": "Computational and cognitive models often handle spatial representations as spatial templates or regions of acceptability for two objects under an explicit (a.k.a. deictic) spatial preposition such as \"on\", \"below\" or \"left\" ", "weight": 0.87}, {"from": "Introduction", "to": "Predicting spatial templates for implicit relationships is notably more challenging than for explicit relationships. Firstly, whereas there are only a few tens of explicit spatial prepositions, there exist thousands of actions, entailing thus a drastic increase in the sparsity of (object 1 , relationship, object 2 ) combinations. Secondly, the complexity of the task radically increases in implicit language. More precisely, while explicit spatial prepositions 1 are highly deterministic about the spatial arrangements (e.g., (object 1 , below, object 2 ) unequivocally implies that object 1 is relatively lower than object 2 ), actions generally are not. E.g., the relative spatial configuration of \"man\" and the object is clearly distinct in (man, pulling, kite) than in (man, pulling, luggage) yet the action is the same. Contrarily, other relationships such as \"jumping\" are highly informative about the spatial template, i.e., in (object 1 , jumping, object 2 ), object 2 is in a lower position than object 1 . Hence, unlike explicit relationships, predicting spatial layouts from implicit spatial language requires spatial common sense knowledge about the objects, actions and their interaction, which suggests the need of learning to compose the triplet (Subject, Relationship, Object) as a whole instead of learning a template for each Relationship.", "weight": 0.87}, {"from": "Introduction", "to": "To systematically study these questions, we propose the task of predicting the relative spatial locations of two objects given a structured text input (Subject, Relationship, Object). We introduce two simple neural-based models trained from annotated images that successfully address the two challenges of implicit spatial language discussed above. Our quantitative evaluation reveals that spatial templates can be reliably predicted from implicit spatial language-as accurately as from explicit spatial language. We also show that our models generalize well to templates of unseen combinations, e.g., predicting (man, riding, elephant) without having been exposed to such scene before, tackling thus the challenge of sparsity. Furthermore, by leveraging word embeddings, the models can correctly generalize to spatial templates with unseen words, e.g., predicting (man, riding, elephant) without having ever seen an \"elephant\" before. Since word embeddings capture attributes of objects ", "weight": 0.87}, {"from": "Introduction", "to": "The rest of the paper is organized as follows. In Sect. 2 we review related research. In Sect. 3 we first introduce the task of predicting spatial templates and then present two simple neural models. Then, in Sect. 4, we describe our experimental setup. In Sect. 5 we present and discuss our results. Finally, in Sect. 6 we summarize the contributions of this article.", "weight": 0.87}, {"from": "Introduction", "to": "Spatial processing has drawn significant attention from the cognitive ", "weight": 0.87}, {"from": "Introduction", "to": "Spatial templates. Earlier approaches have predominantly considered rule-based spatial representations ", "weight": 0.87}, {"from": "Introduction", "to": "Additionally, while they build a spatial template for each (explicit) Relationship, we build a template for each (Subject, Relationship, Object) combination, allowing the template to be determined by the interaction/composition of the Subject, Relationship and Object instead of the Relationship alone. Additionally, the model from ", "weight": 0.87}, {"from": "Introduction", "to": "Leveraging spatial knowledge in tasks. It has been shown that knowledge of the spatial structure in images improves the task of image captioning ", "weight": 0.87}, {"from": "Introduction", "to": "Common sense spatial knowledge. ", "weight": 0.87}, {"from": "Introduction", "to": "Image generation. Although models that generate images from text exist (e.g., DRAW model ", "weight": 0.87}, {"from": "Introduction", "to": "As discussed, spatial knowledge can improve a wide range of tasks ", "weight": 0.87}, {"from": "Introduction", "to": "Obj.", "weight": 0.87}, {"from": "Introduction", "to": "Subj.", "weight": 0.87}, {"from": "Introduction", "to": "x y Elliott and Keller 2013). This suggests that the predictions of our models can be used as spatial common sense input for methods that rely on good spatial priors. Additionally, existing methods require the spatial information to be present in the data and lack the capacity to extrapolate/generalize. Thus, in this paper we focus, first, on showing that generalizing spatial arrangements to unseen objects and object-relation-object combinations is possible and, second, on ensuring that the predicted templates are accurate by performing several quantitative and qualitative evaluations.", "weight": 0.87}, {"from": "Introduction", "to": "3 Proposed task and model", "weight": 0.87}, {"from": "Introduction", "to": "To learn spatial templates, we propose the task of predicting the 2D relative spatial arrangement of two objects under a relationship given a structured text input of the form (Subject, Relationship, Object)-henceforth abbreviated as (S, R, O). Let us denote the 2D coordinates of the center (\"c\") of the Object\u0027s box as", "weight": 0.87}, {"from": "Introduction", "to": "y \u2208 R are the horizontal and vertical components respectively. Let", "weight": 0.87}, {"from": "Introduction", "to": "x , O b y ] \u2208 R 2 be half of the width (O b x ) and half of the height (O b y ) of the Object\u0027s box (\"b\"). We employ a similar notation for the Subject (S c , S b ), and model predictions are denoted with a hat O c , O b . The task consists in predicting the Object\u0027s location and size", "weight": 0.87}, {"from": "Introduction", "to": "given the structured text input (S, R, O) and the location S c and size S b of the Subject ", "weight": 0.87}, {"from": "Introduction", "to": "2 Crucially, we notice that knowing the Subject\u0027s coordinates is not a requirement for generating templates (but only for evaluating against the ground truth) since inputting arbitrary coordinates (e.g., S c =[0.5, 0.5]) still enables visualizing relative object locations. Additionally, we input the Subject\u0027s size in order to provide a reference size to the model. However, we find that without this input, the model still learns to predict an \"average size\" for each Object.", "weight": 0.87}, {"from": "Introduction", "to": "Hence, the goal is to answer common sense spatial questions such as \"if a man is feeding a horse, where is the man relative to the horse?\" or \"where would a child wear her shoes?\"", "weight": 0.87}, {"from": "Introduction", "to": "To build a mapping from the input to the output in our task (Sect. 3.1) we propose two simple neural models ( ", "weight": 0.87}, {"from": "Introduction", "to": "are one-hot encodings of S, R, O (a.k.a. one-of-k encoding, i.e., a sparse vector with 0 everywhere except for a 1 at the position of the k-th word) and", "weight": 0.87}, {"from": "Introduction", "to": "This layer represents objects and relationships as continuous features, enabling thus to introduce external knowledge of unseen objects as features. The embeddings are then concatenated with the Subject center S c and size", "weight": 0.87}, {"from": "Introduction", "to": "which is inputted to a stack of hidden layers that compose S, R and O into a joint hidden representation z h :", "weight": 0.87}, {"from": "Introduction", "to": "where f (\u2022) is an element-wise non-linear function and W h and b h are the weight matrix and bias respectively. 3 (ii) Output and loss. We consider two different possibilities for the output\u0177 that follows immediately after the last layer: ", "weight": 0.87}, {"from": "Introduction", "to": "These models are conceptually different and have different capabilities. While the REG model outputs \"crisp\" pointwise predictions, PIX can model more diffuse spatial templates where the location of the object has more variability, e.g., in (man, flying, kite) the \"kite\" can easily move around. Notice that in contrast with convolutional neural networks (CNNs) our approach does not make use of the image pixels ( ", "weight": 0.87}, {"from": "Introduction", "to": "We employ a 10-fold cross-validation (CV) setting. Data are randomly split into 10 disjoint parts and 10% is employed for testing and 90% for training, repeating this for each of the 10 folds. Reported results are averages over the 10 folds.", "weight": 0.87}, {"from": "Introduction", "to": "We use the Visual Genome dataset ", "weight": 0.87}, {"from": "Introduction", "to": "dog, catches, frisbee boy, feeds, giraffe man, throws, frisbee cat, wears glasses ", "weight": 0.87}, {"from": "Introduction", "to": "We consider the following subsets of the Visual Genome data to evaluate performance. (i) Raw data: Simply the unfiltered instances from the Visual Genome data (Sect. 4.1). This set contains a substantial proportion of meaningless (e.g., (nose, almost, touching)) and irrelevant (e.g., (sign, says, gate 2)) instances. (ii) Generalized Triplets: We pick at random 100 combinations (S, R, O) among the 1,000 most frequent implicit combinations in Visual Genome. This yields \u223c25K instances such as (person, holding, racket), (man, flying, kite), etc. 4 (iii) Generalized Words: We randomly choose 25 objects (e.g., \"woman\", \"apple\", etc.) 5 among the 100 most frequent objects in Visual Genome and take all the instances (\u223c130K) that contain any of these words. For example, since \"apple\" is in our list, e.g., (cat, sniffing, apple) is kept. ", "weight": 0.87}, {"from": "Introduction", "to": "When enforcing generalization conditions in our experiments, all combinations from sets (ii) and (iii) are removed from the training data to prevent the model from seeing them. 7 Even without imposing generalization conditions (or when testing with Raw data), reported results are always on unseen instances-yet the combinations (S, R, O) may have been seen during training (e.g., in different images). All sets above contain exclusively implicit spatial language, although an analogous version of the Raw set where R are explicit spatial prepositions is also considered in our experiments.", "weight": 0.87}, {"from": "Introduction", "to": "The coordinates of bounding boxes in the images are normalized by the width and height of the image. Thus, S c , O c \u2208 [0, 1] 2 . Additionally, we notice that the distinction between left and right is arbitrary regarding the semantics of the image ", "weight": 0.87}, {"from": "Introduction", "to": "The REG model directly outputs Object coordinates, while PIX outputs 2D heatmaps. We however enable evaluating the PIX model with regression/classification metrics by taking the point of maximum activation (or their average, if there are many) as the Object center O c . The predicted Object size O b is not estimated. We use the following performance metrics.", "weight": 0.87}, {"from": "Introduction", "to": "(A) Intersection over Union ", "weight": 0.87}, {"from": "Introduction", "to": "where B O and B O are the predicted and ground truth Object bounding boxes, respectively. If the IoU is larger than 50%, the prediction is counted as correct. It must be noted that our setting is not comparable to object detection (nor our results) since we do not employ the image as input (but text) and thus we cannot leverage the pixels to predict the Object\u0027s location.", "weight": 0.87}, {"from": "Introduction", "to": "(B) Regression. We consider standard regression metrics. ", "weight": 0.87}, {"from": "Introduction", "to": "(D) Pixel (macro) accuracy. The IoU on pixels is equivalent to binary pixel accuracy. However, this is not a good measure here, where class 1 (Object box) comprises, on average, only 5% of the pixels. Thus, a constant prediction of zeros everywhere would obtain 95% accuracy. Hence, we consider macro-averaged pixel accuracy, a.k.a. mean IoU (mIoU) ", "weight": 0.87}, {"from": "Introduction", "to": "We use 300-dimensional GloVe word embeddings (Pennington, Socher, and Manning 2014) pre-trained on the Common Crawl corpus (consisting of 840B-tokens), which we obtain from the authors\u0027 website. 8", "weight": 0.87}, {"from": "Introduction", "to": "Our experiments are implemented in Python 2.7 and we use Keras deep learning framework for our models ", "weight": 0.87}, {"from": "Introduction", "to": "We consider the evaluation sets from Sect. 4.2 and the following variations of PIX and REG models (Sect. 3.2). The subindex EMB denotes a model that employs GloVe embeddings and RND a model with embeddings randomly drawn from a dimension-wise normal distribution of mean (\u03bc) and standard deviation (\u03c3) equal to those of the GloVe embeddings, preserving the original dimensionality (d=300). A third type employs one-hot vectors (1H). We additionally  consider a control method (ctrl) that outputs random normal predictions of \u03bc and \u03c3 equal to the dimension-wise mean and standard deviation of the training targets. We test statistical significance with a Friedman rank test and post hoc Nemenyi tests on the results of the 10 folds. We indicate with an asterisk * in the tables when a method is significantly better than the rest (p \u003c 0.01) within the same model (PIX or REG). ", "weight": 0.87}, {"from": "Introduction", "to": "To ensure that model predictions are meaningful and interpretable, we further validate the quantitative results above with a qualitative evaluation. Notice that all plots in ", "weight": 0.87}, {"from": "Introduction", "to": "We study how the weights of the model provide insight into the spatial properties of words. To obtain more interpretable weights, we learn a REG 9 model without hidden layers, resulting in only an embedding layer followed by a linear output layer\u0177 = W out u + b out , where", "weight": 0.87}, {"from": "Introduction", "to": "By using one-hot encodings w S , w R , w O , the concatenation layer u becomes of size |V S | + |V R | + |V O | + 2 + 2. E.g., if \"wearing\" has one-hot index j in the Relationships\u0027 vocabulary (V R ), its index in the concatenation layer is |V S | + j. The product W out u is a 4-dimensional vector, where its i-th component is the product of the i-th row of W out with the vector u. Thus, the component |V S |+j of the i-th row of W out gives us the influence of the j-th relationship (i.e., \"wearing\") on the i-th dimension of the output\u0177 =   ", "weight": 0.87}, {"from": "Introduction", "to": "Overall, this paper provides insight into the fundamental differences between implicit and explicit spatial language and extends the concept of spatial templates to implicit spatial language, the understanding of which requires common sense spatial knowledge about objects and actions. We define the task of predicting relative spatial arrangements of two objects under a relationship and present two embedding-based neural models that attain promising performance, proving that spatial templates can be accurately predicted from implicit spatial language. Remarkably, our models generalize well, predicting correctly unseen (generalized) object-relationshipobject combinations. Furthermore, the acquired common sense spatial knowledge-aided with word embeddingsallows the model to correctly predict templates for unseen words. Finally, we show that the weights of the model provide great insight into the spatial connotations of words.", "weight": 0.87}, {"from": "Introduction", "to": "A first limitation of our approach is the fully supervised setting where the models are trained using images with detected ground truth objects and parsed text-which aims at keeping the design clean in this first study on implicit spatial templates. Notice however that methods to automatically parse images and text exist. In future work, we aim at implementing our approach in a weakly supervised setting. A second limitation is the 2D spatial treatment of the actual 3D world. It is worth noting however, that our models (PIX and REG) and setting trivially generalize to 3D if appropriate data are available.", "weight": 0.87}, {"from": "Your Visual Research Paper", "to": "Related work", "weight": 0.97}, {"from": "Related work", "to": "Spatial understanding is a fundamental problem with widereaching real-world applications. The representation of spatial knowledge is often modeled with spatial templates, i.e., regions of acceptability of two objects under an explicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with prior work that restricts spatial templates to explicit spatial prepositions (e.g., \"glass on table\"), here we extend this concept to implicit spatial language, i.e., those relationships (generally actions) for which the spatial arrangement of the objects is only implicitly implied (e.g., \"man riding horse\"). In contrast with explicit relationships, predicting spatial arrangements from implicit spatial language requires significant common sense spatial understanding. Here, we introduce the task of predicting spatial templates for two objects under a relationship, which can be seen as a spatial question-answering task with a (2D) continuous output (\"where is the man w.r.t. a horse when the man is walking the horse?\"). We present two simple neural-based models that leverage annotated images and structured text to learn this task. The good performance of these models reveals that spatial locations are to a large extent predictable from implicit spatial language. Crucially, the models attain similar performance in a challenging generalized setting, where the object-relation-object combinations (e.g.,\"man walking dog\") have never been seen before. Next, we go one step further by presenting the models with unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging word embeddings enables the models to output accurate spatial predictions, proving that the models acquire solid common sense spatial knowledge allowing for such generalization.", "weight": 0.87}, {"from": "Related work", "to": "To provide machines with common sense is one of the major long term goals of artificial intelligence. Common sense knowledge regards knowledge that humans have acquired through a lifetime of experiences. It is crucial in language understanding because a lot of content needed for correct understanding is not expressed explicitly but resides in the mind of communicator and audience. In addition, humans rely on their common sense knowledge when performing a variety of tasks including interpreting images, navigation and reasoning, to name a few. Representing and understanding spatial knowledge are in fact imperative for any agent (human, animal or robot) that navigates in a physical world. In this paper, we are interested in acquiring spatial commonsense knowledge from language paired with visual data.", "weight": 0.87}, {"from": "Related work", "to": "Computational and cognitive models often handle spatial representations as spatial templates or regions of acceptability for two objects under an explicit (a.k.a. deictic) spatial preposition such as \"on\", \"below\" or \"left\" ", "weight": 0.87}, {"from": "Related work", "to": "Predicting spatial templates for implicit relationships is notably more challenging than for explicit relationships. Firstly, whereas there are only a few tens of explicit spatial prepositions, there exist thousands of actions, entailing thus a drastic increase in the sparsity of (object 1 , relationship, object 2 ) combinations. Secondly, the complexity of the task radically increases in implicit language. More precisely, while explicit spatial prepositions 1 are highly deterministic about the spatial arrangements (e.g., (object 1 , below, object 2 ) unequivocally implies that object 1 is relatively lower than object 2 ), actions generally are not. E.g., the relative spatial configuration of \"man\" and the object is clearly distinct in (man, pulling, kite) than in (man, pulling, luggage) yet the action is the same. Contrarily, other relationships such as \"jumping\" are highly informative about the spatial template, i.e., in (object 1 , jumping, object 2 ), object 2 is in a lower position than object 1 . Hence, unlike explicit relationships, predicting spatial layouts from implicit spatial language requires spatial common sense knowledge about the objects, actions and their interaction, which suggests the need of learning to compose the triplet (Subject, Relationship, Object) as a whole instead of learning a template for each Relationship.", "weight": 0.87}, {"from": "Related work", "to": "To systematically study these questions, we propose the task of predicting the relative spatial locations of two objects given a structured text input (Subject, Relationship, Object). We introduce two simple neural-based models trained from annotated images that successfully address the two challenges of implicit spatial language discussed above. Our quantitative evaluation reveals that spatial templates can be reliably predicted from implicit spatial language-as accurately as from explicit spatial language. We also show that our models generalize well to templates of unseen combinations, e.g., predicting (man, riding, elephant) without having been exposed to such scene before, tackling thus the challenge of sparsity. Furthermore, by leveraging word embeddings, the models can correctly generalize to spatial templates with unseen words, e.g., predicting (man, riding, elephant) without having ever seen an \"elephant\" before. Since word embeddings capture attributes of objects ", "weight": 0.87}, {"from": "Related work", "to": "The rest of the paper is organized as follows. In Sect. 2 we review related research. In Sect. 3 we first introduce the task of predicting spatial templates and then present two simple neural models. Then, in Sect. 4, we describe our experimental setup. In Sect. 5 we present and discuss our results. Finally, in Sect. 6 we summarize the contributions of this article.", "weight": 0.87}, {"from": "Related work", "to": "Spatial processing has drawn significant attention from the cognitive ", "weight": 0.87}, {"from": "Related work", "to": "Spatial templates. Earlier approaches have predominantly considered rule-based spatial representations ", "weight": 0.87}, {"from": "Related work", "to": "Additionally, while they build a spatial template for each (explicit) Relationship, we build a template for each (Subject, Relationship, Object) combination, allowing the template to be determined by the interaction/composition of the Subject, Relationship and Object instead of the Relationship alone. Additionally, the model from ", "weight": 0.87}, {"from": "Related work", "to": "Leveraging spatial knowledge in tasks. It has been shown that knowledge of the spatial structure in images improves the task of image captioning ", "weight": 0.87}, {"from": "Related work", "to": "Common sense spatial knowledge. ", "weight": 0.87}, {"from": "Related work", "to": "Image generation. Although models that generate images from text exist (e.g., DRAW model ", "weight": 0.87}, {"from": "Related work", "to": "As discussed, spatial knowledge can improve a wide range of tasks ", "weight": 0.87}, {"from": "Related work", "to": "Obj.", "weight": 0.87}, {"from": "Related work", "to": "Subj.", "weight": 0.87}, {"from": "Related work", "to": "x y Elliott and Keller 2013). This suggests that the predictions of our models can be used as spatial common sense input for methods that rely on good spatial priors. Additionally, existing methods require the spatial information to be present in the data and lack the capacity to extrapolate/generalize. Thus, in this paper we focus, first, on showing that generalizing spatial arrangements to unseen objects and object-relation-object combinations is possible and, second, on ensuring that the predicted templates are accurate by performing several quantitative and qualitative evaluations.", "weight": 0.87}, {"from": "Related work", "to": "3 Proposed task and model", "weight": 0.87}, {"from": "Related work", "to": "To learn spatial templates, we propose the task of predicting the 2D relative spatial arrangement of two objects under a relationship given a structured text input of the form (Subject, Relationship, Object)-henceforth abbreviated as (S, R, O). Let us denote the 2D coordinates of the center (\"c\") of the Object\u0027s box as", "weight": 0.87}, {"from": "Related work", "to": "y \u2208 R are the horizontal and vertical components respectively. Let", "weight": 0.87}, {"from": "Related work", "to": "x , O b y ] \u2208 R 2 be half of the width (O b x ) and half of the height (O b y ) of the Object\u0027s box (\"b\"). We employ a similar notation for the Subject (S c , S b ), and model predictions are denoted with a hat O c , O b . The task consists in predicting the Object\u0027s location and size", "weight": 0.87}, {"from": "Related work", "to": "given the structured text input (S, R, O) and the location S c and size S b of the Subject ", "weight": 0.87}, {"from": "Related work", "to": "2 Crucially, we notice that knowing the Subject\u0027s coordinates is not a requirement for generating templates (but only for evaluating against the ground truth) since inputting arbitrary coordinates (e.g., S c =[0.5, 0.5]) still enables visualizing relative object locations. Additionally, we input the Subject\u0027s size in order to provide a reference size to the model. However, we find that without this input, the model still learns to predict an \"average size\" for each Object.", "weight": 0.87}, {"from": "Related work", "to": "Hence, the goal is to answer common sense spatial questions such as \"if a man is feeding a horse, where is the man relative to the horse?\" or \"where would a child wear her shoes?\"", "weight": 0.87}, {"from": "Related work", "to": "To build a mapping from the input to the output in our task (Sect. 3.1) we propose two simple neural models ( ", "weight": 0.87}, {"from": "Related work", "to": "are one-hot encodings of S, R, O (a.k.a. one-of-k encoding, i.e., a sparse vector with 0 everywhere except for a 1 at the position of the k-th word) and", "weight": 0.87}, {"from": "Related work", "to": "This layer represents objects and relationships as continuous features, enabling thus to introduce external knowledge of unseen objects as features. The embeddings are then concatenated with the Subject center S c and size", "weight": 0.87}, {"from": "Related work", "to": "which is inputted to a stack of hidden layers that compose S, R and O into a joint hidden representation z h :", "weight": 0.87}, {"from": "Related work", "to": "where f (\u2022) is an element-wise non-linear function and W h and b h are the weight matrix and bias respectively. 3 (ii) Output and loss. We consider two different possibilities for the output\u0177 that follows immediately after the last layer: ", "weight": 0.87}, {"from": "Related work", "to": "These models are conceptually different and have different capabilities. While the REG model outputs \"crisp\" pointwise predictions, PIX can model more diffuse spatial templates where the location of the object has more variability, e.g., in (man, flying, kite) the \"kite\" can easily move around. Notice that in contrast with convolutional neural networks (CNNs) our approach does not make use of the image pixels ( ", "weight": 0.87}, {"from": "Related work", "to": "We employ a 10-fold cross-validation (CV) setting. Data are randomly split into 10 disjoint parts and 10% is employed for testing and 90% for training, repeating this for each of the 10 folds. Reported results are averages over the 10 folds.", "weight": 0.87}, {"from": "Related work", "to": "We use the Visual Genome dataset ", "weight": 0.87}, {"from": "Related work", "to": "dog, catches, frisbee boy, feeds, giraffe man, throws, frisbee cat, wears glasses ", "weight": 0.87}, {"from": "Related work", "to": "We consider the following subsets of the Visual Genome data to evaluate performance. (i) Raw data: Simply the unfiltered instances from the Visual Genome data (Sect. 4.1). This set contains a substantial proportion of meaningless (e.g., (nose, almost, touching)) and irrelevant (e.g., (sign, says, gate 2)) instances. (ii) Generalized Triplets: We pick at random 100 combinations (S, R, O) among the 1,000 most frequent implicit combinations in Visual Genome. This yields \u223c25K instances such as (person, holding, racket), (man, flying, kite), etc. 4 (iii) Generalized Words: We randomly choose 25 objects (e.g., \"woman\", \"apple\", etc.) 5 among the 100 most frequent objects in Visual Genome and take all the instances (\u223c130K) that contain any of these words. For example, since \"apple\" is in our list, e.g., (cat, sniffing, apple) is kept. ", "weight": 0.87}, {"from": "Related work", "to": "When enforcing generalization conditions in our experiments, all combinations from sets (ii) and (iii) are removed from the training data to prevent the model from seeing them. 7 Even without imposing generalization conditions (or when testing with Raw data), reported results are always on unseen instances-yet the combinations (S, R, O) may have been seen during training (e.g., in different images). All sets above contain exclusively implicit spatial language, although an analogous version of the Raw set where R are explicit spatial prepositions is also considered in our experiments.", "weight": 0.87}, {"from": "Related work", "to": "The coordinates of bounding boxes in the images are normalized by the width and height of the image. Thus, S c , O c \u2208 [0, 1] 2 . Additionally, we notice that the distinction between left and right is arbitrary regarding the semantics of the image ", "weight": 0.87}, {"from": "Related work", "to": "The REG model directly outputs Object coordinates, while PIX outputs 2D heatmaps. We however enable evaluating the PIX model with regression/classification metrics by taking the point of maximum activation (or their average, if there are many) as the Object center O c . The predicted Object size O b is not estimated. We use the following performance metrics.", "weight": 0.87}, {"from": "Related work", "to": "(A) Intersection over Union ", "weight": 0.87}, {"from": "Related work", "to": "where B O and B O are the predicted and ground truth Object bounding boxes, respectively. If the IoU is larger than 50%, the prediction is counted as correct. It must be noted that our setting is not comparable to object detection (nor our results) since we do not employ the image as input (but text) and thus we cannot leverage the pixels to predict the Object\u0027s location.", "weight": 0.87}, {"from": "Related work", "to": "(B) Regression. We consider standard regression metrics. ", "weight": 0.87}, {"from": "Related work", "to": "(D) Pixel (macro) accuracy. The IoU on pixels is equivalent to binary pixel accuracy. However, this is not a good measure here, where class 1 (Object box) comprises, on average, only 5% of the pixels. Thus, a constant prediction of zeros everywhere would obtain 95% accuracy. Hence, we consider macro-averaged pixel accuracy, a.k.a. mean IoU (mIoU) ", "weight": 0.87}, {"from": "Related work", "to": "We use 300-dimensional GloVe word embeddings (Pennington, Socher, and Manning 2014) pre-trained on the Common Crawl corpus (consisting of 840B-tokens), which we obtain from the authors\u0027 website. 8", "weight": 0.87}, {"from": "Related work", "to": "Our experiments are implemented in Python 2.7 and we use Keras deep learning framework for our models ", "weight": 0.87}, {"from": "Related work", "to": "We consider the evaluation sets from Sect. 4.2 and the following variations of PIX and REG models (Sect. 3.2). The subindex EMB denotes a model that employs GloVe embeddings and RND a model with embeddings randomly drawn from a dimension-wise normal distribution of mean (\u03bc) and standard deviation (\u03c3) equal to those of the GloVe embeddings, preserving the original dimensionality (d=300). A third type employs one-hot vectors (1H). We additionally  consider a control method (ctrl) that outputs random normal predictions of \u03bc and \u03c3 equal to the dimension-wise mean and standard deviation of the training targets. We test statistical significance with a Friedman rank test and post hoc Nemenyi tests on the results of the 10 folds. We indicate with an asterisk * in the tables when a method is significantly better than the rest (p \u003c 0.01) within the same model (PIX or REG). ", "weight": 0.87}, {"from": "Related work", "to": "To ensure that model predictions are meaningful and interpretable, we further validate the quantitative results above with a qualitative evaluation. Notice that all plots in ", "weight": 0.87}, {"from": "Related work", "to": "We study how the weights of the model provide insight into the spatial properties of words. To obtain more interpretable weights, we learn a REG 9 model without hidden layers, resulting in only an embedding layer followed by a linear output layer\u0177 = W out u + b out , where", "weight": 0.87}, {"from": "Related work", "to": "By using one-hot encodings w S , w R , w O , the concatenation layer u becomes of size |V S | + |V R | + |V O | + 2 + 2. E.g., if \"wearing\" has one-hot index j in the Relationships\u0027 vocabulary (V R ), its index in the concatenation layer is |V S | + j. The product W out u is a 4-dimensional vector, where its i-th component is the product of the i-th row of W out with the vector u. Thus, the component |V S |+j of the i-th row of W out gives us the influence of the j-th relationship (i.e., \"wearing\") on the i-th dimension of the output\u0177 =   ", "weight": 0.87}, {"from": "Related work", "to": "Overall, this paper provides insight into the fundamental differences between implicit and explicit spatial language and extends the concept of spatial templates to implicit spatial language, the understanding of which requires common sense spatial knowledge about objects and actions. We define the task of predicting relative spatial arrangements of two objects under a relationship and present two embedding-based neural models that attain promising performance, proving that spatial templates can be accurately predicted from implicit spatial language. Remarkably, our models generalize well, predicting correctly unseen (generalized) object-relationshipobject combinations. Furthermore, the acquired common sense spatial knowledge-aided with word embeddingsallows the model to correctly predict templates for unseen words. Finally, we show that the weights of the model provide great insight into the spatial connotations of words.", "weight": 0.87}, {"from": "Related work", "to": "A first limitation of our approach is the fully supervised setting where the models are trained using images with detected ground truth objects and parsed text-which aims at keeping the design clean in this first study on implicit spatial templates. Notice however that methods to automatically parse images and text exist. In future work, we aim at implementing our approach in a weakly supervised setting. A second limitation is the 2D spatial treatment of the actual 3D world. It is worth noting however, that our models (PIX and REG) and setting trivially generalize to 3D if appropriate data are available.", "weight": 0.87}, {"from": "Your Visual Research Paper", "to": "Proposed task", "weight": 0.97}, {"from": "Proposed task", "to": "Spatial understanding is a fundamental problem with widereaching real-world applications. The representation of spatial knowledge is often modeled with spatial templates, i.e., regions of acceptability of two objects under an explicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with prior work that restricts spatial templates to explicit spatial prepositions (e.g., \"glass on table\"), here we extend this concept to implicit spatial language, i.e., those relationships (generally actions) for which the spatial arrangement of the objects is only implicitly implied (e.g., \"man riding horse\"). In contrast with explicit relationships, predicting spatial arrangements from implicit spatial language requires significant common sense spatial understanding. Here, we introduce the task of predicting spatial templates for two objects under a relationship, which can be seen as a spatial question-answering task with a (2D) continuous output (\"where is the man w.r.t. a horse when the man is walking the horse?\"). We present two simple neural-based models that leverage annotated images and structured text to learn this task. The good performance of these models reveals that spatial locations are to a large extent predictable from implicit spatial language. Crucially, the models attain similar performance in a challenging generalized setting, where the object-relation-object combinations (e.g.,\"man walking dog\") have never been seen before. Next, we go one step further by presenting the models with unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging word embeddings enables the models to output accurate spatial predictions, proving that the models acquire solid common sense spatial knowledge allowing for such generalization.", "weight": 0.87}, {"from": "Proposed task", "to": "To provide machines with common sense is one of the major long term goals of artificial intelligence. Common sense knowledge regards knowledge that humans have acquired through a lifetime of experiences. It is crucial in language understanding because a lot of content needed for correct understanding is not expressed explicitly but resides in the mind of communicator and audience. In addition, humans rely on their common sense knowledge when performing a variety of tasks including interpreting images, navigation and reasoning, to name a few. Representing and understanding spatial knowledge are in fact imperative for any agent (human, animal or robot) that navigates in a physical world. In this paper, we are interested in acquiring spatial commonsense knowledge from language paired with visual data.", "weight": 0.87}, {"from": "Proposed task", "to": "Computational and cognitive models often handle spatial representations as spatial templates or regions of acceptability for two objects under an explicit (a.k.a. deictic) spatial preposition such as \"on\", \"below\" or \"left\" ", "weight": 0.87}, {"from": "Proposed task", "to": "Predicting spatial templates for implicit relationships is notably more challenging than for explicit relationships. Firstly, whereas there are only a few tens of explicit spatial prepositions, there exist thousands of actions, entailing thus a drastic increase in the sparsity of (object 1 , relationship, object 2 ) combinations. Secondly, the complexity of the task radically increases in implicit language. More precisely, while explicit spatial prepositions 1 are highly deterministic about the spatial arrangements (e.g., (object 1 , below, object 2 ) unequivocally implies that object 1 is relatively lower than object 2 ), actions generally are not. E.g., the relative spatial configuration of \"man\" and the object is clearly distinct in (man, pulling, kite) than in (man, pulling, luggage) yet the action is the same. Contrarily, other relationships such as \"jumping\" are highly informative about the spatial template, i.e., in (object 1 , jumping, object 2 ), object 2 is in a lower position than object 1 . Hence, unlike explicit relationships, predicting spatial layouts from implicit spatial language requires spatial common sense knowledge about the objects, actions and their interaction, which suggests the need of learning to compose the triplet (Subject, Relationship, Object) as a whole instead of learning a template for each Relationship.", "weight": 0.87}, {"from": "Proposed task", "to": "To systematically study these questions, we propose the task of predicting the relative spatial locations of two objects given a structured text input (Subject, Relationship, Object). We introduce two simple neural-based models trained from annotated images that successfully address the two challenges of implicit spatial language discussed above. Our quantitative evaluation reveals that spatial templates can be reliably predicted from implicit spatial language-as accurately as from explicit spatial language. We also show that our models generalize well to templates of unseen combinations, e.g., predicting (man, riding, elephant) without having been exposed to such scene before, tackling thus the challenge of sparsity. Furthermore, by leveraging word embeddings, the models can correctly generalize to spatial templates with unseen words, e.g., predicting (man, riding, elephant) without having ever seen an \"elephant\" before. Since word embeddings capture attributes of objects ", "weight": 0.87}, {"from": "Proposed task", "to": "The rest of the paper is organized as follows. In Sect. 2 we review related research. In Sect. 3 we first introduce the task of predicting spatial templates and then present two simple neural models. Then, in Sect. 4, we describe our experimental setup. In Sect. 5 we present and discuss our results. Finally, in Sect. 6 we summarize the contributions of this article.", "weight": 0.87}, {"from": "Proposed task", "to": "Spatial processing has drawn significant attention from the cognitive ", "weight": 0.87}, {"from": "Proposed task", "to": "Spatial templates. Earlier approaches have predominantly considered rule-based spatial representations ", "weight": 0.87}, {"from": "Proposed task", "to": "Additionally, while they build a spatial template for each (explicit) Relationship, we build a template for each (Subject, Relationship, Object) combination, allowing the template to be determined by the interaction/composition of the Subject, Relationship and Object instead of the Relationship alone. Additionally, the model from ", "weight": 0.87}, {"from": "Proposed task", "to": "Leveraging spatial knowledge in tasks. It has been shown that knowledge of the spatial structure in images improves the task of image captioning ", "weight": 0.87}, {"from": "Proposed task", "to": "Common sense spatial knowledge. ", "weight": 0.87}, {"from": "Proposed task", "to": "Image generation. Although models that generate images from text exist (e.g., DRAW model ", "weight": 0.87}, {"from": "Proposed task", "to": "As discussed, spatial knowledge can improve a wide range of tasks ", "weight": 0.87}, {"from": "Proposed task", "to": "Obj.", "weight": 0.87}, {"from": "Proposed task", "to": "Subj.", "weight": 0.87}, {"from": "Proposed task", "to": "x y Elliott and Keller 2013). This suggests that the predictions of our models can be used as spatial common sense input for methods that rely on good spatial priors. Additionally, existing methods require the spatial information to be present in the data and lack the capacity to extrapolate/generalize. Thus, in this paper we focus, first, on showing that generalizing spatial arrangements to unseen objects and object-relation-object combinations is possible and, second, on ensuring that the predicted templates are accurate by performing several quantitative and qualitative evaluations.", "weight": 0.87}, {"from": "Proposed task", "to": "3 Proposed task and model", "weight": 0.87}, {"from": "Proposed task", "to": "To learn spatial templates, we propose the task of predicting the 2D relative spatial arrangement of two objects under a relationship given a structured text input of the form (Subject, Relationship, Object)-henceforth abbreviated as (S, R, O). Let us denote the 2D coordinates of the center (\"c\") of the Object\u0027s box as", "weight": 0.87}, {"from": "Proposed task", "to": "y \u2208 R are the horizontal and vertical components respectively. Let", "weight": 0.87}, {"from": "Proposed task", "to": "x , O b y ] \u2208 R 2 be half of the width (O b x ) and half of the height (O b y ) of the Object\u0027s box (\"b\"). We employ a similar notation for the Subject (S c , S b ), and model predictions are denoted with a hat O c , O b . The task consists in predicting the Object\u0027s location and size", "weight": 0.87}, {"from": "Proposed task", "to": "given the structured text input (S, R, O) and the location S c and size S b of the Subject ", "weight": 0.87}, {"from": "Proposed task", "to": "2 Crucially, we notice that knowing the Subject\u0027s coordinates is not a requirement for generating templates (but only for evaluating against the ground truth) since inputting arbitrary coordinates (e.g., S c =[0.5, 0.5]) still enables visualizing relative object locations. Additionally, we input the Subject\u0027s size in order to provide a reference size to the model. However, we find that without this input, the model still learns to predict an \"average size\" for each Object.", "weight": 0.87}, {"from": "Proposed task", "to": "Hence, the goal is to answer common sense spatial questions such as \"if a man is feeding a horse, where is the man relative to the horse?\" or \"where would a child wear her shoes?\"", "weight": 0.87}, {"from": "Proposed task", "to": "To build a mapping from the input to the output in our task (Sect. 3.1) we propose two simple neural models ( ", "weight": 0.87}, {"from": "Proposed task", "to": "are one-hot encodings of S, R, O (a.k.a. one-of-k encoding, i.e., a sparse vector with 0 everywhere except for a 1 at the position of the k-th word) and", "weight": 0.87}, {"from": "Proposed task", "to": "This layer represents objects and relationships as continuous features, enabling thus to introduce external knowledge of unseen objects as features. The embeddings are then concatenated with the Subject center S c and size", "weight": 0.87}, {"from": "Proposed task", "to": "which is inputted to a stack of hidden layers that compose S, R and O into a joint hidden representation z h :", "weight": 0.87}, {"from": "Proposed task", "to": "where f (\u2022) is an element-wise non-linear function and W h and b h are the weight matrix and bias respectively. 3 (ii) Output and loss. We consider two different possibilities for the output\u0177 that follows immediately after the last layer: ", "weight": 0.87}, {"from": "Proposed task", "to": "These models are conceptually different and have different capabilities. While the REG model outputs \"crisp\" pointwise predictions, PIX can model more diffuse spatial templates where the location of the object has more variability, e.g., in (man, flying, kite) the \"kite\" can easily move around. Notice that in contrast with convolutional neural networks (CNNs) our approach does not make use of the image pixels ( ", "weight": 0.87}, {"from": "Proposed task", "to": "We employ a 10-fold cross-validation (CV) setting. Data are randomly split into 10 disjoint parts and 10% is employed for testing and 90% for training, repeating this for each of the 10 folds. Reported results are averages over the 10 folds.", "weight": 0.87}, {"from": "Proposed task", "to": "We use the Visual Genome dataset ", "weight": 0.87}, {"from": "Proposed task", "to": "dog, catches, frisbee boy, feeds, giraffe man, throws, frisbee cat, wears glasses ", "weight": 0.87}, {"from": "Proposed task", "to": "We consider the following subsets of the Visual Genome data to evaluate performance. (i) Raw data: Simply the unfiltered instances from the Visual Genome data (Sect. 4.1). This set contains a substantial proportion of meaningless (e.g., (nose, almost, touching)) and irrelevant (e.g., (sign, says, gate 2)) instances. (ii) Generalized Triplets: We pick at random 100 combinations (S, R, O) among the 1,000 most frequent implicit combinations in Visual Genome. This yields \u223c25K instances such as (person, holding, racket), (man, flying, kite), etc. 4 (iii) Generalized Words: We randomly choose 25 objects (e.g., \"woman\", \"apple\", etc.) 5 among the 100 most frequent objects in Visual Genome and take all the instances (\u223c130K) that contain any of these words. For example, since \"apple\" is in our list, e.g., (cat, sniffing, apple) is kept. ", "weight": 0.87}, {"from": "Proposed task", "to": "When enforcing generalization conditions in our experiments, all combinations from sets (ii) and (iii) are removed from the training data to prevent the model from seeing them. 7 Even without imposing generalization conditions (or when testing with Raw data), reported results are always on unseen instances-yet the combinations (S, R, O) may have been seen during training (e.g., in different images). All sets above contain exclusively implicit spatial language, although an analogous version of the Raw set where R are explicit spatial prepositions is also considered in our experiments.", "weight": 0.87}, {"from": "Proposed task", "to": "The coordinates of bounding boxes in the images are normalized by the width and height of the image. Thus, S c , O c \u2208 [0, 1] 2 . Additionally, we notice that the distinction between left and right is arbitrary regarding the semantics of the image ", "weight": 0.87}, {"from": "Proposed task", "to": "The REG model directly outputs Object coordinates, while PIX outputs 2D heatmaps. We however enable evaluating the PIX model with regression/classification metrics by taking the point of maximum activation (or their average, if there are many) as the Object center O c . The predicted Object size O b is not estimated. We use the following performance metrics.", "weight": 0.87}, {"from": "Proposed task", "to": "(A) Intersection over Union ", "weight": 0.87}, {"from": "Proposed task", "to": "where B O and B O are the predicted and ground truth Object bounding boxes, respectively. If the IoU is larger than 50%, the prediction is counted as correct. It must be noted that our setting is not comparable to object detection (nor our results) since we do not employ the image as input (but text) and thus we cannot leverage the pixels to predict the Object\u0027s location.", "weight": 0.87}, {"from": "Proposed task", "to": "(B) Regression. We consider standard regression metrics. ", "weight": 0.87}, {"from": "Proposed task", "to": "(D) Pixel (macro) accuracy. The IoU on pixels is equivalent to binary pixel accuracy. However, this is not a good measure here, where class 1 (Object box) comprises, on average, only 5% of the pixels. Thus, a constant prediction of zeros everywhere would obtain 95% accuracy. Hence, we consider macro-averaged pixel accuracy, a.k.a. mean IoU (mIoU) ", "weight": 0.87}, {"from": "Proposed task", "to": "We use 300-dimensional GloVe word embeddings (Pennington, Socher, and Manning 2014) pre-trained on the Common Crawl corpus (consisting of 840B-tokens), which we obtain from the authors\u0027 website. 8", "weight": 0.87}, {"from": "Proposed task", "to": "Our experiments are implemented in Python 2.7 and we use Keras deep learning framework for our models ", "weight": 0.87}, {"from": "Proposed task", "to": "We consider the evaluation sets from Sect. 4.2 and the following variations of PIX and REG models (Sect. 3.2). The subindex EMB denotes a model that employs GloVe embeddings and RND a model with embeddings randomly drawn from a dimension-wise normal distribution of mean (\u03bc) and standard deviation (\u03c3) equal to those of the GloVe embeddings, preserving the original dimensionality (d=300). A third type employs one-hot vectors (1H). We additionally  consider a control method (ctrl) that outputs random normal predictions of \u03bc and \u03c3 equal to the dimension-wise mean and standard deviation of the training targets. We test statistical significance with a Friedman rank test and post hoc Nemenyi tests on the results of the 10 folds. We indicate with an asterisk * in the tables when a method is significantly better than the rest (p \u003c 0.01) within the same model (PIX or REG). ", "weight": 0.87}, {"from": "Proposed task", "to": "To ensure that model predictions are meaningful and interpretable, we further validate the quantitative results above with a qualitative evaluation. Notice that all plots in ", "weight": 0.87}, {"from": "Proposed task", "to": "We study how the weights of the model provide insight into the spatial properties of words. To obtain more interpretable weights, we learn a REG 9 model without hidden layers, resulting in only an embedding layer followed by a linear output layer\u0177 = W out u + b out , where", "weight": 0.87}, {"from": "Proposed task", "to": "By using one-hot encodings w S , w R , w O , the concatenation layer u becomes of size |V S | + |V R | + |V O | + 2 + 2. E.g., if \"wearing\" has one-hot index j in the Relationships\u0027 vocabulary (V R ), its index in the concatenation layer is |V S | + j. The product W out u is a 4-dimensional vector, where its i-th component is the product of the i-th row of W out with the vector u. Thus, the component |V S |+j of the i-th row of W out gives us the influence of the j-th relationship (i.e., \"wearing\") on the i-th dimension of the output\u0177 =   ", "weight": 0.87}, {"from": "Proposed task", "to": "Overall, this paper provides insight into the fundamental differences between implicit and explicit spatial language and extends the concept of spatial templates to implicit spatial language, the understanding of which requires common sense spatial knowledge about objects and actions. We define the task of predicting relative spatial arrangements of two objects under a relationship and present two embedding-based neural models that attain promising performance, proving that spatial templates can be accurately predicted from implicit spatial language. Remarkably, our models generalize well, predicting correctly unseen (generalized) object-relationshipobject combinations. Furthermore, the acquired common sense spatial knowledge-aided with word embeddingsallows the model to correctly predict templates for unseen words. Finally, we show that the weights of the model provide great insight into the spatial connotations of words.", "weight": 0.87}, {"from": "Proposed task", "to": "A first limitation of our approach is the fully supervised setting where the models are trained using images with detected ground truth objects and parsed text-which aims at keeping the design clean in this first study on implicit spatial templates. Notice however that methods to automatically parse images and text exist. In future work, we aim at implementing our approach in a weakly supervised setting. A second limitation is the 2D spatial treatment of the actual 3D world. It is worth noting however, that our models (PIX and REG) and setting trivially generalize to 3D if appropriate data are available.", "weight": 0.87}, {"from": "Your Visual Research Paper", "to": "Proposed models", "weight": 0.97}, {"from": "Proposed models", "to": "Spatial understanding is a fundamental problem with widereaching real-world applications. The representation of spatial knowledge is often modeled with spatial templates, i.e., regions of acceptability of two objects under an explicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with prior work that restricts spatial templates to explicit spatial prepositions (e.g., \"glass on table\"), here we extend this concept to implicit spatial language, i.e., those relationships (generally actions) for which the spatial arrangement of the objects is only implicitly implied (e.g., \"man riding horse\"). In contrast with explicit relationships, predicting spatial arrangements from implicit spatial language requires significant common sense spatial understanding. Here, we introduce the task of predicting spatial templates for two objects under a relationship, which can be seen as a spatial question-answering task with a (2D) continuous output (\"where is the man w.r.t. a horse when the man is walking the horse?\"). We present two simple neural-based models that leverage annotated images and structured text to learn this task. The good performance of these models reveals that spatial locations are to a large extent predictable from implicit spatial language. Crucially, the models attain similar performance in a challenging generalized setting, where the object-relation-object combinations (e.g.,\"man walking dog\") have never been seen before. Next, we go one step further by presenting the models with unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging word embeddings enables the models to output accurate spatial predictions, proving that the models acquire solid common sense spatial knowledge allowing for such generalization.", "weight": 0.87}, {"from": "Proposed models", "to": "To provide machines with common sense is one of the major long term goals of artificial intelligence. Common sense knowledge regards knowledge that humans have acquired through a lifetime of experiences. It is crucial in language understanding because a lot of content needed for correct understanding is not expressed explicitly but resides in the mind of communicator and audience. In addition, humans rely on their common sense knowledge when performing a variety of tasks including interpreting images, navigation and reasoning, to name a few. Representing and understanding spatial knowledge are in fact imperative for any agent (human, animal or robot) that navigates in a physical world. In this paper, we are interested in acquiring spatial commonsense knowledge from language paired with visual data.", "weight": 0.87}, {"from": "Proposed models", "to": "Computational and cognitive models often handle spatial representations as spatial templates or regions of acceptability for two objects under an explicit (a.k.a. deictic) spatial preposition such as \"on\", \"below\" or \"left\" ", "weight": 0.87}, {"from": "Proposed models", "to": "Predicting spatial templates for implicit relationships is notably more challenging than for explicit relationships. Firstly, whereas there are only a few tens of explicit spatial prepositions, there exist thousands of actions, entailing thus a drastic increase in the sparsity of (object 1 , relationship, object 2 ) combinations. Secondly, the complexity of the task radically increases in implicit language. More precisely, while explicit spatial prepositions 1 are highly deterministic about the spatial arrangements (e.g., (object 1 , below, object 2 ) unequivocally implies that object 1 is relatively lower than object 2 ), actions generally are not. E.g., the relative spatial configuration of \"man\" and the object is clearly distinct in (man, pulling, kite) than in (man, pulling, luggage) yet the action is the same. Contrarily, other relationships such as \"jumping\" are highly informative about the spatial template, i.e., in (object 1 , jumping, object 2 ), object 2 is in a lower position than object 1 . Hence, unlike explicit relationships, predicting spatial layouts from implicit spatial language requires spatial common sense knowledge about the objects, actions and their interaction, which suggests the need of learning to compose the triplet (Subject, Relationship, Object) as a whole instead of learning a template for each Relationship.", "weight": 0.87}, {"from": "Proposed models", "to": "To systematically study these questions, we propose the task of predicting the relative spatial locations of two objects given a structured text input (Subject, Relationship, Object). We introduce two simple neural-based models trained from annotated images that successfully address the two challenges of implicit spatial language discussed above. Our quantitative evaluation reveals that spatial templates can be reliably predicted from implicit spatial language-as accurately as from explicit spatial language. We also show that our models generalize well to templates of unseen combinations, e.g., predicting (man, riding, elephant) without having been exposed to such scene before, tackling thus the challenge of sparsity. Furthermore, by leveraging word embeddings, the models can correctly generalize to spatial templates with unseen words, e.g., predicting (man, riding, elephant) without having ever seen an \"elephant\" before. Since word embeddings capture attributes of objects ", "weight": 0.87}, {"from": "Proposed models", "to": "The rest of the paper is organized as follows. In Sect. 2 we review related research. In Sect. 3 we first introduce the task of predicting spatial templates and then present two simple neural models. Then, in Sect. 4, we describe our experimental setup. In Sect. 5 we present and discuss our results. Finally, in Sect. 6 we summarize the contributions of this article.", "weight": 0.87}, {"from": "Proposed models", "to": "Spatial processing has drawn significant attention from the cognitive ", "weight": 0.87}, {"from": "Proposed models", "to": "Spatial templates. Earlier approaches have predominantly considered rule-based spatial representations ", "weight": 0.87}, {"from": "Proposed models", "to": "Additionally, while they build a spatial template for each (explicit) Relationship, we build a template for each (Subject, Relationship, Object) combination, allowing the template to be determined by the interaction/composition of the Subject, Relationship and Object instead of the Relationship alone. Additionally, the model from ", "weight": 0.87}, {"from": "Proposed models", "to": "Leveraging spatial knowledge in tasks. It has been shown that knowledge of the spatial structure in images improves the task of image captioning ", "weight": 0.87}, {"from": "Proposed models", "to": "Common sense spatial knowledge. ", "weight": 0.87}, {"from": "Proposed models", "to": "Image generation. Although models that generate images from text exist (e.g., DRAW model ", "weight": 0.87}, {"from": "Proposed models", "to": "As discussed, spatial knowledge can improve a wide range of tasks ", "weight": 0.87}, {"from": "Proposed models", "to": "Obj.", "weight": 0.87}, {"from": "Proposed models", "to": "Subj.", "weight": 0.87}, {"from": "Proposed models", "to": "x y Elliott and Keller 2013). This suggests that the predictions of our models can be used as spatial common sense input for methods that rely on good spatial priors. Additionally, existing methods require the spatial information to be present in the data and lack the capacity to extrapolate/generalize. Thus, in this paper we focus, first, on showing that generalizing spatial arrangements to unseen objects and object-relation-object combinations is possible and, second, on ensuring that the predicted templates are accurate by performing several quantitative and qualitative evaluations.", "weight": 0.87}, {"from": "Proposed models", "to": "3 Proposed task and model", "weight": 0.87}, {"from": "Proposed models", "to": "To learn spatial templates, we propose the task of predicting the 2D relative spatial arrangement of two objects under a relationship given a structured text input of the form (Subject, Relationship, Object)-henceforth abbreviated as (S, R, O). Let us denote the 2D coordinates of the center (\"c\") of the Object\u0027s box as", "weight": 0.87}, {"from": "Proposed models", "to": "y \u2208 R are the horizontal and vertical components respectively. Let", "weight": 0.87}, {"from": "Proposed models", "to": "x , O b y ] \u2208 R 2 be half of the width (O b x ) and half of the height (O b y ) of the Object\u0027s box (\"b\"). We employ a similar notation for the Subject (S c , S b ), and model predictions are denoted with a hat O c , O b . The task consists in predicting the Object\u0027s location and size", "weight": 0.87}, {"from": "Proposed models", "to": "given the structured text input (S, R, O) and the location S c and size S b of the Subject ", "weight": 0.87}, {"from": "Proposed models", "to": "2 Crucially, we notice that knowing the Subject\u0027s coordinates is not a requirement for generating templates (but only for evaluating against the ground truth) since inputting arbitrary coordinates (e.g., S c =[0.5, 0.5]) still enables visualizing relative object locations. Additionally, we input the Subject\u0027s size in order to provide a reference size to the model. However, we find that without this input, the model still learns to predict an \"average size\" for each Object.", "weight": 0.87}, {"from": "Proposed models", "to": "Hence, the goal is to answer common sense spatial questions such as \"if a man is feeding a horse, where is the man relative to the horse?\" or \"where would a child wear her shoes?\"", "weight": 0.87}, {"from": "Proposed models", "to": "To build a mapping from the input to the output in our task (Sect. 3.1) we propose two simple neural models ( ", "weight": 0.87}, {"from": "Proposed models", "to": "are one-hot encodings of S, R, O (a.k.a. one-of-k encoding, i.e., a sparse vector with 0 everywhere except for a 1 at the position of the k-th word) and", "weight": 0.87}, {"from": "Proposed models", "to": "This layer represents objects and relationships as continuous features, enabling thus to introduce external knowledge of unseen objects as features. The embeddings are then concatenated with the Subject center S c and size", "weight": 0.87}, {"from": "Proposed models", "to": "which is inputted to a stack of hidden layers that compose S, R and O into a joint hidden representation z h :", "weight": 0.87}, {"from": "Proposed models", "to": "where f (\u2022) is an element-wise non-linear function and W h and b h are the weight matrix and bias respectively. 3 (ii) Output and loss. We consider two different possibilities for the output\u0177 that follows immediately after the last layer: ", "weight": 0.87}, {"from": "Proposed models", "to": "These models are conceptually different and have different capabilities. While the REG model outputs \"crisp\" pointwise predictions, PIX can model more diffuse spatial templates where the location of the object has more variability, e.g., in (man, flying, kite) the \"kite\" can easily move around. Notice that in contrast with convolutional neural networks (CNNs) our approach does not make use of the image pixels ( ", "weight": 0.87}, {"from": "Proposed models", "to": "We employ a 10-fold cross-validation (CV) setting. Data are randomly split into 10 disjoint parts and 10% is employed for testing and 90% for training, repeating this for each of the 10 folds. Reported results are averages over the 10 folds.", "weight": 0.87}, {"from": "Proposed models", "to": "We use the Visual Genome dataset ", "weight": 0.87}, {"from": "Proposed models", "to": "dog, catches, frisbee boy, feeds, giraffe man, throws, frisbee cat, wears glasses ", "weight": 0.87}, {"from": "Proposed models", "to": "We consider the following subsets of the Visual Genome data to evaluate performance. (i) Raw data: Simply the unfiltered instances from the Visual Genome data (Sect. 4.1). This set contains a substantial proportion of meaningless (e.g., (nose, almost, touching)) and irrelevant (e.g., (sign, says, gate 2)) instances. (ii) Generalized Triplets: We pick at random 100 combinations (S, R, O) among the 1,000 most frequent implicit combinations in Visual Genome. This yields \u223c25K instances such as (person, holding, racket), (man, flying, kite), etc. 4 (iii) Generalized Words: We randomly choose 25 objects (e.g., \"woman\", \"apple\", etc.) 5 among the 100 most frequent objects in Visual Genome and take all the instances (\u223c130K) that contain any of these words. For example, since \"apple\" is in our list, e.g., (cat, sniffing, apple) is kept. ", "weight": 0.87}, {"from": "Proposed models", "to": "When enforcing generalization conditions in our experiments, all combinations from sets (ii) and (iii) are removed from the training data to prevent the model from seeing them. 7 Even without imposing generalization conditions (or when testing with Raw data), reported results are always on unseen instances-yet the combinations (S, R, O) may have been seen during training (e.g., in different images). All sets above contain exclusively implicit spatial language, although an analogous version of the Raw set where R are explicit spatial prepositions is also considered in our experiments.", "weight": 0.87}, {"from": "Proposed models", "to": "The coordinates of bounding boxes in the images are normalized by the width and height of the image. Thus, S c , O c \u2208 [0, 1] 2 . Additionally, we notice that the distinction between left and right is arbitrary regarding the semantics of the image ", "weight": 0.87}, {"from": "Proposed models", "to": "The REG model directly outputs Object coordinates, while PIX outputs 2D heatmaps. We however enable evaluating the PIX model with regression/classification metrics by taking the point of maximum activation (or their average, if there are many) as the Object center O c . The predicted Object size O b is not estimated. We use the following performance metrics.", "weight": 0.87}, {"from": "Proposed models", "to": "(A) Intersection over Union ", "weight": 0.87}, {"from": "Proposed models", "to": "where B O and B O are the predicted and ground truth Object bounding boxes, respectively. If the IoU is larger than 50%, the prediction is counted as correct. It must be noted that our setting is not comparable to object detection (nor our results) since we do not employ the image as input (but text) and thus we cannot leverage the pixels to predict the Object\u0027s location.", "weight": 0.87}, {"from": "Proposed models", "to": "(B) Regression. We consider standard regression metrics. ", "weight": 0.87}, {"from": "Proposed models", "to": "(D) Pixel (macro) accuracy. The IoU on pixels is equivalent to binary pixel accuracy. However, this is not a good measure here, where class 1 (Object box) comprises, on average, only 5% of the pixels. Thus, a constant prediction of zeros everywhere would obtain 95% accuracy. Hence, we consider macro-averaged pixel accuracy, a.k.a. mean IoU (mIoU) ", "weight": 0.87}, {"from": "Proposed models", "to": "We use 300-dimensional GloVe word embeddings (Pennington, Socher, and Manning 2014) pre-trained on the Common Crawl corpus (consisting of 840B-tokens), which we obtain from the authors\u0027 website. 8", "weight": 0.87}, {"from": "Proposed models", "to": "Our experiments are implemented in Python 2.7 and we use Keras deep learning framework for our models ", "weight": 0.87}, {"from": "Proposed models", "to": "We consider the evaluation sets from Sect. 4.2 and the following variations of PIX and REG models (Sect. 3.2). The subindex EMB denotes a model that employs GloVe embeddings and RND a model with embeddings randomly drawn from a dimension-wise normal distribution of mean (\u03bc) and standard deviation (\u03c3) equal to those of the GloVe embeddings, preserving the original dimensionality (d=300). A third type employs one-hot vectors (1H). We additionally  consider a control method (ctrl) that outputs random normal predictions of \u03bc and \u03c3 equal to the dimension-wise mean and standard deviation of the training targets. We test statistical significance with a Friedman rank test and post hoc Nemenyi tests on the results of the 10 folds. We indicate with an asterisk * in the tables when a method is significantly better than the rest (p \u003c 0.01) within the same model (PIX or REG). ", "weight": 0.87}, {"from": "Proposed models", "to": "To ensure that model predictions are meaningful and interpretable, we further validate the quantitative results above with a qualitative evaluation. Notice that all plots in ", "weight": 0.87}, {"from": "Proposed models", "to": "We study how the weights of the model provide insight into the spatial properties of words. To obtain more interpretable weights, we learn a REG 9 model without hidden layers, resulting in only an embedding layer followed by a linear output layer\u0177 = W out u + b out , where", "weight": 0.87}, {"from": "Proposed models", "to": "By using one-hot encodings w S , w R , w O , the concatenation layer u becomes of size |V S | + |V R | + |V O | + 2 + 2. E.g., if \"wearing\" has one-hot index j in the Relationships\u0027 vocabulary (V R ), its index in the concatenation layer is |V S | + j. The product W out u is a 4-dimensional vector, where its i-th component is the product of the i-th row of W out with the vector u. Thus, the component |V S |+j of the i-th row of W out gives us the influence of the j-th relationship (i.e., \"wearing\") on the i-th dimension of the output\u0177 =   ", "weight": 0.87}, {"from": "Proposed models", "to": "Overall, this paper provides insight into the fundamental differences between implicit and explicit spatial language and extends the concept of spatial templates to implicit spatial language, the understanding of which requires common sense spatial knowledge about objects and actions. We define the task of predicting relative spatial arrangements of two objects under a relationship and present two embedding-based neural models that attain promising performance, proving that spatial templates can be accurately predicted from implicit spatial language. Remarkably, our models generalize well, predicting correctly unseen (generalized) object-relationshipobject combinations. Furthermore, the acquired common sense spatial knowledge-aided with word embeddingsallows the model to correctly predict templates for unseen words. Finally, we show that the weights of the model provide great insight into the spatial connotations of words.", "weight": 0.87}, {"from": "Proposed models", "to": "A first limitation of our approach is the fully supervised setting where the models are trained using images with detected ground truth objects and parsed text-which aims at keeping the design clean in this first study on implicit spatial templates. Notice however that methods to automatically parse images and text exist. In future work, we aim at implementing our approach in a weakly supervised setting. A second limitation is the 2D spatial treatment of the actual 3D world. It is worth noting however, that our models (PIX and REG) and setting trivially generalize to 3D if appropriate data are available.", "weight": 0.87}, {"from": "Your Visual Research Paper", "to": "Experimental setup", "weight": 0.97}, {"from": "Experimental setup", "to": "Spatial understanding is a fundamental problem with widereaching real-world applications. The representation of spatial knowledge is often modeled with spatial templates, i.e., regions of acceptability of two objects under an explicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with prior work that restricts spatial templates to explicit spatial prepositions (e.g., \"glass on table\"), here we extend this concept to implicit spatial language, i.e., those relationships (generally actions) for which the spatial arrangement of the objects is only implicitly implied (e.g., \"man riding horse\"). In contrast with explicit relationships, predicting spatial arrangements from implicit spatial language requires significant common sense spatial understanding. Here, we introduce the task of predicting spatial templates for two objects under a relationship, which can be seen as a spatial question-answering task with a (2D) continuous output (\"where is the man w.r.t. a horse when the man is walking the horse?\"). We present two simple neural-based models that leverage annotated images and structured text to learn this task. The good performance of these models reveals that spatial locations are to a large extent predictable from implicit spatial language. Crucially, the models attain similar performance in a challenging generalized setting, where the object-relation-object combinations (e.g.,\"man walking dog\") have never been seen before. Next, we go one step further by presenting the models with unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging word embeddings enables the models to output accurate spatial predictions, proving that the models acquire solid common sense spatial knowledge allowing for such generalization.", "weight": 0.87}, {"from": "Experimental setup", "to": "To provide machines with common sense is one of the major long term goals of artificial intelligence. Common sense knowledge regards knowledge that humans have acquired through a lifetime of experiences. It is crucial in language understanding because a lot of content needed for correct understanding is not expressed explicitly but resides in the mind of communicator and audience. In addition, humans rely on their common sense knowledge when performing a variety of tasks including interpreting images, navigation and reasoning, to name a few. Representing and understanding spatial knowledge are in fact imperative for any agent (human, animal or robot) that navigates in a physical world. In this paper, we are interested in acquiring spatial commonsense knowledge from language paired with visual data.", "weight": 0.87}, {"from": "Experimental setup", "to": "Computational and cognitive models often handle spatial representations as spatial templates or regions of acceptability for two objects under an explicit (a.k.a. deictic) spatial preposition such as \"on\", \"below\" or \"left\" ", "weight": 0.87}, {"from": "Experimental setup", "to": "Predicting spatial templates for implicit relationships is notably more challenging than for explicit relationships. Firstly, whereas there are only a few tens of explicit spatial prepositions, there exist thousands of actions, entailing thus a drastic increase in the sparsity of (object 1 , relationship, object 2 ) combinations. Secondly, the complexity of the task radically increases in implicit language. More precisely, while explicit spatial prepositions 1 are highly deterministic about the spatial arrangements (e.g., (object 1 , below, object 2 ) unequivocally implies that object 1 is relatively lower than object 2 ), actions generally are not. E.g., the relative spatial configuration of \"man\" and the object is clearly distinct in (man, pulling, kite) than in (man, pulling, luggage) yet the action is the same. Contrarily, other relationships such as \"jumping\" are highly informative about the spatial template, i.e., in (object 1 , jumping, object 2 ), object 2 is in a lower position than object 1 . Hence, unlike explicit relationships, predicting spatial layouts from implicit spatial language requires spatial common sense knowledge about the objects, actions and their interaction, which suggests the need of learning to compose the triplet (Subject, Relationship, Object) as a whole instead of learning a template for each Relationship.", "weight": 0.87}, {"from": "Experimental setup", "to": "To systematically study these questions, we propose the task of predicting the relative spatial locations of two objects given a structured text input (Subject, Relationship, Object). We introduce two simple neural-based models trained from annotated images that successfully address the two challenges of implicit spatial language discussed above. Our quantitative evaluation reveals that spatial templates can be reliably predicted from implicit spatial language-as accurately as from explicit spatial language. We also show that our models generalize well to templates of unseen combinations, e.g., predicting (man, riding, elephant) without having been exposed to such scene before, tackling thus the challenge of sparsity. Furthermore, by leveraging word embeddings, the models can correctly generalize to spatial templates with unseen words, e.g., predicting (man, riding, elephant) without having ever seen an \"elephant\" before. Since word embeddings capture attributes of objects ", "weight": 0.87}, {"from": "Experimental setup", "to": "The rest of the paper is organized as follows. In Sect. 2 we review related research. In Sect. 3 we first introduce the task of predicting spatial templates and then present two simple neural models. Then, in Sect. 4, we describe our experimental setup. In Sect. 5 we present and discuss our results. Finally, in Sect. 6 we summarize the contributions of this article.", "weight": 0.87}, {"from": "Experimental setup", "to": "Spatial processing has drawn significant attention from the cognitive ", "weight": 0.87}, {"from": "Experimental setup", "to": "Spatial templates. Earlier approaches have predominantly considered rule-based spatial representations ", "weight": 0.87}, {"from": "Experimental setup", "to": "Additionally, while they build a spatial template for each (explicit) Relationship, we build a template for each (Subject, Relationship, Object) combination, allowing the template to be determined by the interaction/composition of the Subject, Relationship and Object instead of the Relationship alone. Additionally, the model from ", "weight": 0.87}, {"from": "Experimental setup", "to": "Leveraging spatial knowledge in tasks. It has been shown that knowledge of the spatial structure in images improves the task of image captioning ", "weight": 0.87}, {"from": "Experimental setup", "to": "Common sense spatial knowledge. ", "weight": 0.87}, {"from": "Experimental setup", "to": "Image generation. Although models that generate images from text exist (e.g., DRAW model ", "weight": 0.87}, {"from": "Experimental setup", "to": "As discussed, spatial knowledge can improve a wide range of tasks ", "weight": 0.87}, {"from": "Experimental setup", "to": "Obj.", "weight": 0.87}, {"from": "Experimental setup", "to": "Subj.", "weight": 0.87}, {"from": "Experimental setup", "to": "x y Elliott and Keller 2013). This suggests that the predictions of our models can be used as spatial common sense input for methods that rely on good spatial priors. Additionally, existing methods require the spatial information to be present in the data and lack the capacity to extrapolate/generalize. Thus, in this paper we focus, first, on showing that generalizing spatial arrangements to unseen objects and object-relation-object combinations is possible and, second, on ensuring that the predicted templates are accurate by performing several quantitative and qualitative evaluations.", "weight": 0.87}, {"from": "Experimental setup", "to": "3 Proposed task and model", "weight": 0.87}, {"from": "Experimental setup", "to": "To learn spatial templates, we propose the task of predicting the 2D relative spatial arrangement of two objects under a relationship given a structured text input of the form (Subject, Relationship, Object)-henceforth abbreviated as (S, R, O). Let us denote the 2D coordinates of the center (\"c\") of the Object\u0027s box as", "weight": 0.87}, {"from": "Experimental setup", "to": "y \u2208 R are the horizontal and vertical components respectively. Let", "weight": 0.87}, {"from": "Experimental setup", "to": "x , O b y ] \u2208 R 2 be half of the width (O b x ) and half of the height (O b y ) of the Object\u0027s box (\"b\"). We employ a similar notation for the Subject (S c , S b ), and model predictions are denoted with a hat O c , O b . The task consists in predicting the Object\u0027s location and size", "weight": 0.87}, {"from": "Experimental setup", "to": "given the structured text input (S, R, O) and the location S c and size S b of the Subject ", "weight": 0.87}, {"from": "Experimental setup", "to": "2 Crucially, we notice that knowing the Subject\u0027s coordinates is not a requirement for generating templates (but only for evaluating against the ground truth) since inputting arbitrary coordinates (e.g., S c =[0.5, 0.5]) still enables visualizing relative object locations. Additionally, we input the Subject\u0027s size in order to provide a reference size to the model. However, we find that without this input, the model still learns to predict an \"average size\" for each Object.", "weight": 0.87}, {"from": "Experimental setup", "to": "Hence, the goal is to answer common sense spatial questions such as \"if a man is feeding a horse, where is the man relative to the horse?\" or \"where would a child wear her shoes?\"", "weight": 0.87}, {"from": "Experimental setup", "to": "To build a mapping from the input to the output in our task (Sect. 3.1) we propose two simple neural models ( ", "weight": 0.87}, {"from": "Experimental setup", "to": "are one-hot encodings of S, R, O (a.k.a. one-of-k encoding, i.e., a sparse vector with 0 everywhere except for a 1 at the position of the k-th word) and", "weight": 0.87}, {"from": "Experimental setup", "to": "This layer represents objects and relationships as continuous features, enabling thus to introduce external knowledge of unseen objects as features. The embeddings are then concatenated with the Subject center S c and size", "weight": 0.87}, {"from": "Experimental setup", "to": "which is inputted to a stack of hidden layers that compose S, R and O into a joint hidden representation z h :", "weight": 0.87}, {"from": "Experimental setup", "to": "where f (\u2022) is an element-wise non-linear function and W h and b h are the weight matrix and bias respectively. 3 (ii) Output and loss. We consider two different possibilities for the output\u0177 that follows immediately after the last layer: ", "weight": 0.87}, {"from": "Experimental setup", "to": "These models are conceptually different and have different capabilities. While the REG model outputs \"crisp\" pointwise predictions, PIX can model more diffuse spatial templates where the location of the object has more variability, e.g., in (man, flying, kite) the \"kite\" can easily move around. Notice that in contrast with convolutional neural networks (CNNs) our approach does not make use of the image pixels ( ", "weight": 0.87}, {"from": "Experimental setup", "to": "We employ a 10-fold cross-validation (CV) setting. Data are randomly split into 10 disjoint parts and 10% is employed for testing and 90% for training, repeating this for each of the 10 folds. Reported results are averages over the 10 folds.", "weight": 0.87}, {"from": "Experimental setup", "to": "We use the Visual Genome dataset ", "weight": 0.87}, {"from": "Experimental setup", "to": "dog, catches, frisbee boy, feeds, giraffe man, throws, frisbee cat, wears glasses ", "weight": 0.87}, {"from": "Experimental setup", "to": "We consider the following subsets of the Visual Genome data to evaluate performance. (i) Raw data: Simply the unfiltered instances from the Visual Genome data (Sect. 4.1). This set contains a substantial proportion of meaningless (e.g., (nose, almost, touching)) and irrelevant (e.g., (sign, says, gate 2)) instances. (ii) Generalized Triplets: We pick at random 100 combinations (S, R, O) among the 1,000 most frequent implicit combinations in Visual Genome. This yields \u223c25K instances such as (person, holding, racket), (man, flying, kite), etc. 4 (iii) Generalized Words: We randomly choose 25 objects (e.g., \"woman\", \"apple\", etc.) 5 among the 100 most frequent objects in Visual Genome and take all the instances (\u223c130K) that contain any of these words. For example, since \"apple\" is in our list, e.g., (cat, sniffing, apple) is kept. ", "weight": 0.87}, {"from": "Experimental setup", "to": "When enforcing generalization conditions in our experiments, all combinations from sets (ii) and (iii) are removed from the training data to prevent the model from seeing them. 7 Even without imposing generalization conditions (or when testing with Raw data), reported results are always on unseen instances-yet the combinations (S, R, O) may have been seen during training (e.g., in different images). All sets above contain exclusively implicit spatial language, although an analogous version of the Raw set where R are explicit spatial prepositions is also considered in our experiments.", "weight": 0.87}, {"from": "Experimental setup", "to": "The coordinates of bounding boxes in the images are normalized by the width and height of the image. Thus, S c , O c \u2208 [0, 1] 2 . Additionally, we notice that the distinction between left and right is arbitrary regarding the semantics of the image ", "weight": 0.87}, {"from": "Experimental setup", "to": "The REG model directly outputs Object coordinates, while PIX outputs 2D heatmaps. We however enable evaluating the PIX model with regression/classification metrics by taking the point of maximum activation (or their average, if there are many) as the Object center O c . The predicted Object size O b is not estimated. We use the following performance metrics.", "weight": 0.87}, {"from": "Experimental setup", "to": "(A) Intersection over Union ", "weight": 0.87}, {"from": "Experimental setup", "to": "where B O and B O are the predicted and ground truth Object bounding boxes, respectively. If the IoU is larger than 50%, the prediction is counted as correct. It must be noted that our setting is not comparable to object detection (nor our results) since we do not employ the image as input (but text) and thus we cannot leverage the pixels to predict the Object\u0027s location.", "weight": 0.87}, {"from": "Experimental setup", "to": "(B) Regression. We consider standard regression metrics. ", "weight": 0.87}, {"from": "Experimental setup", "to": "(D) Pixel (macro) accuracy. The IoU on pixels is equivalent to binary pixel accuracy. However, this is not a good measure here, where class 1 (Object box) comprises, on average, only 5% of the pixels. Thus, a constant prediction of zeros everywhere would obtain 95% accuracy. Hence, we consider macro-averaged pixel accuracy, a.k.a. mean IoU (mIoU) ", "weight": 0.87}, {"from": "Experimental setup", "to": "We use 300-dimensional GloVe word embeddings (Pennington, Socher, and Manning 2014) pre-trained on the Common Crawl corpus (consisting of 840B-tokens), which we obtain from the authors\u0027 website. 8", "weight": 0.87}, {"from": "Experimental setup", "to": "Our experiments are implemented in Python 2.7 and we use Keras deep learning framework for our models ", "weight": 0.87}, {"from": "Experimental setup", "to": "We consider the evaluation sets from Sect. 4.2 and the following variations of PIX and REG models (Sect. 3.2). The subindex EMB denotes a model that employs GloVe embeddings and RND a model with embeddings randomly drawn from a dimension-wise normal distribution of mean (\u03bc) and standard deviation (\u03c3) equal to those of the GloVe embeddings, preserving the original dimensionality (d=300). A third type employs one-hot vectors (1H). We additionally  consider a control method (ctrl) that outputs random normal predictions of \u03bc and \u03c3 equal to the dimension-wise mean and standard deviation of the training targets. We test statistical significance with a Friedman rank test and post hoc Nemenyi tests on the results of the 10 folds. We indicate with an asterisk * in the tables when a method is significantly better than the rest (p \u003c 0.01) within the same model (PIX or REG). ", "weight": 0.87}, {"from": "Experimental setup", "to": "To ensure that model predictions are meaningful and interpretable, we further validate the quantitative results above with a qualitative evaluation. Notice that all plots in ", "weight": 0.87}, {"from": "Experimental setup", "to": "We study how the weights of the model provide insight into the spatial properties of words. To obtain more interpretable weights, we learn a REG 9 model without hidden layers, resulting in only an embedding layer followed by a linear output layer\u0177 = W out u + b out , where", "weight": 0.87}, {"from": "Experimental setup", "to": "By using one-hot encodings w S , w R , w O , the concatenation layer u becomes of size |V S | + |V R | + |V O | + 2 + 2. E.g., if \"wearing\" has one-hot index j in the Relationships\u0027 vocabulary (V R ), its index in the concatenation layer is |V S | + j. The product W out u is a 4-dimensional vector, where its i-th component is the product of the i-th row of W out with the vector u. Thus, the component |V S |+j of the i-th row of W out gives us the influence of the j-th relationship (i.e., \"wearing\") on the i-th dimension of the output\u0177 =   ", "weight": 0.87}, {"from": "Experimental setup", "to": "Overall, this paper provides insight into the fundamental differences between implicit and explicit spatial language and extends the concept of spatial templates to implicit spatial language, the understanding of which requires common sense spatial knowledge about objects and actions. We define the task of predicting relative spatial arrangements of two objects under a relationship and present two embedding-based neural models that attain promising performance, proving that spatial templates can be accurately predicted from implicit spatial language. Remarkably, our models generalize well, predicting correctly unseen (generalized) object-relationshipobject combinations. Furthermore, the acquired common sense spatial knowledge-aided with word embeddingsallows the model to correctly predict templates for unseen words. Finally, we show that the weights of the model provide great insight into the spatial connotations of words.", "weight": 0.87}, {"from": "Experimental setup", "to": "A first limitation of our approach is the fully supervised setting where the models are trained using images with detected ground truth objects and parsed text-which aims at keeping the design clean in this first study on implicit spatial templates. Notice however that methods to automatically parse images and text exist. In future work, we aim at implementing our approach in a weakly supervised setting. A second limitation is the 2D spatial treatment of the actual 3D world. It is worth noting however, that our models (PIX and REG) and setting trivially generalize to 3D if appropriate data are available.", "weight": 0.87}, {"from": "Your Visual Research Paper", "to": "Visual Genome data set", "weight": 0.97}, {"from": "Visual Genome data set", "to": "Spatial understanding is a fundamental problem with widereaching real-world applications. The representation of spatial knowledge is often modeled with spatial templates, i.e., regions of acceptability of two objects under an explicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with prior work that restricts spatial templates to explicit spatial prepositions (e.g., \"glass on table\"), here we extend this concept to implicit spatial language, i.e., those relationships (generally actions) for which the spatial arrangement of the objects is only implicitly implied (e.g., \"man riding horse\"). In contrast with explicit relationships, predicting spatial arrangements from implicit spatial language requires significant common sense spatial understanding. Here, we introduce the task of predicting spatial templates for two objects under a relationship, which can be seen as a spatial question-answering task with a (2D) continuous output (\"where is the man w.r.t. a horse when the man is walking the horse?\"). We present two simple neural-based models that leverage annotated images and structured text to learn this task. The good performance of these models reveals that spatial locations are to a large extent predictable from implicit spatial language. Crucially, the models attain similar performance in a challenging generalized setting, where the object-relation-object combinations (e.g.,\"man walking dog\") have never been seen before. Next, we go one step further by presenting the models with unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging word embeddings enables the models to output accurate spatial predictions, proving that the models acquire solid common sense spatial knowledge allowing for such generalization.", "weight": 0.87}, {"from": "Visual Genome data set", "to": "To provide machines with common sense is one of the major long term goals of artificial intelligence. Common sense knowledge regards knowledge that humans have acquired through a lifetime of experiences. It is crucial in language understanding because a lot of content needed for correct understanding is not expressed explicitly but resides in the mind of communicator and audience. In addition, humans rely on their common sense knowledge when performing a variety of tasks including interpreting images, navigation and reasoning, to name a few. Representing and understanding spatial knowledge are in fact imperative for any agent (human, animal or robot) that navigates in a physical world. In this paper, we are interested in acquiring spatial commonsense knowledge from language paired with visual data.", "weight": 0.87}, {"from": "Visual Genome data set", "to": "Computational and cognitive models often handle spatial representations as spatial templates or regions of acceptability for two objects under an explicit (a.k.a. deictic) spatial preposition such as \"on\", \"below\" or \"left\" ", "weight": 0.87}, {"from": "Visual Genome data set", "to": "Predicting spatial templates for implicit relationships is notably more challenging than for explicit relationships. Firstly, whereas there are only a few tens of explicit spatial prepositions, there exist thousands of actions, entailing thus a drastic increase in the sparsity of (object 1 , relationship, object 2 ) combinations. Secondly, the complexity of the task radically increases in implicit language. More precisely, while explicit spatial prepositions 1 are highly deterministic about the spatial arrangements (e.g., (object 1 , below, object 2 ) unequivocally implies that object 1 is relatively lower than object 2 ), actions generally are not. E.g., the relative spatial configuration of \"man\" and the object is clearly distinct in (man, pulling, kite) than in (man, pulling, luggage) yet the action is the same. Contrarily, other relationships such as \"jumping\" are highly informative about the spatial template, i.e., in (object 1 , jumping, object 2 ), object 2 is in a lower position than object 1 . Hence, unlike explicit relationships, predicting spatial layouts from implicit spatial language requires spatial common sense knowledge about the objects, actions and their interaction, which suggests the need of learning to compose the triplet (Subject, Relationship, Object) as a whole instead of learning a template for each Relationship.", "weight": 0.87}, {"from": "Visual Genome data set", "to": "To systematically study these questions, we propose the task of predicting the relative spatial locations of two objects given a structured text input (Subject, Relationship, Object). We introduce two simple neural-based models trained from annotated images that successfully address the two challenges of implicit spatial language discussed above. Our quantitative evaluation reveals that spatial templates can be reliably predicted from implicit spatial language-as accurately as from explicit spatial language. We also show that our models generalize well to templates of unseen combinations, e.g., predicting (man, riding, elephant) without having been exposed to such scene before, tackling thus the challenge of sparsity. Furthermore, by leveraging word embeddings, the models can correctly generalize to spatial templates with unseen words, e.g., predicting (man, riding, elephant) without having ever seen an \"elephant\" before. Since word embeddings capture attributes of objects ", "weight": 0.87}, {"from": "Visual Genome data set", "to": "The rest of the paper is organized as follows. In Sect. 2 we review related research. In Sect. 3 we first introduce the task of predicting spatial templates and then present two simple neural models. Then, in Sect. 4, we describe our experimental setup. In Sect. 5 we present and discuss our results. Finally, in Sect. 6 we summarize the contributions of this article.", "weight": 0.87}, {"from": "Visual Genome data set", "to": "Spatial processing has drawn significant attention from the cognitive ", "weight": 0.87}, {"from": "Visual Genome data set", "to": "Spatial templates. Earlier approaches have predominantly considered rule-based spatial representations ", "weight": 0.87}, {"from": "Visual Genome data set", "to": "Additionally, while they build a spatial template for each (explicit) Relationship, we build a template for each (Subject, Relationship, Object) combination, allowing the template to be determined by the interaction/composition of the Subject, Relationship and Object instead of the Relationship alone. Additionally, the model from ", "weight": 0.87}, {"from": "Visual Genome data set", "to": "Leveraging spatial knowledge in tasks. It has been shown that knowledge of the spatial structure in images improves the task of image captioning ", "weight": 0.87}, {"from": "Visual Genome data set", "to": "Common sense spatial knowledge. ", "weight": 0.87}, {"from": "Visual Genome data set", "to": "Image generation. Although models that generate images from text exist (e.g., DRAW model ", "weight": 0.87}, {"from": "Visual Genome data set", "to": "As discussed, spatial knowledge can improve a wide range of tasks ", "weight": 0.87}, {"from": "Visual Genome data set", "to": "Obj.", "weight": 0.87}, {"from": "Visual Genome data set", "to": "Subj.", "weight": 0.87}, {"from": "Visual Genome data set", "to": "x y Elliott and Keller 2013). This suggests that the predictions of our models can be used as spatial common sense input for methods that rely on good spatial priors. Additionally, existing methods require the spatial information to be present in the data and lack the capacity to extrapolate/generalize. Thus, in this paper we focus, first, on showing that generalizing spatial arrangements to unseen objects and object-relation-object combinations is possible and, second, on ensuring that the predicted templates are accurate by performing several quantitative and qualitative evaluations.", "weight": 0.87}, {"from": "Visual Genome data set", "to": "3 Proposed task and model", "weight": 0.87}, {"from": "Visual Genome data set", "to": "To learn spatial templates, we propose the task of predicting the 2D relative spatial arrangement of two objects under a relationship given a structured text input of the form (Subject, Relationship, Object)-henceforth abbreviated as (S, R, O). Let us denote the 2D coordinates of the center (\"c\") of the Object\u0027s box as", "weight": 0.87}, {"from": "Visual Genome data set", "to": "y \u2208 R are the horizontal and vertical components respectively. Let", "weight": 0.87}, {"from": "Visual Genome data set", "to": "x , O b y ] \u2208 R 2 be half of the width (O b x ) and half of the height (O b y ) of the Object\u0027s box (\"b\"). We employ a similar notation for the Subject (S c , S b ), and model predictions are denoted with a hat O c , O b . The task consists in predicting the Object\u0027s location and size", "weight": 0.87}, {"from": "Visual Genome data set", "to": "given the structured text input (S, R, O) and the location S c and size S b of the Subject ", "weight": 0.87}, {"from": "Visual Genome data set", "to": "2 Crucially, we notice that knowing the Subject\u0027s coordinates is not a requirement for generating templates (but only for evaluating against the ground truth) since inputting arbitrary coordinates (e.g., S c =[0.5, 0.5]) still enables visualizing relative object locations. Additionally, we input the Subject\u0027s size in order to provide a reference size to the model. However, we find that without this input, the model still learns to predict an \"average size\" for each Object.", "weight": 0.87}, {"from": "Visual Genome data set", "to": "Hence, the goal is to answer common sense spatial questions such as \"if a man is feeding a horse, where is the man relative to the horse?\" or \"where would a child wear her shoes?\"", "weight": 0.87}, {"from": "Visual Genome data set", "to": "To build a mapping from the input to the output in our task (Sect. 3.1) we propose two simple neural models ( ", "weight": 0.87}, {"from": "Visual Genome data set", "to": "are one-hot encodings of S, R, O (a.k.a. one-of-k encoding, i.e., a sparse vector with 0 everywhere except for a 1 at the position of the k-th word) and", "weight": 0.87}, {"from": "Visual Genome data set", "to": "This layer represents objects and relationships as continuous features, enabling thus to introduce external knowledge of unseen objects as features. The embeddings are then concatenated with the Subject center S c and size", "weight": 0.87}, {"from": "Visual Genome data set", "to": "which is inputted to a stack of hidden layers that compose S, R and O into a joint hidden representation z h :", "weight": 0.87}, {"from": "Visual Genome data set", "to": "where f (\u2022) is an element-wise non-linear function and W h and b h are the weight matrix and bias respectively. 3 (ii) Output and loss. We consider two different possibilities for the output\u0177 that follows immediately after the last layer: ", "weight": 0.87}, {"from": "Visual Genome data set", "to": "These models are conceptually different and have different capabilities. While the REG model outputs \"crisp\" pointwise predictions, PIX can model more diffuse spatial templates where the location of the object has more variability, e.g., in (man, flying, kite) the \"kite\" can easily move around. Notice that in contrast with convolutional neural networks (CNNs) our approach does not make use of the image pixels ( ", "weight": 0.87}, {"from": "Visual Genome data set", "to": "We employ a 10-fold cross-validation (CV) setting. Data are randomly split into 10 disjoint parts and 10% is employed for testing and 90% for training, repeating this for each of the 10 folds. Reported results are averages over the 10 folds.", "weight": 0.87}, {"from": "Visual Genome data set", "to": "We use the Visual Genome dataset ", "weight": 0.87}, {"from": "Visual Genome data set", "to": "dog, catches, frisbee boy, feeds, giraffe man, throws, frisbee cat, wears glasses ", "weight": 0.87}, {"from": "Visual Genome data set", "to": "We consider the following subsets of the Visual Genome data to evaluate performance. (i) Raw data: Simply the unfiltered instances from the Visual Genome data (Sect. 4.1). This set contains a substantial proportion of meaningless (e.g., (nose, almost, touching)) and irrelevant (e.g., (sign, says, gate 2)) instances. (ii) Generalized Triplets: We pick at random 100 combinations (S, R, O) among the 1,000 most frequent implicit combinations in Visual Genome. This yields \u223c25K instances such as (person, holding, racket), (man, flying, kite), etc. 4 (iii) Generalized Words: We randomly choose 25 objects (e.g., \"woman\", \"apple\", etc.) 5 among the 100 most frequent objects in Visual Genome and take all the instances (\u223c130K) that contain any of these words. For example, since \"apple\" is in our list, e.g., (cat, sniffing, apple) is kept. ", "weight": 0.87}, {"from": "Visual Genome data set", "to": "When enforcing generalization conditions in our experiments, all combinations from sets (ii) and (iii) are removed from the training data to prevent the model from seeing them. 7 Even without imposing generalization conditions (or when testing with Raw data), reported results are always on unseen instances-yet the combinations (S, R, O) may have been seen during training (e.g., in different images). All sets above contain exclusively implicit spatial language, although an analogous version of the Raw set where R are explicit spatial prepositions is also considered in our experiments.", "weight": 0.87}, {"from": "Visual Genome data set", "to": "The coordinates of bounding boxes in the images are normalized by the width and height of the image. Thus, S c , O c \u2208 [0, 1] 2 . Additionally, we notice that the distinction between left and right is arbitrary regarding the semantics of the image ", "weight": 0.87}, {"from": "Visual Genome data set", "to": "The REG model directly outputs Object coordinates, while PIX outputs 2D heatmaps. We however enable evaluating the PIX model with regression/classification metrics by taking the point of maximum activation (or their average, if there are many) as the Object center O c . The predicted Object size O b is not estimated. We use the following performance metrics.", "weight": 0.87}, {"from": "Visual Genome data set", "to": "(A) Intersection over Union ", "weight": 0.87}, {"from": "Visual Genome data set", "to": "where B O and B O are the predicted and ground truth Object bounding boxes, respectively. If the IoU is larger than 50%, the prediction is counted as correct. It must be noted that our setting is not comparable to object detection (nor our results) since we do not employ the image as input (but text) and thus we cannot leverage the pixels to predict the Object\u0027s location.", "weight": 0.87}, {"from": "Visual Genome data set", "to": "(B) Regression. We consider standard regression metrics. ", "weight": 0.87}, {"from": "Visual Genome data set", "to": "(D) Pixel (macro) accuracy. The IoU on pixels is equivalent to binary pixel accuracy. However, this is not a good measure here, where class 1 (Object box) comprises, on average, only 5% of the pixels. Thus, a constant prediction of zeros everywhere would obtain 95% accuracy. Hence, we consider macro-averaged pixel accuracy, a.k.a. mean IoU (mIoU) ", "weight": 0.87}, {"from": "Visual Genome data set", "to": "We use 300-dimensional GloVe word embeddings (Pennington, Socher, and Manning 2014) pre-trained on the Common Crawl corpus (consisting of 840B-tokens), which we obtain from the authors\u0027 website. 8", "weight": 0.87}, {"from": "Visual Genome data set", "to": "Our experiments are implemented in Python 2.7 and we use Keras deep learning framework for our models ", "weight": 0.87}, {"from": "Visual Genome data set", "to": "We consider the evaluation sets from Sect. 4.2 and the following variations of PIX and REG models (Sect. 3.2). The subindex EMB denotes a model that employs GloVe embeddings and RND a model with embeddings randomly drawn from a dimension-wise normal distribution of mean (\u03bc) and standard deviation (\u03c3) equal to those of the GloVe embeddings, preserving the original dimensionality (d=300). A third type employs one-hot vectors (1H). We additionally  consider a control method (ctrl) that outputs random normal predictions of \u03bc and \u03c3 equal to the dimension-wise mean and standard deviation of the training targets. We test statistical significance with a Friedman rank test and post hoc Nemenyi tests on the results of the 10 folds. We indicate with an asterisk * in the tables when a method is significantly better than the rest (p \u003c 0.01) within the same model (PIX or REG). ", "weight": 0.87}, {"from": "Visual Genome data set", "to": "To ensure that model predictions are meaningful and interpretable, we further validate the quantitative results above with a qualitative evaluation. Notice that all plots in ", "weight": 0.87}, {"from": "Visual Genome data set", "to": "We study how the weights of the model provide insight into the spatial properties of words. To obtain more interpretable weights, we learn a REG 9 model without hidden layers, resulting in only an embedding layer followed by a linear output layer\u0177 = W out u + b out , where", "weight": 0.87}, {"from": "Visual Genome data set", "to": "By using one-hot encodings w S , w R , w O , the concatenation layer u becomes of size |V S | + |V R | + |V O | + 2 + 2. E.g., if \"wearing\" has one-hot index j in the Relationships\u0027 vocabulary (V R ), its index in the concatenation layer is |V S | + j. The product W out u is a 4-dimensional vector, where its i-th component is the product of the i-th row of W out with the vector u. Thus, the component |V S |+j of the i-th row of W out gives us the influence of the j-th relationship (i.e., \"wearing\") on the i-th dimension of the output\u0177 =   ", "weight": 0.87}, {"from": "Visual Genome data set", "to": "Overall, this paper provides insight into the fundamental differences between implicit and explicit spatial language and extends the concept of spatial templates to implicit spatial language, the understanding of which requires common sense spatial knowledge about objects and actions. We define the task of predicting relative spatial arrangements of two objects under a relationship and present two embedding-based neural models that attain promising performance, proving that spatial templates can be accurately predicted from implicit spatial language. Remarkably, our models generalize well, predicting correctly unseen (generalized) object-relationshipobject combinations. Furthermore, the acquired common sense spatial knowledge-aided with word embeddingsallows the model to correctly predict templates for unseen words. Finally, we show that the weights of the model provide great insight into the spatial connotations of words.", "weight": 0.87}, {"from": "Visual Genome data set", "to": "A first limitation of our approach is the fully supervised setting where the models are trained using images with detected ground truth objects and parsed text-which aims at keeping the design clean in this first study on implicit spatial templates. Notice however that methods to automatically parse images and text exist. In future work, we aim at implementing our approach in a weakly supervised setting. A second limitation is the 2D spatial treatment of the actual 3D world. It is worth noting however, that our models (PIX and REG) and setting trivially generalize to 3D if appropriate data are available.", "weight": 0.87}, {"from": "Your Visual Research Paper", "to": "Evaluation sets", "weight": 0.97}, {"from": "Evaluation sets", "to": "Spatial understanding is a fundamental problem with widereaching real-world applications. The representation of spatial knowledge is often modeled with spatial templates, i.e., regions of acceptability of two objects under an explicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with prior work that restricts spatial templates to explicit spatial prepositions (e.g., \"glass on table\"), here we extend this concept to implicit spatial language, i.e., those relationships (generally actions) for which the spatial arrangement of the objects is only implicitly implied (e.g., \"man riding horse\"). In contrast with explicit relationships, predicting spatial arrangements from implicit spatial language requires significant common sense spatial understanding. Here, we introduce the task of predicting spatial templates for two objects under a relationship, which can be seen as a spatial question-answering task with a (2D) continuous output (\"where is the man w.r.t. a horse when the man is walking the horse?\"). We present two simple neural-based models that leverage annotated images and structured text to learn this task. The good performance of these models reveals that spatial locations are to a large extent predictable from implicit spatial language. Crucially, the models attain similar performance in a challenging generalized setting, where the object-relation-object combinations (e.g.,\"man walking dog\") have never been seen before. Next, we go one step further by presenting the models with unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging word embeddings enables the models to output accurate spatial predictions, proving that the models acquire solid common sense spatial knowledge allowing for such generalization.", "weight": 0.87}, {"from": "Evaluation sets", "to": "To provide machines with common sense is one of the major long term goals of artificial intelligence. Common sense knowledge regards knowledge that humans have acquired through a lifetime of experiences. It is crucial in language understanding because a lot of content needed for correct understanding is not expressed explicitly but resides in the mind of communicator and audience. In addition, humans rely on their common sense knowledge when performing a variety of tasks including interpreting images, navigation and reasoning, to name a few. Representing and understanding spatial knowledge are in fact imperative for any agent (human, animal or robot) that navigates in a physical world. In this paper, we are interested in acquiring spatial commonsense knowledge from language paired with visual data.", "weight": 0.87}, {"from": "Evaluation sets", "to": "Computational and cognitive models often handle spatial representations as spatial templates or regions of acceptability for two objects under an explicit (a.k.a. deictic) spatial preposition such as \"on\", \"below\" or \"left\" ", "weight": 0.87}, {"from": "Evaluation sets", "to": "Predicting spatial templates for implicit relationships is notably more challenging than for explicit relationships. Firstly, whereas there are only a few tens of explicit spatial prepositions, there exist thousands of actions, entailing thus a drastic increase in the sparsity of (object 1 , relationship, object 2 ) combinations. Secondly, the complexity of the task radically increases in implicit language. More precisely, while explicit spatial prepositions 1 are highly deterministic about the spatial arrangements (e.g., (object 1 , below, object 2 ) unequivocally implies that object 1 is relatively lower than object 2 ), actions generally are not. E.g., the relative spatial configuration of \"man\" and the object is clearly distinct in (man, pulling, kite) than in (man, pulling, luggage) yet the action is the same. Contrarily, other relationships such as \"jumping\" are highly informative about the spatial template, i.e., in (object 1 , jumping, object 2 ), object 2 is in a lower position than object 1 . Hence, unlike explicit relationships, predicting spatial layouts from implicit spatial language requires spatial common sense knowledge about the objects, actions and their interaction, which suggests the need of learning to compose the triplet (Subject, Relationship, Object) as a whole instead of learning a template for each Relationship.", "weight": 0.87}, {"from": "Evaluation sets", "to": "To systematically study these questions, we propose the task of predicting the relative spatial locations of two objects given a structured text input (Subject, Relationship, Object). We introduce two simple neural-based models trained from annotated images that successfully address the two challenges of implicit spatial language discussed above. Our quantitative evaluation reveals that spatial templates can be reliably predicted from implicit spatial language-as accurately as from explicit spatial language. We also show that our models generalize well to templates of unseen combinations, e.g., predicting (man, riding, elephant) without having been exposed to such scene before, tackling thus the challenge of sparsity. Furthermore, by leveraging word embeddings, the models can correctly generalize to spatial templates with unseen words, e.g., predicting (man, riding, elephant) without having ever seen an \"elephant\" before. Since word embeddings capture attributes of objects ", "weight": 0.87}, {"from": "Evaluation sets", "to": "The rest of the paper is organized as follows. In Sect. 2 we review related research. In Sect. 3 we first introduce the task of predicting spatial templates and then present two simple neural models. Then, in Sect. 4, we describe our experimental setup. In Sect. 5 we present and discuss our results. Finally, in Sect. 6 we summarize the contributions of this article.", "weight": 0.87}, {"from": "Evaluation sets", "to": "Spatial processing has drawn significant attention from the cognitive ", "weight": 0.87}, {"from": "Evaluation sets", "to": "Spatial templates. Earlier approaches have predominantly considered rule-based spatial representations ", "weight": 0.87}, {"from": "Evaluation sets", "to": "Additionally, while they build a spatial template for each (explicit) Relationship, we build a template for each (Subject, Relationship, Object) combination, allowing the template to be determined by the interaction/composition of the Subject, Relationship and Object instead of the Relationship alone. Additionally, the model from ", "weight": 0.87}, {"from": "Evaluation sets", "to": "Leveraging spatial knowledge in tasks. It has been shown that knowledge of the spatial structure in images improves the task of image captioning ", "weight": 0.87}, {"from": "Evaluation sets", "to": "Common sense spatial knowledge. ", "weight": 0.87}, {"from": "Evaluation sets", "to": "Image generation. Although models that generate images from text exist (e.g., DRAW model ", "weight": 0.87}, {"from": "Evaluation sets", "to": "As discussed, spatial knowledge can improve a wide range of tasks ", "weight": 0.87}, {"from": "Evaluation sets", "to": "Obj.", "weight": 0.87}, {"from": "Evaluation sets", "to": "Subj.", "weight": 0.87}, {"from": "Evaluation sets", "to": "x y Elliott and Keller 2013). This suggests that the predictions of our models can be used as spatial common sense input for methods that rely on good spatial priors. Additionally, existing methods require the spatial information to be present in the data and lack the capacity to extrapolate/generalize. Thus, in this paper we focus, first, on showing that generalizing spatial arrangements to unseen objects and object-relation-object combinations is possible and, second, on ensuring that the predicted templates are accurate by performing several quantitative and qualitative evaluations.", "weight": 0.87}, {"from": "Evaluation sets", "to": "3 Proposed task and model", "weight": 0.87}, {"from": "Evaluation sets", "to": "To learn spatial templates, we propose the task of predicting the 2D relative spatial arrangement of two objects under a relationship given a structured text input of the form (Subject, Relationship, Object)-henceforth abbreviated as (S, R, O). Let us denote the 2D coordinates of the center (\"c\") of the Object\u0027s box as", "weight": 0.87}, {"from": "Evaluation sets", "to": "y \u2208 R are the horizontal and vertical components respectively. Let", "weight": 0.87}, {"from": "Evaluation sets", "to": "x , O b y ] \u2208 R 2 be half of the width (O b x ) and half of the height (O b y ) of the Object\u0027s box (\"b\"). We employ a similar notation for the Subject (S c , S b ), and model predictions are denoted with a hat O c , O b . The task consists in predicting the Object\u0027s location and size", "weight": 0.87}, {"from": "Evaluation sets", "to": "given the structured text input (S, R, O) and the location S c and size S b of the Subject ", "weight": 0.87}, {"from": "Evaluation sets", "to": "2 Crucially, we notice that knowing the Subject\u0027s coordinates is not a requirement for generating templates (but only for evaluating against the ground truth) since inputting arbitrary coordinates (e.g., S c =[0.5, 0.5]) still enables visualizing relative object locations. Additionally, we input the Subject\u0027s size in order to provide a reference size to the model. However, we find that without this input, the model still learns to predict an \"average size\" for each Object.", "weight": 0.87}, {"from": "Evaluation sets", "to": "Hence, the goal is to answer common sense spatial questions such as \"if a man is feeding a horse, where is the man relative to the horse?\" or \"where would a child wear her shoes?\"", "weight": 0.87}, {"from": "Evaluation sets", "to": "To build a mapping from the input to the output in our task (Sect. 3.1) we propose two simple neural models ( ", "weight": 0.87}, {"from": "Evaluation sets", "to": "are one-hot encodings of S, R, O (a.k.a. one-of-k encoding, i.e., a sparse vector with 0 everywhere except for a 1 at the position of the k-th word) and", "weight": 0.87}, {"from": "Evaluation sets", "to": "This layer represents objects and relationships as continuous features, enabling thus to introduce external knowledge of unseen objects as features. The embeddings are then concatenated with the Subject center S c and size", "weight": 0.87}, {"from": "Evaluation sets", "to": "which is inputted to a stack of hidden layers that compose S, R and O into a joint hidden representation z h :", "weight": 0.87}, {"from": "Evaluation sets", "to": "where f (\u2022) is an element-wise non-linear function and W h and b h are the weight matrix and bias respectively. 3 (ii) Output and loss. We consider two different possibilities for the output\u0177 that follows immediately after the last layer: ", "weight": 0.87}, {"from": "Evaluation sets", "to": "These models are conceptually different and have different capabilities. While the REG model outputs \"crisp\" pointwise predictions, PIX can model more diffuse spatial templates where the location of the object has more variability, e.g., in (man, flying, kite) the \"kite\" can easily move around. Notice that in contrast with convolutional neural networks (CNNs) our approach does not make use of the image pixels ( ", "weight": 0.87}, {"from": "Evaluation sets", "to": "We employ a 10-fold cross-validation (CV) setting. Data are randomly split into 10 disjoint parts and 10% is employed for testing and 90% for training, repeating this for each of the 10 folds. Reported results are averages over the 10 folds.", "weight": 0.87}, {"from": "Evaluation sets", "to": "We use the Visual Genome dataset ", "weight": 0.87}, {"from": "Evaluation sets", "to": "dog, catches, frisbee boy, feeds, giraffe man, throws, frisbee cat, wears glasses ", "weight": 0.87}, {"from": "Evaluation sets", "to": "We consider the following subsets of the Visual Genome data to evaluate performance. (i) Raw data: Simply the unfiltered instances from the Visual Genome data (Sect. 4.1). This set contains a substantial proportion of meaningless (e.g., (nose, almost, touching)) and irrelevant (e.g., (sign, says, gate 2)) instances. (ii) Generalized Triplets: We pick at random 100 combinations (S, R, O) among the 1,000 most frequent implicit combinations in Visual Genome. This yields \u223c25K instances such as (person, holding, racket), (man, flying, kite), etc. 4 (iii) Generalized Words: We randomly choose 25 objects (e.g., \"woman\", \"apple\", etc.) 5 among the 100 most frequent objects in Visual Genome and take all the instances (\u223c130K) that contain any of these words. For example, since \"apple\" is in our list, e.g., (cat, sniffing, apple) is kept. ", "weight": 0.87}, {"from": "Evaluation sets", "to": "When enforcing generalization conditions in our experiments, all combinations from sets (ii) and (iii) are removed from the training data to prevent the model from seeing them. 7 Even without imposing generalization conditions (or when testing with Raw data), reported results are always on unseen instances-yet the combinations (S, R, O) may have been seen during training (e.g., in different images). All sets above contain exclusively implicit spatial language, although an analogous version of the Raw set where R are explicit spatial prepositions is also considered in our experiments.", "weight": 0.87}, {"from": "Evaluation sets", "to": "The coordinates of bounding boxes in the images are normalized by the width and height of the image. Thus, S c , O c \u2208 [0, 1] 2 . Additionally, we notice that the distinction between left and right is arbitrary regarding the semantics of the image ", "weight": 0.87}, {"from": "Evaluation sets", "to": "The REG model directly outputs Object coordinates, while PIX outputs 2D heatmaps. We however enable evaluating the PIX model with regression/classification metrics by taking the point of maximum activation (or their average, if there are many) as the Object center O c . The predicted Object size O b is not estimated. We use the following performance metrics.", "weight": 0.87}, {"from": "Evaluation sets", "to": "(A) Intersection over Union ", "weight": 0.87}, {"from": "Evaluation sets", "to": "where B O and B O are the predicted and ground truth Object bounding boxes, respectively. If the IoU is larger than 50%, the prediction is counted as correct. It must be noted that our setting is not comparable to object detection (nor our results) since we do not employ the image as input (but text) and thus we cannot leverage the pixels to predict the Object\u0027s location.", "weight": 0.87}, {"from": "Evaluation sets", "to": "(B) Regression. We consider standard regression metrics. ", "weight": 0.87}, {"from": "Evaluation sets", "to": "(D) Pixel (macro) accuracy. The IoU on pixels is equivalent to binary pixel accuracy. However, this is not a good measure here, where class 1 (Object box) comprises, on average, only 5% of the pixels. Thus, a constant prediction of zeros everywhere would obtain 95% accuracy. Hence, we consider macro-averaged pixel accuracy, a.k.a. mean IoU (mIoU) ", "weight": 0.87}, {"from": "Evaluation sets", "to": "We use 300-dimensional GloVe word embeddings (Pennington, Socher, and Manning 2014) pre-trained on the Common Crawl corpus (consisting of 840B-tokens), which we obtain from the authors\u0027 website. 8", "weight": 0.87}, {"from": "Evaluation sets", "to": "Our experiments are implemented in Python 2.7 and we use Keras deep learning framework for our models ", "weight": 0.87}, {"from": "Evaluation sets", "to": "We consider the evaluation sets from Sect. 4.2 and the following variations of PIX and REG models (Sect. 3.2). The subindex EMB denotes a model that employs GloVe embeddings and RND a model with embeddings randomly drawn from a dimension-wise normal distribution of mean (\u03bc) and standard deviation (\u03c3) equal to those of the GloVe embeddings, preserving the original dimensionality (d=300). A third type employs one-hot vectors (1H). We additionally  consider a control method (ctrl) that outputs random normal predictions of \u03bc and \u03c3 equal to the dimension-wise mean and standard deviation of the training targets. We test statistical significance with a Friedman rank test and post hoc Nemenyi tests on the results of the 10 folds. We indicate with an asterisk * in the tables when a method is significantly better than the rest (p \u003c 0.01) within the same model (PIX or REG). ", "weight": 0.87}, {"from": "Evaluation sets", "to": "To ensure that model predictions are meaningful and interpretable, we further validate the quantitative results above with a qualitative evaluation. Notice that all plots in ", "weight": 0.87}, {"from": "Evaluation sets", "to": "We study how the weights of the model provide insight into the spatial properties of words. To obtain more interpretable weights, we learn a REG 9 model without hidden layers, resulting in only an embedding layer followed by a linear output layer\u0177 = W out u + b out , where", "weight": 0.87}, {"from": "Evaluation sets", "to": "By using one-hot encodings w S , w R , w O , the concatenation layer u becomes of size |V S | + |V R | + |V O | + 2 + 2. E.g., if \"wearing\" has one-hot index j in the Relationships\u0027 vocabulary (V R ), its index in the concatenation layer is |V S | + j. The product W out u is a 4-dimensional vector, where its i-th component is the product of the i-th row of W out with the vector u. Thus, the component |V S |+j of the i-th row of W out gives us the influence of the j-th relationship (i.e., \"wearing\") on the i-th dimension of the output\u0177 =   ", "weight": 0.87}, {"from": "Evaluation sets", "to": "Overall, this paper provides insight into the fundamental differences between implicit and explicit spatial language and extends the concept of spatial templates to implicit spatial language, the understanding of which requires common sense spatial knowledge about objects and actions. We define the task of predicting relative spatial arrangements of two objects under a relationship and present two embedding-based neural models that attain promising performance, proving that spatial templates can be accurately predicted from implicit spatial language. Remarkably, our models generalize well, predicting correctly unseen (generalized) object-relationshipobject combinations. Furthermore, the acquired common sense spatial knowledge-aided with word embeddingsallows the model to correctly predict templates for unseen words. Finally, we show that the weights of the model provide great insight into the spatial connotations of words.", "weight": 0.87}, {"from": "Evaluation sets", "to": "A first limitation of our approach is the fully supervised setting where the models are trained using images with detected ground truth objects and parsed text-which aims at keeping the design clean in this first study on implicit spatial templates. Notice however that methods to automatically parse images and text exist. In future work, we aim at implementing our approach in a weakly supervised setting. A second limitation is the 2D spatial treatment of the actual 3D world. It is worth noting however, that our models (PIX and REG) and setting trivially generalize to 3D if appropriate data are available.", "weight": 0.87}, {"from": "Your Visual Research Paper", "to": "Data pre-processing", "weight": 0.97}, {"from": "Data pre-processing", "to": "Spatial understanding is a fundamental problem with widereaching real-world applications. The representation of spatial knowledge is often modeled with spatial templates, i.e., regions of acceptability of two objects under an explicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with prior work that restricts spatial templates to explicit spatial prepositions (e.g., \"glass on table\"), here we extend this concept to implicit spatial language, i.e., those relationships (generally actions) for which the spatial arrangement of the objects is only implicitly implied (e.g., \"man riding horse\"). In contrast with explicit relationships, predicting spatial arrangements from implicit spatial language requires significant common sense spatial understanding. Here, we introduce the task of predicting spatial templates for two objects under a relationship, which can be seen as a spatial question-answering task with a (2D) continuous output (\"where is the man w.r.t. a horse when the man is walking the horse?\"). We present two simple neural-based models that leverage annotated images and structured text to learn this task. The good performance of these models reveals that spatial locations are to a large extent predictable from implicit spatial language. Crucially, the models attain similar performance in a challenging generalized setting, where the object-relation-object combinations (e.g.,\"man walking dog\") have never been seen before. Next, we go one step further by presenting the models with unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging word embeddings enables the models to output accurate spatial predictions, proving that the models acquire solid common sense spatial knowledge allowing for such generalization.", "weight": 0.87}, {"from": "Data pre-processing", "to": "To provide machines with common sense is one of the major long term goals of artificial intelligence. Common sense knowledge regards knowledge that humans have acquired through a lifetime of experiences. It is crucial in language understanding because a lot of content needed for correct understanding is not expressed explicitly but resides in the mind of communicator and audience. In addition, humans rely on their common sense knowledge when performing a variety of tasks including interpreting images, navigation and reasoning, to name a few. Representing and understanding spatial knowledge are in fact imperative for any agent (human, animal or robot) that navigates in a physical world. In this paper, we are interested in acquiring spatial commonsense knowledge from language paired with visual data.", "weight": 0.87}, {"from": "Data pre-processing", "to": "Computational and cognitive models often handle spatial representations as spatial templates or regions of acceptability for two objects under an explicit (a.k.a. deictic) spatial preposition such as \"on\", \"below\" or \"left\" ", "weight": 0.87}, {"from": "Data pre-processing", "to": "Predicting spatial templates for implicit relationships is notably more challenging than for explicit relationships. Firstly, whereas there are only a few tens of explicit spatial prepositions, there exist thousands of actions, entailing thus a drastic increase in the sparsity of (object 1 , relationship, object 2 ) combinations. Secondly, the complexity of the task radically increases in implicit language. More precisely, while explicit spatial prepositions 1 are highly deterministic about the spatial arrangements (e.g., (object 1 , below, object 2 ) unequivocally implies that object 1 is relatively lower than object 2 ), actions generally are not. E.g., the relative spatial configuration of \"man\" and the object is clearly distinct in (man, pulling, kite) than in (man, pulling, luggage) yet the action is the same. Contrarily, other relationships such as \"jumping\" are highly informative about the spatial template, i.e., in (object 1 , jumping, object 2 ), object 2 is in a lower position than object 1 . Hence, unlike explicit relationships, predicting spatial layouts from implicit spatial language requires spatial common sense knowledge about the objects, actions and their interaction, which suggests the need of learning to compose the triplet (Subject, Relationship, Object) as a whole instead of learning a template for each Relationship.", "weight": 0.87}, {"from": "Data pre-processing", "to": "To systematically study these questions, we propose the task of predicting the relative spatial locations of two objects given a structured text input (Subject, Relationship, Object). We introduce two simple neural-based models trained from annotated images that successfully address the two challenges of implicit spatial language discussed above. Our quantitative evaluation reveals that spatial templates can be reliably predicted from implicit spatial language-as accurately as from explicit spatial language. We also show that our models generalize well to templates of unseen combinations, e.g., predicting (man, riding, elephant) without having been exposed to such scene before, tackling thus the challenge of sparsity. Furthermore, by leveraging word embeddings, the models can correctly generalize to spatial templates with unseen words, e.g., predicting (man, riding, elephant) without having ever seen an \"elephant\" before. Since word embeddings capture attributes of objects ", "weight": 0.87}, {"from": "Data pre-processing", "to": "The rest of the paper is organized as follows. In Sect. 2 we review related research. In Sect. 3 we first introduce the task of predicting spatial templates and then present two simple neural models. Then, in Sect. 4, we describe our experimental setup. In Sect. 5 we present and discuss our results. Finally, in Sect. 6 we summarize the contributions of this article.", "weight": 0.87}, {"from": "Data pre-processing", "to": "Spatial processing has drawn significant attention from the cognitive ", "weight": 0.87}, {"from": "Data pre-processing", "to": "Spatial templates. Earlier approaches have predominantly considered rule-based spatial representations ", "weight": 0.87}, {"from": "Data pre-processing", "to": "Additionally, while they build a spatial template for each (explicit) Relationship, we build a template for each (Subject, Relationship, Object) combination, allowing the template to be determined by the interaction/composition of the Subject, Relationship and Object instead of the Relationship alone. Additionally, the model from ", "weight": 0.87}, {"from": "Data pre-processing", "to": "Leveraging spatial knowledge in tasks. It has been shown that knowledge of the spatial structure in images improves the task of image captioning ", "weight": 0.87}, {"from": "Data pre-processing", "to": "Common sense spatial knowledge. ", "weight": 0.87}, {"from": "Data pre-processing", "to": "Image generation. Although models that generate images from text exist (e.g., DRAW model ", "weight": 0.87}, {"from": "Data pre-processing", "to": "As discussed, spatial knowledge can improve a wide range of tasks ", "weight": 0.87}, {"from": "Data pre-processing", "to": "Obj.", "weight": 0.87}, {"from": "Data pre-processing", "to": "Subj.", "weight": 0.87}, {"from": "Data pre-processing", "to": "x y Elliott and Keller 2013). This suggests that the predictions of our models can be used as spatial common sense input for methods that rely on good spatial priors. Additionally, existing methods require the spatial information to be present in the data and lack the capacity to extrapolate/generalize. Thus, in this paper we focus, first, on showing that generalizing spatial arrangements to unseen objects and object-relation-object combinations is possible and, second, on ensuring that the predicted templates are accurate by performing several quantitative and qualitative evaluations.", "weight": 0.87}, {"from": "Data pre-processing", "to": "3 Proposed task and model", "weight": 0.87}, {"from": "Data pre-processing", "to": "To learn spatial templates, we propose the task of predicting the 2D relative spatial arrangement of two objects under a relationship given a structured text input of the form (Subject, Relationship, Object)-henceforth abbreviated as (S, R, O). Let us denote the 2D coordinates of the center (\"c\") of the Object\u0027s box as", "weight": 0.87}, {"from": "Data pre-processing", "to": "y \u2208 R are the horizontal and vertical components respectively. Let", "weight": 0.87}, {"from": "Data pre-processing", "to": "x , O b y ] \u2208 R 2 be half of the width (O b x ) and half of the height (O b y ) of the Object\u0027s box (\"b\"). We employ a similar notation for the Subject (S c , S b ), and model predictions are denoted with a hat O c , O b . The task consists in predicting the Object\u0027s location and size", "weight": 0.87}, {"from": "Data pre-processing", "to": "given the structured text input (S, R, O) and the location S c and size S b of the Subject ", "weight": 0.87}, {"from": "Data pre-processing", "to": "2 Crucially, we notice that knowing the Subject\u0027s coordinates is not a requirement for generating templates (but only for evaluating against the ground truth) since inputting arbitrary coordinates (e.g., S c =[0.5, 0.5]) still enables visualizing relative object locations. Additionally, we input the Subject\u0027s size in order to provide a reference size to the model. However, we find that without this input, the model still learns to predict an \"average size\" for each Object.", "weight": 0.87}, {"from": "Data pre-processing", "to": "Hence, the goal is to answer common sense spatial questions such as \"if a man is feeding a horse, where is the man relative to the horse?\" or \"where would a child wear her shoes?\"", "weight": 0.87}, {"from": "Data pre-processing", "to": "To build a mapping from the input to the output in our task (Sect. 3.1) we propose two simple neural models ( ", "weight": 0.87}, {"from": "Data pre-processing", "to": "are one-hot encodings of S, R, O (a.k.a. one-of-k encoding, i.e., a sparse vector with 0 everywhere except for a 1 at the position of the k-th word) and", "weight": 0.87}, {"from": "Data pre-processing", "to": "This layer represents objects and relationships as continuous features, enabling thus to introduce external knowledge of unseen objects as features. The embeddings are then concatenated with the Subject center S c and size", "weight": 0.87}, {"from": "Data pre-processing", "to": "which is inputted to a stack of hidden layers that compose S, R and O into a joint hidden representation z h :", "weight": 0.87}, {"from": "Data pre-processing", "to": "where f (\u2022) is an element-wise non-linear function and W h and b h are the weight matrix and bias respectively. 3 (ii) Output and loss. We consider two different possibilities for the output\u0177 that follows immediately after the last layer: ", "weight": 0.87}, {"from": "Data pre-processing", "to": "These models are conceptually different and have different capabilities. While the REG model outputs \"crisp\" pointwise predictions, PIX can model more diffuse spatial templates where the location of the object has more variability, e.g., in (man, flying, kite) the \"kite\" can easily move around. Notice that in contrast with convolutional neural networks (CNNs) our approach does not make use of the image pixels ( ", "weight": 0.87}, {"from": "Data pre-processing", "to": "We employ a 10-fold cross-validation (CV) setting. Data are randomly split into 10 disjoint parts and 10% is employed for testing and 90% for training, repeating this for each of the 10 folds. Reported results are averages over the 10 folds.", "weight": 0.87}, {"from": "Data pre-processing", "to": "We use the Visual Genome dataset ", "weight": 0.87}, {"from": "Data pre-processing", "to": "dog, catches, frisbee boy, feeds, giraffe man, throws, frisbee cat, wears glasses ", "weight": 0.87}, {"from": "Data pre-processing", "to": "We consider the following subsets of the Visual Genome data to evaluate performance. (i) Raw data: Simply the unfiltered instances from the Visual Genome data (Sect. 4.1). This set contains a substantial proportion of meaningless (e.g., (nose, almost, touching)) and irrelevant (e.g., (sign, says, gate 2)) instances. (ii) Generalized Triplets: We pick at random 100 combinations (S, R, O) among the 1,000 most frequent implicit combinations in Visual Genome. This yields \u223c25K instances such as (person, holding, racket), (man, flying, kite), etc. 4 (iii) Generalized Words: We randomly choose 25 objects (e.g., \"woman\", \"apple\", etc.) 5 among the 100 most frequent objects in Visual Genome and take all the instances (\u223c130K) that contain any of these words. For example, since \"apple\" is in our list, e.g., (cat, sniffing, apple) is kept. ", "weight": 0.87}, {"from": "Data pre-processing", "to": "When enforcing generalization conditions in our experiments, all combinations from sets (ii) and (iii) are removed from the training data to prevent the model from seeing them. 7 Even without imposing generalization conditions (or when testing with Raw data), reported results are always on unseen instances-yet the combinations (S, R, O) may have been seen during training (e.g., in different images). All sets above contain exclusively implicit spatial language, although an analogous version of the Raw set where R are explicit spatial prepositions is also considered in our experiments.", "weight": 0.87}, {"from": "Data pre-processing", "to": "The coordinates of bounding boxes in the images are normalized by the width and height of the image. Thus, S c , O c \u2208 [0, 1] 2 . Additionally, we notice that the distinction between left and right is arbitrary regarding the semantics of the image ", "weight": 0.87}, {"from": "Data pre-processing", "to": "The REG model directly outputs Object coordinates, while PIX outputs 2D heatmaps. We however enable evaluating the PIX model with regression/classification metrics by taking the point of maximum activation (or their average, if there are many) as the Object center O c . The predicted Object size O b is not estimated. We use the following performance metrics.", "weight": 0.87}, {"from": "Data pre-processing", "to": "(A) Intersection over Union ", "weight": 0.87}, {"from": "Data pre-processing", "to": "where B O and B O are the predicted and ground truth Object bounding boxes, respectively. If the IoU is larger than 50%, the prediction is counted as correct. It must be noted that our setting is not comparable to object detection (nor our results) since we do not employ the image as input (but text) and thus we cannot leverage the pixels to predict the Object\u0027s location.", "weight": 0.87}, {"from": "Data pre-processing", "to": "(B) Regression. We consider standard regression metrics. ", "weight": 0.87}, {"from": "Data pre-processing", "to": "(D) Pixel (macro) accuracy. The IoU on pixels is equivalent to binary pixel accuracy. However, this is not a good measure here, where class 1 (Object box) comprises, on average, only 5% of the pixels. Thus, a constant prediction of zeros everywhere would obtain 95% accuracy. Hence, we consider macro-averaged pixel accuracy, a.k.a. mean IoU (mIoU) ", "weight": 0.87}, {"from": "Data pre-processing", "to": "We use 300-dimensional GloVe word embeddings (Pennington, Socher, and Manning 2014) pre-trained on the Common Crawl corpus (consisting of 840B-tokens), which we obtain from the authors\u0027 website. 8", "weight": 0.87}, {"from": "Data pre-processing", "to": "Our experiments are implemented in Python 2.7 and we use Keras deep learning framework for our models ", "weight": 0.87}, {"from": "Data pre-processing", "to": "We consider the evaluation sets from Sect. 4.2 and the following variations of PIX and REG models (Sect. 3.2). The subindex EMB denotes a model that employs GloVe embeddings and RND a model with embeddings randomly drawn from a dimension-wise normal distribution of mean (\u03bc) and standard deviation (\u03c3) equal to those of the GloVe embeddings, preserving the original dimensionality (d=300). A third type employs one-hot vectors (1H). We additionally  consider a control method (ctrl) that outputs random normal predictions of \u03bc and \u03c3 equal to the dimension-wise mean and standard deviation of the training targets. We test statistical significance with a Friedman rank test and post hoc Nemenyi tests on the results of the 10 folds. We indicate with an asterisk * in the tables when a method is significantly better than the rest (p \u003c 0.01) within the same model (PIX or REG). ", "weight": 0.87}, {"from": "Data pre-processing", "to": "To ensure that model predictions are meaningful and interpretable, we further validate the quantitative results above with a qualitative evaluation. Notice that all plots in ", "weight": 0.87}, {"from": "Data pre-processing", "to": "We study how the weights of the model provide insight into the spatial properties of words. To obtain more interpretable weights, we learn a REG 9 model without hidden layers, resulting in only an embedding layer followed by a linear output layer\u0177 = W out u + b out , where", "weight": 0.87}, {"from": "Data pre-processing", "to": "By using one-hot encodings w S , w R , w O , the concatenation layer u becomes of size |V S | + |V R | + |V O | + 2 + 2. E.g., if \"wearing\" has one-hot index j in the Relationships\u0027 vocabulary (V R ), its index in the concatenation layer is |V S | + j. The product W out u is a 4-dimensional vector, where its i-th component is the product of the i-th row of W out with the vector u. Thus, the component |V S |+j of the i-th row of W out gives us the influence of the j-th relationship (i.e., \"wearing\") on the i-th dimension of the output\u0177 =   ", "weight": 0.87}, {"from": "Data pre-processing", "to": "Overall, this paper provides insight into the fundamental differences between implicit and explicit spatial language and extends the concept of spatial templates to implicit spatial language, the understanding of which requires common sense spatial knowledge about objects and actions. We define the task of predicting relative spatial arrangements of two objects under a relationship and present two embedding-based neural models that attain promising performance, proving that spatial templates can be accurately predicted from implicit spatial language. Remarkably, our models generalize well, predicting correctly unseen (generalized) object-relationshipobject combinations. Furthermore, the acquired common sense spatial knowledge-aided with word embeddingsallows the model to correctly predict templates for unseen words. Finally, we show that the weights of the model provide great insight into the spatial connotations of words.", "weight": 0.87}, {"from": "Data pre-processing", "to": "A first limitation of our approach is the fully supervised setting where the models are trained using images with detected ground truth objects and parsed text-which aims at keeping the design clean in this first study on implicit spatial templates. Notice however that methods to automatically parse images and text exist. In future work, we aim at implementing our approach in a weakly supervised setting. A second limitation is the 2D spatial treatment of the actual 3D world. It is worth noting however, that our models (PIX and REG) and setting trivially generalize to 3D if appropriate data are available.", "weight": 0.87}, {"from": "Your Visual Research Paper", "to": "Evaluation metrics", "weight": 0.97}, {"from": "Evaluation metrics", "to": "Spatial understanding is a fundamental problem with widereaching real-world applications. The representation of spatial knowledge is often modeled with spatial templates, i.e., regions of acceptability of two objects under an explicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with prior work that restricts spatial templates to explicit spatial prepositions (e.g., \"glass on table\"), here we extend this concept to implicit spatial language, i.e., those relationships (generally actions) for which the spatial arrangement of the objects is only implicitly implied (e.g., \"man riding horse\"). In contrast with explicit relationships, predicting spatial arrangements from implicit spatial language requires significant common sense spatial understanding. Here, we introduce the task of predicting spatial templates for two objects under a relationship, which can be seen as a spatial question-answering task with a (2D) continuous output (\"where is the man w.r.t. a horse when the man is walking the horse?\"). We present two simple neural-based models that leverage annotated images and structured text to learn this task. The good performance of these models reveals that spatial locations are to a large extent predictable from implicit spatial language. Crucially, the models attain similar performance in a challenging generalized setting, where the object-relation-object combinations (e.g.,\"man walking dog\") have never been seen before. Next, we go one step further by presenting the models with unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging word embeddings enables the models to output accurate spatial predictions, proving that the models acquire solid common sense spatial knowledge allowing for such generalization.", "weight": 0.87}, {"from": "Evaluation metrics", "to": "To provide machines with common sense is one of the major long term goals of artificial intelligence. Common sense knowledge regards knowledge that humans have acquired through a lifetime of experiences. It is crucial in language understanding because a lot of content needed for correct understanding is not expressed explicitly but resides in the mind of communicator and audience. In addition, humans rely on their common sense knowledge when performing a variety of tasks including interpreting images, navigation and reasoning, to name a few. Representing and understanding spatial knowledge are in fact imperative for any agent (human, animal or robot) that navigates in a physical world. In this paper, we are interested in acquiring spatial commonsense knowledge from language paired with visual data.", "weight": 0.87}, {"from": "Evaluation metrics", "to": "Computational and cognitive models often handle spatial representations as spatial templates or regions of acceptability for two objects under an explicit (a.k.a. deictic) spatial preposition such as \"on\", \"below\" or \"left\" ", "weight": 0.87}, {"from": "Evaluation metrics", "to": "Predicting spatial templates for implicit relationships is notably more challenging than for explicit relationships. Firstly, whereas there are only a few tens of explicit spatial prepositions, there exist thousands of actions, entailing thus a drastic increase in the sparsity of (object 1 , relationship, object 2 ) combinations. Secondly, the complexity of the task radically increases in implicit language. More precisely, while explicit spatial prepositions 1 are highly deterministic about the spatial arrangements (e.g., (object 1 , below, object 2 ) unequivocally implies that object 1 is relatively lower than object 2 ), actions generally are not. E.g., the relative spatial configuration of \"man\" and the object is clearly distinct in (man, pulling, kite) than in (man, pulling, luggage) yet the action is the same. Contrarily, other relationships such as \"jumping\" are highly informative about the spatial template, i.e., in (object 1 , jumping, object 2 ), object 2 is in a lower position than object 1 . Hence, unlike explicit relationships, predicting spatial layouts from implicit spatial language requires spatial common sense knowledge about the objects, actions and their interaction, which suggests the need of learning to compose the triplet (Subject, Relationship, Object) as a whole instead of learning a template for each Relationship.", "weight": 0.87}, {"from": "Evaluation metrics", "to": "To systematically study these questions, we propose the task of predicting the relative spatial locations of two objects given a structured text input (Subject, Relationship, Object). We introduce two simple neural-based models trained from annotated images that successfully address the two challenges of implicit spatial language discussed above. Our quantitative evaluation reveals that spatial templates can be reliably predicted from implicit spatial language-as accurately as from explicit spatial language. We also show that our models generalize well to templates of unseen combinations, e.g., predicting (man, riding, elephant) without having been exposed to such scene before, tackling thus the challenge of sparsity. Furthermore, by leveraging word embeddings, the models can correctly generalize to spatial templates with unseen words, e.g., predicting (man, riding, elephant) without having ever seen an \"elephant\" before. Since word embeddings capture attributes of objects ", "weight": 0.87}, {"from": "Evaluation metrics", "to": "The rest of the paper is organized as follows. In Sect. 2 we review related research. In Sect. 3 we first introduce the task of predicting spatial templates and then present two simple neural models. Then, in Sect. 4, we describe our experimental setup. In Sect. 5 we present and discuss our results. Finally, in Sect. 6 we summarize the contributions of this article.", "weight": 0.87}, {"from": "Evaluation metrics", "to": "Spatial processing has drawn significant attention from the cognitive ", "weight": 0.87}, {"from": "Evaluation metrics", "to": "Spatial templates. Earlier approaches have predominantly considered rule-based spatial representations ", "weight": 0.87}, {"from": "Evaluation metrics", "to": "Additionally, while they build a spatial template for each (explicit) Relationship, we build a template for each (Subject, Relationship, Object) combination, allowing the template to be determined by the interaction/composition of the Subject, Relationship and Object instead of the Relationship alone. Additionally, the model from ", "weight": 0.87}, {"from": "Evaluation metrics", "to": "Leveraging spatial knowledge in tasks. It has been shown that knowledge of the spatial structure in images improves the task of image captioning ", "weight": 0.87}, {"from": "Evaluation metrics", "to": "Common sense spatial knowledge. ", "weight": 0.87}, {"from": "Evaluation metrics", "to": "Image generation. Although models that generate images from text exist (e.g., DRAW model ", "weight": 0.87}, {"from": "Evaluation metrics", "to": "As discussed, spatial knowledge can improve a wide range of tasks ", "weight": 0.87}, {"from": "Evaluation metrics", "to": "Obj.", "weight": 0.87}, {"from": "Evaluation metrics", "to": "Subj.", "weight": 0.87}, {"from": "Evaluation metrics", "to": "x y Elliott and Keller 2013). This suggests that the predictions of our models can be used as spatial common sense input for methods that rely on good spatial priors. Additionally, existing methods require the spatial information to be present in the data and lack the capacity to extrapolate/generalize. Thus, in this paper we focus, first, on showing that generalizing spatial arrangements to unseen objects and object-relation-object combinations is possible and, second, on ensuring that the predicted templates are accurate by performing several quantitative and qualitative evaluations.", "weight": 0.87}, {"from": "Evaluation metrics", "to": "3 Proposed task and model", "weight": 0.87}, {"from": "Evaluation metrics", "to": "To learn spatial templates, we propose the task of predicting the 2D relative spatial arrangement of two objects under a relationship given a structured text input of the form (Subject, Relationship, Object)-henceforth abbreviated as (S, R, O). Let us denote the 2D coordinates of the center (\"c\") of the Object\u0027s box as", "weight": 0.87}, {"from": "Evaluation metrics", "to": "y \u2208 R are the horizontal and vertical components respectively. Let", "weight": 0.87}, {"from": "Evaluation metrics", "to": "x , O b y ] \u2208 R 2 be half of the width (O b x ) and half of the height (O b y ) of the Object\u0027s box (\"b\"). We employ a similar notation for the Subject (S c , S b ), and model predictions are denoted with a hat O c , O b . The task consists in predicting the Object\u0027s location and size", "weight": 0.87}, {"from": "Evaluation metrics", "to": "given the structured text input (S, R, O) and the location S c and size S b of the Subject ", "weight": 0.87}, {"from": "Evaluation metrics", "to": "2 Crucially, we notice that knowing the Subject\u0027s coordinates is not a requirement for generating templates (but only for evaluating against the ground truth) since inputting arbitrary coordinates (e.g., S c =[0.5, 0.5]) still enables visualizing relative object locations. Additionally, we input the Subject\u0027s size in order to provide a reference size to the model. However, we find that without this input, the model still learns to predict an \"average size\" for each Object.", "weight": 0.87}, {"from": "Evaluation metrics", "to": "Hence, the goal is to answer common sense spatial questions such as \"if a man is feeding a horse, where is the man relative to the horse?\" or \"where would a child wear her shoes?\"", "weight": 0.87}, {"from": "Evaluation metrics", "to": "To build a mapping from the input to the output in our task (Sect. 3.1) we propose two simple neural models ( ", "weight": 0.87}, {"from": "Evaluation metrics", "to": "are one-hot encodings of S, R, O (a.k.a. one-of-k encoding, i.e., a sparse vector with 0 everywhere except for a 1 at the position of the k-th word) and", "weight": 0.87}, {"from": "Evaluation metrics", "to": "This layer represents objects and relationships as continuous features, enabling thus to introduce external knowledge of unseen objects as features. The embeddings are then concatenated with the Subject center S c and size", "weight": 0.87}, {"from": "Evaluation metrics", "to": "which is inputted to a stack of hidden layers that compose S, R and O into a joint hidden representation z h :", "weight": 0.87}, {"from": "Evaluation metrics", "to": "where f (\u2022) is an element-wise non-linear function and W h and b h are the weight matrix and bias respectively. 3 (ii) Output and loss. We consider two different possibilities for the output\u0177 that follows immediately after the last layer: ", "weight": 0.87}, {"from": "Evaluation metrics", "to": "These models are conceptually different and have different capabilities. While the REG model outputs \"crisp\" pointwise predictions, PIX can model more diffuse spatial templates where the location of the object has more variability, e.g., in (man, flying, kite) the \"kite\" can easily move around. Notice that in contrast with convolutional neural networks (CNNs) our approach does not make use of the image pixels ( ", "weight": 0.87}, {"from": "Evaluation metrics", "to": "We employ a 10-fold cross-validation (CV) setting. Data are randomly split into 10 disjoint parts and 10% is employed for testing and 90% for training, repeating this for each of the 10 folds. Reported results are averages over the 10 folds.", "weight": 0.87}, {"from": "Evaluation metrics", "to": "We use the Visual Genome dataset ", "weight": 0.87}, {"from": "Evaluation metrics", "to": "dog, catches, frisbee boy, feeds, giraffe man, throws, frisbee cat, wears glasses ", "weight": 0.87}, {"from": "Evaluation metrics", "to": "We consider the following subsets of the Visual Genome data to evaluate performance. (i) Raw data: Simply the unfiltered instances from the Visual Genome data (Sect. 4.1). This set contains a substantial proportion of meaningless (e.g., (nose, almost, touching)) and irrelevant (e.g., (sign, says, gate 2)) instances. (ii) Generalized Triplets: We pick at random 100 combinations (S, R, O) among the 1,000 most frequent implicit combinations in Visual Genome. This yields \u223c25K instances such as (person, holding, racket), (man, flying, kite), etc. 4 (iii) Generalized Words: We randomly choose 25 objects (e.g., \"woman\", \"apple\", etc.) 5 among the 100 most frequent objects in Visual Genome and take all the instances (\u223c130K) that contain any of these words. For example, since \"apple\" is in our list, e.g., (cat, sniffing, apple) is kept. ", "weight": 0.87}, {"from": "Evaluation metrics", "to": "When enforcing generalization conditions in our experiments, all combinations from sets (ii) and (iii) are removed from the training data to prevent the model from seeing them. 7 Even without imposing generalization conditions (or when testing with Raw data), reported results are always on unseen instances-yet the combinations (S, R, O) may have been seen during training (e.g., in different images). All sets above contain exclusively implicit spatial language, although an analogous version of the Raw set where R are explicit spatial prepositions is also considered in our experiments.", "weight": 0.87}, {"from": "Evaluation metrics", "to": "The coordinates of bounding boxes in the images are normalized by the width and height of the image. Thus, S c , O c \u2208 [0, 1] 2 . Additionally, we notice that the distinction between left and right is arbitrary regarding the semantics of the image ", "weight": 0.87}, {"from": "Evaluation metrics", "to": "The REG model directly outputs Object coordinates, while PIX outputs 2D heatmaps. We however enable evaluating the PIX model with regression/classification metrics by taking the point of maximum activation (or their average, if there are many) as the Object center O c . The predicted Object size O b is not estimated. We use the following performance metrics.", "weight": 0.87}, {"from": "Evaluation metrics", "to": "(A) Intersection over Union ", "weight": 0.87}, {"from": "Evaluation metrics", "to": "where B O and B O are the predicted and ground truth Object bounding boxes, respectively. If the IoU is larger than 50%, the prediction is counted as correct. It must be noted that our setting is not comparable to object detection (nor our results) since we do not employ the image as input (but text) and thus we cannot leverage the pixels to predict the Object\u0027s location.", "weight": 0.87}, {"from": "Evaluation metrics", "to": "(B) Regression. We consider standard regression metrics. ", "weight": 0.87}, {"from": "Evaluation metrics", "to": "(D) Pixel (macro) accuracy. The IoU on pixels is equivalent to binary pixel accuracy. However, this is not a good measure here, where class 1 (Object box) comprises, on average, only 5% of the pixels. Thus, a constant prediction of zeros everywhere would obtain 95% accuracy. Hence, we consider macro-averaged pixel accuracy, a.k.a. mean IoU (mIoU) ", "weight": 0.87}, {"from": "Evaluation metrics", "to": "We use 300-dimensional GloVe word embeddings (Pennington, Socher, and Manning 2014) pre-trained on the Common Crawl corpus (consisting of 840B-tokens), which we obtain from the authors\u0027 website. 8", "weight": 0.87}, {"from": "Evaluation metrics", "to": "Our experiments are implemented in Python 2.7 and we use Keras deep learning framework for our models ", "weight": 0.87}, {"from": "Evaluation metrics", "to": "We consider the evaluation sets from Sect. 4.2 and the following variations of PIX and REG models (Sect. 3.2). The subindex EMB denotes a model that employs GloVe embeddings and RND a model with embeddings randomly drawn from a dimension-wise normal distribution of mean (\u03bc) and standard deviation (\u03c3) equal to those of the GloVe embeddings, preserving the original dimensionality (d=300). A third type employs one-hot vectors (1H). We additionally  consider a control method (ctrl) that outputs random normal predictions of \u03bc and \u03c3 equal to the dimension-wise mean and standard deviation of the training targets. We test statistical significance with a Friedman rank test and post hoc Nemenyi tests on the results of the 10 folds. We indicate with an asterisk * in the tables when a method is significantly better than the rest (p \u003c 0.01) within the same model (PIX or REG). ", "weight": 0.87}, {"from": "Evaluation metrics", "to": "To ensure that model predictions are meaningful and interpretable, we further validate the quantitative results above with a qualitative evaluation. Notice that all plots in ", "weight": 0.87}, {"from": "Evaluation metrics", "to": "We study how the weights of the model provide insight into the spatial properties of words. To obtain more interpretable weights, we learn a REG 9 model without hidden layers, resulting in only an embedding layer followed by a linear output layer\u0177 = W out u + b out , where", "weight": 0.87}, {"from": "Evaluation metrics", "to": "By using one-hot encodings w S , w R , w O , the concatenation layer u becomes of size |V S | + |V R | + |V O | + 2 + 2. E.g., if \"wearing\" has one-hot index j in the Relationships\u0027 vocabulary (V R ), its index in the concatenation layer is |V S | + j. The product W out u is a 4-dimensional vector, where its i-th component is the product of the i-th row of W out with the vector u. Thus, the component |V S |+j of the i-th row of W out gives us the influence of the j-th relationship (i.e., \"wearing\") on the i-th dimension of the output\u0177 =   ", "weight": 0.87}, {"from": "Evaluation metrics", "to": "Overall, this paper provides insight into the fundamental differences between implicit and explicit spatial language and extends the concept of spatial templates to implicit spatial language, the understanding of which requires common sense spatial knowledge about objects and actions. We define the task of predicting relative spatial arrangements of two objects under a relationship and present two embedding-based neural models that attain promising performance, proving that spatial templates can be accurately predicted from implicit spatial language. Remarkably, our models generalize well, predicting correctly unseen (generalized) object-relationshipobject combinations. Furthermore, the acquired common sense spatial knowledge-aided with word embeddingsallows the model to correctly predict templates for unseen words. Finally, we show that the weights of the model provide great insight into the spatial connotations of words.", "weight": 0.87}, {"from": "Evaluation metrics", "to": "A first limitation of our approach is the fully supervised setting where the models are trained using images with detected ground truth objects and parsed text-which aims at keeping the design clean in this first study on implicit spatial templates. Notice however that methods to automatically parse images and text exist. In future work, we aim at implementing our approach in a weakly supervised setting. A second limitation is the 2D spatial treatment of the actual 3D world. It is worth noting however, that our models (PIX and REG) and setting trivially generalize to 3D if appropriate data are available.", "weight": 0.87}, {"from": "Your Visual Research Paper", "to": "Word embeddings", "weight": 0.97}, {"from": "Word embeddings", "to": "Spatial understanding is a fundamental problem with widereaching real-world applications. The representation of spatial knowledge is often modeled with spatial templates, i.e., regions of acceptability of two objects under an explicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with prior work that restricts spatial templates to explicit spatial prepositions (e.g., \"glass on table\"), here we extend this concept to implicit spatial language, i.e., those relationships (generally actions) for which the spatial arrangement of the objects is only implicitly implied (e.g., \"man riding horse\"). In contrast with explicit relationships, predicting spatial arrangements from implicit spatial language requires significant common sense spatial understanding. Here, we introduce the task of predicting spatial templates for two objects under a relationship, which can be seen as a spatial question-answering task with a (2D) continuous output (\"where is the man w.r.t. a horse when the man is walking the horse?\"). We present two simple neural-based models that leverage annotated images and structured text to learn this task. The good performance of these models reveals that spatial locations are to a large extent predictable from implicit spatial language. Crucially, the models attain similar performance in a challenging generalized setting, where the object-relation-object combinations (e.g.,\"man walking dog\") have never been seen before. Next, we go one step further by presenting the models with unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging word embeddings enables the models to output accurate spatial predictions, proving that the models acquire solid common sense spatial knowledge allowing for such generalization.", "weight": 0.87}, {"from": "Word embeddings", "to": "To provide machines with common sense is one of the major long term goals of artificial intelligence. Common sense knowledge regards knowledge that humans have acquired through a lifetime of experiences. It is crucial in language understanding because a lot of content needed for correct understanding is not expressed explicitly but resides in the mind of communicator and audience. In addition, humans rely on their common sense knowledge when performing a variety of tasks including interpreting images, navigation and reasoning, to name a few. Representing and understanding spatial knowledge are in fact imperative for any agent (human, animal or robot) that navigates in a physical world. In this paper, we are interested in acquiring spatial commonsense knowledge from language paired with visual data.", "weight": 0.87}, {"from": "Word embeddings", "to": "Computational and cognitive models often handle spatial representations as spatial templates or regions of acceptability for two objects under an explicit (a.k.a. deictic) spatial preposition such as \"on\", \"below\" or \"left\" ", "weight": 0.87}, {"from": "Word embeddings", "to": "Predicting spatial templates for implicit relationships is notably more challenging than for explicit relationships. Firstly, whereas there are only a few tens of explicit spatial prepositions, there exist thousands of actions, entailing thus a drastic increase in the sparsity of (object 1 , relationship, object 2 ) combinations. Secondly, the complexity of the task radically increases in implicit language. More precisely, while explicit spatial prepositions 1 are highly deterministic about the spatial arrangements (e.g., (object 1 , below, object 2 ) unequivocally implies that object 1 is relatively lower than object 2 ), actions generally are not. E.g., the relative spatial configuration of \"man\" and the object is clearly distinct in (man, pulling, kite) than in (man, pulling, luggage) yet the action is the same. Contrarily, other relationships such as \"jumping\" are highly informative about the spatial template, i.e., in (object 1 , jumping, object 2 ), object 2 is in a lower position than object 1 . Hence, unlike explicit relationships, predicting spatial layouts from implicit spatial language requires spatial common sense knowledge about the objects, actions and their interaction, which suggests the need of learning to compose the triplet (Subject, Relationship, Object) as a whole instead of learning a template for each Relationship.", "weight": 0.87}, {"from": "Word embeddings", "to": "To systematically study these questions, we propose the task of predicting the relative spatial locations of two objects given a structured text input (Subject, Relationship, Object). We introduce two simple neural-based models trained from annotated images that successfully address the two challenges of implicit spatial language discussed above. Our quantitative evaluation reveals that spatial templates can be reliably predicted from implicit spatial language-as accurately as from explicit spatial language. We also show that our models generalize well to templates of unseen combinations, e.g., predicting (man, riding, elephant) without having been exposed to such scene before, tackling thus the challenge of sparsity. Furthermore, by leveraging word embeddings, the models can correctly generalize to spatial templates with unseen words, e.g., predicting (man, riding, elephant) without having ever seen an \"elephant\" before. Since word embeddings capture attributes of objects ", "weight": 0.87}, {"from": "Word embeddings", "to": "The rest of the paper is organized as follows. In Sect. 2 we review related research. In Sect. 3 we first introduce the task of predicting spatial templates and then present two simple neural models. Then, in Sect. 4, we describe our experimental setup. In Sect. 5 we present and discuss our results. Finally, in Sect. 6 we summarize the contributions of this article.", "weight": 0.87}, {"from": "Word embeddings", "to": "Spatial processing has drawn significant attention from the cognitive ", "weight": 0.87}, {"from": "Word embeddings", "to": "Spatial templates. Earlier approaches have predominantly considered rule-based spatial representations ", "weight": 0.87}, {"from": "Word embeddings", "to": "Additionally, while they build a spatial template for each (explicit) Relationship, we build a template for each (Subject, Relationship, Object) combination, allowing the template to be determined by the interaction/composition of the Subject, Relationship and Object instead of the Relationship alone. Additionally, the model from ", "weight": 0.87}, {"from": "Word embeddings", "to": "Leveraging spatial knowledge in tasks. It has been shown that knowledge of the spatial structure in images improves the task of image captioning ", "weight": 0.87}, {"from": "Word embeddings", "to": "Common sense spatial knowledge. ", "weight": 0.87}, {"from": "Word embeddings", "to": "Image generation. Although models that generate images from text exist (e.g., DRAW model ", "weight": 0.87}, {"from": "Word embeddings", "to": "As discussed, spatial knowledge can improve a wide range of tasks ", "weight": 0.87}, {"from": "Word embeddings", "to": "Obj.", "weight": 0.87}, {"from": "Word embeddings", "to": "Subj.", "weight": 0.87}, {"from": "Word embeddings", "to": "x y Elliott and Keller 2013). This suggests that the predictions of our models can be used as spatial common sense input for methods that rely on good spatial priors. Additionally, existing methods require the spatial information to be present in the data and lack the capacity to extrapolate/generalize. Thus, in this paper we focus, first, on showing that generalizing spatial arrangements to unseen objects and object-relation-object combinations is possible and, second, on ensuring that the predicted templates are accurate by performing several quantitative and qualitative evaluations.", "weight": 0.87}, {"from": "Word embeddings", "to": "3 Proposed task and model", "weight": 0.87}, {"from": "Word embeddings", "to": "To learn spatial templates, we propose the task of predicting the 2D relative spatial arrangement of two objects under a relationship given a structured text input of the form (Subject, Relationship, Object)-henceforth abbreviated as (S, R, O). Let us denote the 2D coordinates of the center (\"c\") of the Object\u0027s box as", "weight": 0.87}, {"from": "Word embeddings", "to": "y \u2208 R are the horizontal and vertical components respectively. Let", "weight": 0.87}, {"from": "Word embeddings", "to": "x , O b y ] \u2208 R 2 be half of the width (O b x ) and half of the height (O b y ) of the Object\u0027s box (\"b\"). We employ a similar notation for the Subject (S c , S b ), and model predictions are denoted with a hat O c , O b . The task consists in predicting the Object\u0027s location and size", "weight": 0.87}, {"from": "Word embeddings", "to": "given the structured text input (S, R, O) and the location S c and size S b of the Subject ", "weight": 0.87}, {"from": "Word embeddings", "to": "2 Crucially, we notice that knowing the Subject\u0027s coordinates is not a requirement for generating templates (but only for evaluating against the ground truth) since inputting arbitrary coordinates (e.g., S c =[0.5, 0.5]) still enables visualizing relative object locations. Additionally, we input the Subject\u0027s size in order to provide a reference size to the model. However, we find that without this input, the model still learns to predict an \"average size\" for each Object.", "weight": 0.87}, {"from": "Word embeddings", "to": "Hence, the goal is to answer common sense spatial questions such as \"if a man is feeding a horse, where is the man relative to the horse?\" or \"where would a child wear her shoes?\"", "weight": 0.87}, {"from": "Word embeddings", "to": "To build a mapping from the input to the output in our task (Sect. 3.1) we propose two simple neural models ( ", "weight": 0.87}, {"from": "Word embeddings", "to": "are one-hot encodings of S, R, O (a.k.a. one-of-k encoding, i.e., a sparse vector with 0 everywhere except for a 1 at the position of the k-th word) and", "weight": 0.87}, {"from": "Word embeddings", "to": "This layer represents objects and relationships as continuous features, enabling thus to introduce external knowledge of unseen objects as features. The embeddings are then concatenated with the Subject center S c and size", "weight": 0.87}, {"from": "Word embeddings", "to": "which is inputted to a stack of hidden layers that compose S, R and O into a joint hidden representation z h :", "weight": 0.87}, {"from": "Word embeddings", "to": "where f (\u2022) is an element-wise non-linear function and W h and b h are the weight matrix and bias respectively. 3 (ii) Output and loss. We consider two different possibilities for the output\u0177 that follows immediately after the last layer: ", "weight": 0.87}, {"from": "Word embeddings", "to": "These models are conceptually different and have different capabilities. While the REG model outputs \"crisp\" pointwise predictions, PIX can model more diffuse spatial templates where the location of the object has more variability, e.g., in (man, flying, kite) the \"kite\" can easily move around. Notice that in contrast with convolutional neural networks (CNNs) our approach does not make use of the image pixels ( ", "weight": 0.87}, {"from": "Word embeddings", "to": "We employ a 10-fold cross-validation (CV) setting. Data are randomly split into 10 disjoint parts and 10% is employed for testing and 90% for training, repeating this for each of the 10 folds. Reported results are averages over the 10 folds.", "weight": 0.87}, {"from": "Word embeddings", "to": "We use the Visual Genome dataset ", "weight": 0.87}, {"from": "Word embeddings", "to": "dog, catches, frisbee boy, feeds, giraffe man, throws, frisbee cat, wears glasses ", "weight": 0.87}, {"from": "Word embeddings", "to": "We consider the following subsets of the Visual Genome data to evaluate performance. (i) Raw data: Simply the unfiltered instances from the Visual Genome data (Sect. 4.1). This set contains a substantial proportion of meaningless (e.g., (nose, almost, touching)) and irrelevant (e.g., (sign, says, gate 2)) instances. (ii) Generalized Triplets: We pick at random 100 combinations (S, R, O) among the 1,000 most frequent implicit combinations in Visual Genome. This yields \u223c25K instances such as (person, holding, racket), (man, flying, kite), etc. 4 (iii) Generalized Words: We randomly choose 25 objects (e.g., \"woman\", \"apple\", etc.) 5 among the 100 most frequent objects in Visual Genome and take all the instances (\u223c130K) that contain any of these words. For example, since \"apple\" is in our list, e.g., (cat, sniffing, apple) is kept. ", "weight": 0.87}, {"from": "Word embeddings", "to": "When enforcing generalization conditions in our experiments, all combinations from sets (ii) and (iii) are removed from the training data to prevent the model from seeing them. 7 Even without imposing generalization conditions (or when testing with Raw data), reported results are always on unseen instances-yet the combinations (S, R, O) may have been seen during training (e.g., in different images). All sets above contain exclusively implicit spatial language, although an analogous version of the Raw set where R are explicit spatial prepositions is also considered in our experiments.", "weight": 0.87}, {"from": "Word embeddings", "to": "The coordinates of bounding boxes in the images are normalized by the width and height of the image. Thus, S c , O c \u2208 [0, 1] 2 . Additionally, we notice that the distinction between left and right is arbitrary regarding the semantics of the image ", "weight": 0.87}, {"from": "Word embeddings", "to": "The REG model directly outputs Object coordinates, while PIX outputs 2D heatmaps. We however enable evaluating the PIX model with regression/classification metrics by taking the point of maximum activation (or their average, if there are many) as the Object center O c . The predicted Object size O b is not estimated. We use the following performance metrics.", "weight": 0.87}, {"from": "Word embeddings", "to": "(A) Intersection over Union ", "weight": 0.87}, {"from": "Word embeddings", "to": "where B O and B O are the predicted and ground truth Object bounding boxes, respectively. If the IoU is larger than 50%, the prediction is counted as correct. It must be noted that our setting is not comparable to object detection (nor our results) since we do not employ the image as input (but text) and thus we cannot leverage the pixels to predict the Object\u0027s location.", "weight": 0.87}, {"from": "Word embeddings", "to": "(B) Regression. We consider standard regression metrics. ", "weight": 0.87}, {"from": "Word embeddings", "to": "(D) Pixel (macro) accuracy. The IoU on pixels is equivalent to binary pixel accuracy. However, this is not a good measure here, where class 1 (Object box) comprises, on average, only 5% of the pixels. Thus, a constant prediction of zeros everywhere would obtain 95% accuracy. Hence, we consider macro-averaged pixel accuracy, a.k.a. mean IoU (mIoU) ", "weight": 0.87}, {"from": "Word embeddings", "to": "We use 300-dimensional GloVe word embeddings (Pennington, Socher, and Manning 2014) pre-trained on the Common Crawl corpus (consisting of 840B-tokens), which we obtain from the authors\u0027 website. 8", "weight": 0.87}, {"from": "Word embeddings", "to": "Our experiments are implemented in Python 2.7 and we use Keras deep learning framework for our models ", "weight": 0.87}, {"from": "Word embeddings", "to": "We consider the evaluation sets from Sect. 4.2 and the following variations of PIX and REG models (Sect. 3.2). The subindex EMB denotes a model that employs GloVe embeddings and RND a model with embeddings randomly drawn from a dimension-wise normal distribution of mean (\u03bc) and standard deviation (\u03c3) equal to those of the GloVe embeddings, preserving the original dimensionality (d=300). A third type employs one-hot vectors (1H). We additionally  consider a control method (ctrl) that outputs random normal predictions of \u03bc and \u03c3 equal to the dimension-wise mean and standard deviation of the training targets. We test statistical significance with a Friedman rank test and post hoc Nemenyi tests on the results of the 10 folds. We indicate with an asterisk * in the tables when a method is significantly better than the rest (p \u003c 0.01) within the same model (PIX or REG). ", "weight": 0.87}, {"from": "Word embeddings", "to": "To ensure that model predictions are meaningful and interpretable, we further validate the quantitative results above with a qualitative evaluation. Notice that all plots in ", "weight": 0.87}, {"from": "Word embeddings", "to": "We study how the weights of the model provide insight into the spatial properties of words. To obtain more interpretable weights, we learn a REG 9 model without hidden layers, resulting in only an embedding layer followed by a linear output layer\u0177 = W out u + b out , where", "weight": 0.87}, {"from": "Word embeddings", "to": "By using one-hot encodings w S , w R , w O , the concatenation layer u becomes of size |V S | + |V R | + |V O | + 2 + 2. E.g., if \"wearing\" has one-hot index j in the Relationships\u0027 vocabulary (V R ), its index in the concatenation layer is |V S | + j. The product W out u is a 4-dimensional vector, where its i-th component is the product of the i-th row of W out with the vector u. Thus, the component |V S |+j of the i-th row of W out gives us the influence of the j-th relationship (i.e., \"wearing\") on the i-th dimension of the output\u0177 =   ", "weight": 0.87}, {"from": "Word embeddings", "to": "Overall, this paper provides insight into the fundamental differences between implicit and explicit spatial language and extends the concept of spatial templates to implicit spatial language, the understanding of which requires common sense spatial knowledge about objects and actions. We define the task of predicting relative spatial arrangements of two objects under a relationship and present two embedding-based neural models that attain promising performance, proving that spatial templates can be accurately predicted from implicit spatial language. Remarkably, our models generalize well, predicting correctly unseen (generalized) object-relationshipobject combinations. Furthermore, the acquired common sense spatial knowledge-aided with word embeddingsallows the model to correctly predict templates for unseen words. Finally, we show that the weights of the model provide great insight into the spatial connotations of words.", "weight": 0.87}, {"from": "Word embeddings", "to": "A first limitation of our approach is the fully supervised setting where the models are trained using images with detected ground truth objects and parsed text-which aims at keeping the design clean in this first study on implicit spatial templates. Notice however that methods to automatically parse images and text exist. In future work, we aim at implementing our approach in a weakly supervised setting. A second limitation is the 2D spatial treatment of the actual 3D world. It is worth noting however, that our models (PIX and REG) and setting trivially generalize to 3D if appropriate data are available.", "weight": 0.87}, {"from": "Your Visual Research Paper", "to": "Model hyperparameters and implementation", "weight": 0.97}, {"from": "Model hyperparameters and implementation", "to": "Spatial understanding is a fundamental problem with widereaching real-world applications. The representation of spatial knowledge is often modeled with spatial templates, i.e., regions of acceptability of two objects under an explicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with prior work that restricts spatial templates to explicit spatial prepositions (e.g., \"glass on table\"), here we extend this concept to implicit spatial language, i.e., those relationships (generally actions) for which the spatial arrangement of the objects is only implicitly implied (e.g., \"man riding horse\"). In contrast with explicit relationships, predicting spatial arrangements from implicit spatial language requires significant common sense spatial understanding. Here, we introduce the task of predicting spatial templates for two objects under a relationship, which can be seen as a spatial question-answering task with a (2D) continuous output (\"where is the man w.r.t. a horse when the man is walking the horse?\"). We present two simple neural-based models that leverage annotated images and structured text to learn this task. The good performance of these models reveals that spatial locations are to a large extent predictable from implicit spatial language. Crucially, the models attain similar performance in a challenging generalized setting, where the object-relation-object combinations (e.g.,\"man walking dog\") have never been seen before. Next, we go one step further by presenting the models with unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging word embeddings enables the models to output accurate spatial predictions, proving that the models acquire solid common sense spatial knowledge allowing for such generalization.", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "To provide machines with common sense is one of the major long term goals of artificial intelligence. Common sense knowledge regards knowledge that humans have acquired through a lifetime of experiences. It is crucial in language understanding because a lot of content needed for correct understanding is not expressed explicitly but resides in the mind of communicator and audience. In addition, humans rely on their common sense knowledge when performing a variety of tasks including interpreting images, navigation and reasoning, to name a few. Representing and understanding spatial knowledge are in fact imperative for any agent (human, animal or robot) that navigates in a physical world. In this paper, we are interested in acquiring spatial commonsense knowledge from language paired with visual data.", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "Computational and cognitive models often handle spatial representations as spatial templates or regions of acceptability for two objects under an explicit (a.k.a. deictic) spatial preposition such as \"on\", \"below\" or \"left\" ", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "Predicting spatial templates for implicit relationships is notably more challenging than for explicit relationships. Firstly, whereas there are only a few tens of explicit spatial prepositions, there exist thousands of actions, entailing thus a drastic increase in the sparsity of (object 1 , relationship, object 2 ) combinations. Secondly, the complexity of the task radically increases in implicit language. More precisely, while explicit spatial prepositions 1 are highly deterministic about the spatial arrangements (e.g., (object 1 , below, object 2 ) unequivocally implies that object 1 is relatively lower than object 2 ), actions generally are not. E.g., the relative spatial configuration of \"man\" and the object is clearly distinct in (man, pulling, kite) than in (man, pulling, luggage) yet the action is the same. Contrarily, other relationships such as \"jumping\" are highly informative about the spatial template, i.e., in (object 1 , jumping, object 2 ), object 2 is in a lower position than object 1 . Hence, unlike explicit relationships, predicting spatial layouts from implicit spatial language requires spatial common sense knowledge about the objects, actions and their interaction, which suggests the need of learning to compose the triplet (Subject, Relationship, Object) as a whole instead of learning a template for each Relationship.", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "To systematically study these questions, we propose the task of predicting the relative spatial locations of two objects given a structured text input (Subject, Relationship, Object). We introduce two simple neural-based models trained from annotated images that successfully address the two challenges of implicit spatial language discussed above. Our quantitative evaluation reveals that spatial templates can be reliably predicted from implicit spatial language-as accurately as from explicit spatial language. We also show that our models generalize well to templates of unseen combinations, e.g., predicting (man, riding, elephant) without having been exposed to such scene before, tackling thus the challenge of sparsity. Furthermore, by leveraging word embeddings, the models can correctly generalize to spatial templates with unseen words, e.g., predicting (man, riding, elephant) without having ever seen an \"elephant\" before. Since word embeddings capture attributes of objects ", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "The rest of the paper is organized as follows. In Sect. 2 we review related research. In Sect. 3 we first introduce the task of predicting spatial templates and then present two simple neural models. Then, in Sect. 4, we describe our experimental setup. In Sect. 5 we present and discuss our results. Finally, in Sect. 6 we summarize the contributions of this article.", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "Spatial processing has drawn significant attention from the cognitive ", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "Spatial templates. Earlier approaches have predominantly considered rule-based spatial representations ", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "Additionally, while they build a spatial template for each (explicit) Relationship, we build a template for each (Subject, Relationship, Object) combination, allowing the template to be determined by the interaction/composition of the Subject, Relationship and Object instead of the Relationship alone. Additionally, the model from ", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "Leveraging spatial knowledge in tasks. It has been shown that knowledge of the spatial structure in images improves the task of image captioning ", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "Common sense spatial knowledge. ", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "Image generation. Although models that generate images from text exist (e.g., DRAW model ", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "As discussed, spatial knowledge can improve a wide range of tasks ", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "Obj.", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "Subj.", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "x y Elliott and Keller 2013). This suggests that the predictions of our models can be used as spatial common sense input for methods that rely on good spatial priors. Additionally, existing methods require the spatial information to be present in the data and lack the capacity to extrapolate/generalize. Thus, in this paper we focus, first, on showing that generalizing spatial arrangements to unseen objects and object-relation-object combinations is possible and, second, on ensuring that the predicted templates are accurate by performing several quantitative and qualitative evaluations.", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "3 Proposed task and model", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "To learn spatial templates, we propose the task of predicting the 2D relative spatial arrangement of two objects under a relationship given a structured text input of the form (Subject, Relationship, Object)-henceforth abbreviated as (S, R, O). Let us denote the 2D coordinates of the center (\"c\") of the Object\u0027s box as", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "y \u2208 R are the horizontal and vertical components respectively. Let", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "x , O b y ] \u2208 R 2 be half of the width (O b x ) and half of the height (O b y ) of the Object\u0027s box (\"b\"). We employ a similar notation for the Subject (S c , S b ), and model predictions are denoted with a hat O c , O b . The task consists in predicting the Object\u0027s location and size", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "given the structured text input (S, R, O) and the location S c and size S b of the Subject ", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "2 Crucially, we notice that knowing the Subject\u0027s coordinates is not a requirement for generating templates (but only for evaluating against the ground truth) since inputting arbitrary coordinates (e.g., S c =[0.5, 0.5]) still enables visualizing relative object locations. Additionally, we input the Subject\u0027s size in order to provide a reference size to the model. However, we find that without this input, the model still learns to predict an \"average size\" for each Object.", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "Hence, the goal is to answer common sense spatial questions such as \"if a man is feeding a horse, where is the man relative to the horse?\" or \"where would a child wear her shoes?\"", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "To build a mapping from the input to the output in our task (Sect. 3.1) we propose two simple neural models ( ", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "are one-hot encodings of S, R, O (a.k.a. one-of-k encoding, i.e., a sparse vector with 0 everywhere except for a 1 at the position of the k-th word) and", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "This layer represents objects and relationships as continuous features, enabling thus to introduce external knowledge of unseen objects as features. The embeddings are then concatenated with the Subject center S c and size", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "which is inputted to a stack of hidden layers that compose S, R and O into a joint hidden representation z h :", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "where f (\u2022) is an element-wise non-linear function and W h and b h are the weight matrix and bias respectively. 3 (ii) Output and loss. We consider two different possibilities for the output\u0177 that follows immediately after the last layer: ", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "These models are conceptually different and have different capabilities. While the REG model outputs \"crisp\" pointwise predictions, PIX can model more diffuse spatial templates where the location of the object has more variability, e.g., in (man, flying, kite) the \"kite\" can easily move around. Notice that in contrast with convolutional neural networks (CNNs) our approach does not make use of the image pixels ( ", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "We employ a 10-fold cross-validation (CV) setting. Data are randomly split into 10 disjoint parts and 10% is employed for testing and 90% for training, repeating this for each of the 10 folds. Reported results are averages over the 10 folds.", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "We use the Visual Genome dataset ", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "dog, catches, frisbee boy, feeds, giraffe man, throws, frisbee cat, wears glasses ", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "We consider the following subsets of the Visual Genome data to evaluate performance. (i) Raw data: Simply the unfiltered instances from the Visual Genome data (Sect. 4.1). This set contains a substantial proportion of meaningless (e.g., (nose, almost, touching)) and irrelevant (e.g., (sign, says, gate 2)) instances. (ii) Generalized Triplets: We pick at random 100 combinations (S, R, O) among the 1,000 most frequent implicit combinations in Visual Genome. This yields \u223c25K instances such as (person, holding, racket), (man, flying, kite), etc. 4 (iii) Generalized Words: We randomly choose 25 objects (e.g., \"woman\", \"apple\", etc.) 5 among the 100 most frequent objects in Visual Genome and take all the instances (\u223c130K) that contain any of these words. For example, since \"apple\" is in our list, e.g., (cat, sniffing, apple) is kept. ", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "When enforcing generalization conditions in our experiments, all combinations from sets (ii) and (iii) are removed from the training data to prevent the model from seeing them. 7 Even without imposing generalization conditions (or when testing with Raw data), reported results are always on unseen instances-yet the combinations (S, R, O) may have been seen during training (e.g., in different images). All sets above contain exclusively implicit spatial language, although an analogous version of the Raw set where R are explicit spatial prepositions is also considered in our experiments.", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "The coordinates of bounding boxes in the images are normalized by the width and height of the image. Thus, S c , O c \u2208 [0, 1] 2 . Additionally, we notice that the distinction between left and right is arbitrary regarding the semantics of the image ", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "The REG model directly outputs Object coordinates, while PIX outputs 2D heatmaps. We however enable evaluating the PIX model with regression/classification metrics by taking the point of maximum activation (or their average, if there are many) as the Object center O c . The predicted Object size O b is not estimated. We use the following performance metrics.", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "(A) Intersection over Union ", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "where B O and B O are the predicted and ground truth Object bounding boxes, respectively. If the IoU is larger than 50%, the prediction is counted as correct. It must be noted that our setting is not comparable to object detection (nor our results) since we do not employ the image as input (but text) and thus we cannot leverage the pixels to predict the Object\u0027s location.", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "(B) Regression. We consider standard regression metrics. ", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "(D) Pixel (macro) accuracy. The IoU on pixels is equivalent to binary pixel accuracy. However, this is not a good measure here, where class 1 (Object box) comprises, on average, only 5% of the pixels. Thus, a constant prediction of zeros everywhere would obtain 95% accuracy. Hence, we consider macro-averaged pixel accuracy, a.k.a. mean IoU (mIoU) ", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "We use 300-dimensional GloVe word embeddings (Pennington, Socher, and Manning 2014) pre-trained on the Common Crawl corpus (consisting of 840B-tokens), which we obtain from the authors\u0027 website. 8", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "Our experiments are implemented in Python 2.7 and we use Keras deep learning framework for our models ", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "We consider the evaluation sets from Sect. 4.2 and the following variations of PIX and REG models (Sect. 3.2). The subindex EMB denotes a model that employs GloVe embeddings and RND a model with embeddings randomly drawn from a dimension-wise normal distribution of mean (\u03bc) and standard deviation (\u03c3) equal to those of the GloVe embeddings, preserving the original dimensionality (d=300). A third type employs one-hot vectors (1H). We additionally  consider a control method (ctrl) that outputs random normal predictions of \u03bc and \u03c3 equal to the dimension-wise mean and standard deviation of the training targets. We test statistical significance with a Friedman rank test and post hoc Nemenyi tests on the results of the 10 folds. We indicate with an asterisk * in the tables when a method is significantly better than the rest (p \u003c 0.01) within the same model (PIX or REG). ", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "To ensure that model predictions are meaningful and interpretable, we further validate the quantitative results above with a qualitative evaluation. Notice that all plots in ", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "We study how the weights of the model provide insight into the spatial properties of words. To obtain more interpretable weights, we learn a REG 9 model without hidden layers, resulting in only an embedding layer followed by a linear output layer\u0177 = W out u + b out , where", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "By using one-hot encodings w S , w R , w O , the concatenation layer u becomes of size |V S | + |V R | + |V O | + 2 + 2. E.g., if \"wearing\" has one-hot index j in the Relationships\u0027 vocabulary (V R ), its index in the concatenation layer is |V S | + j. The product W out u is a 4-dimensional vector, where its i-th component is the product of the i-th row of W out with the vector u. Thus, the component |V S |+j of the i-th row of W out gives us the influence of the j-th relationship (i.e., \"wearing\") on the i-th dimension of the output\u0177 =   ", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "Overall, this paper provides insight into the fundamental differences between implicit and explicit spatial language and extends the concept of spatial templates to implicit spatial language, the understanding of which requires common sense spatial knowledge about objects and actions. We define the task of predicting relative spatial arrangements of two objects under a relationship and present two embedding-based neural models that attain promising performance, proving that spatial templates can be accurately predicted from implicit spatial language. Remarkably, our models generalize well, predicting correctly unseen (generalized) object-relationshipobject combinations. Furthermore, the acquired common sense spatial knowledge-aided with word embeddingsallows the model to correctly predict templates for unseen words. Finally, we show that the weights of the model provide great insight into the spatial connotations of words.", "weight": 0.87}, {"from": "Model hyperparameters and implementation", "to": "A first limitation of our approach is the fully supervised setting where the models are trained using images with detected ground truth objects and parsed text-which aims at keeping the design clean in this first study on implicit spatial templates. Notice however that methods to automatically parse images and text exist. In future work, we aim at implementing our approach in a weakly supervised setting. A second limitation is the 2D spatial treatment of the actual 3D world. It is worth noting however, that our models (PIX and REG) and setting trivially generalize to 3D if appropriate data are available.", "weight": 0.87}, {"from": "Your Visual Research Paper", "to": "Results and discussion", "weight": 0.97}, {"from": "Results and discussion", "to": "Spatial understanding is a fundamental problem with widereaching real-world applications. The representation of spatial knowledge is often modeled with spatial templates, i.e., regions of acceptability of two objects under an explicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with prior work that restricts spatial templates to explicit spatial prepositions (e.g., \"glass on table\"), here we extend this concept to implicit spatial language, i.e., those relationships (generally actions) for which the spatial arrangement of the objects is only implicitly implied (e.g., \"man riding horse\"). In contrast with explicit relationships, predicting spatial arrangements from implicit spatial language requires significant common sense spatial understanding. Here, we introduce the task of predicting spatial templates for two objects under a relationship, which can be seen as a spatial question-answering task with a (2D) continuous output (\"where is the man w.r.t. a horse when the man is walking the horse?\"). We present two simple neural-based models that leverage annotated images and structured text to learn this task. The good performance of these models reveals that spatial locations are to a large extent predictable from implicit spatial language. Crucially, the models attain similar performance in a challenging generalized setting, where the object-relation-object combinations (e.g.,\"man walking dog\") have never been seen before. Next, we go one step further by presenting the models with unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging word embeddings enables the models to output accurate spatial predictions, proving that the models acquire solid common sense spatial knowledge allowing for such generalization.", "weight": 0.87}, {"from": "Results and discussion", "to": "To provide machines with common sense is one of the major long term goals of artificial intelligence. Common sense knowledge regards knowledge that humans have acquired through a lifetime of experiences. It is crucial in language understanding because a lot of content needed for correct understanding is not expressed explicitly but resides in the mind of communicator and audience. In addition, humans rely on their common sense knowledge when performing a variety of tasks including interpreting images, navigation and reasoning, to name a few. Representing and understanding spatial knowledge are in fact imperative for any agent (human, animal or robot) that navigates in a physical world. In this paper, we are interested in acquiring spatial commonsense knowledge from language paired with visual data.", "weight": 0.87}, {"from": "Results and discussion", "to": "Computational and cognitive models often handle spatial representations as spatial templates or regions of acceptability for two objects under an explicit (a.k.a. deictic) spatial preposition such as \"on\", \"below\" or \"left\" ", "weight": 0.87}, {"from": "Results and discussion", "to": "Predicting spatial templates for implicit relationships is notably more challenging than for explicit relationships. Firstly, whereas there are only a few tens of explicit spatial prepositions, there exist thousands of actions, entailing thus a drastic increase in the sparsity of (object 1 , relationship, object 2 ) combinations. Secondly, the complexity of the task radically increases in implicit language. More precisely, while explicit spatial prepositions 1 are highly deterministic about the spatial arrangements (e.g., (object 1 , below, object 2 ) unequivocally implies that object 1 is relatively lower than object 2 ), actions generally are not. E.g., the relative spatial configuration of \"man\" and the object is clearly distinct in (man, pulling, kite) than in (man, pulling, luggage) yet the action is the same. Contrarily, other relationships such as \"jumping\" are highly informative about the spatial template, i.e., in (object 1 , jumping, object 2 ), object 2 is in a lower position than object 1 . Hence, unlike explicit relationships, predicting spatial layouts from implicit spatial language requires spatial common sense knowledge about the objects, actions and their interaction, which suggests the need of learning to compose the triplet (Subject, Relationship, Object) as a whole instead of learning a template for each Relationship.", "weight": 0.87}, {"from": "Results and discussion", "to": "To systematically study these questions, we propose the task of predicting the relative spatial locations of two objects given a structured text input (Subject, Relationship, Object). We introduce two simple neural-based models trained from annotated images that successfully address the two challenges of implicit spatial language discussed above. Our quantitative evaluation reveals that spatial templates can be reliably predicted from implicit spatial language-as accurately as from explicit spatial language. We also show that our models generalize well to templates of unseen combinations, e.g., predicting (man, riding, elephant) without having been exposed to such scene before, tackling thus the challenge of sparsity. Furthermore, by leveraging word embeddings, the models can correctly generalize to spatial templates with unseen words, e.g., predicting (man, riding, elephant) without having ever seen an \"elephant\" before. Since word embeddings capture attributes of objects ", "weight": 0.87}, {"from": "Results and discussion", "to": "The rest of the paper is organized as follows. In Sect. 2 we review related research. In Sect. 3 we first introduce the task of predicting spatial templates and then present two simple neural models. Then, in Sect. 4, we describe our experimental setup. In Sect. 5 we present and discuss our results. Finally, in Sect. 6 we summarize the contributions of this article.", "weight": 0.87}, {"from": "Results and discussion", "to": "Spatial processing has drawn significant attention from the cognitive ", "weight": 0.87}, {"from": "Results and discussion", "to": "Spatial templates. Earlier approaches have predominantly considered rule-based spatial representations ", "weight": 0.87}, {"from": "Results and discussion", "to": "Additionally, while they build a spatial template for each (explicit) Relationship, we build a template for each (Subject, Relationship, Object) combination, allowing the template to be determined by the interaction/composition of the Subject, Relationship and Object instead of the Relationship alone. Additionally, the model from ", "weight": 0.87}, {"from": "Results and discussion", "to": "Leveraging spatial knowledge in tasks. It has been shown that knowledge of the spatial structure in images improves the task of image captioning ", "weight": 0.87}, {"from": "Results and discussion", "to": "Common sense spatial knowledge. ", "weight": 0.87}, {"from": "Results and discussion", "to": "Image generation. Although models that generate images from text exist (e.g., DRAW model ", "weight": 0.87}, {"from": "Results and discussion", "to": "As discussed, spatial knowledge can improve a wide range of tasks ", "weight": 0.87}, {"from": "Results and discussion", "to": "Obj.", "weight": 0.87}, {"from": "Results and discussion", "to": "Subj.", "weight": 0.87}, {"from": "Results and discussion", "to": "x y Elliott and Keller 2013). This suggests that the predictions of our models can be used as spatial common sense input for methods that rely on good spatial priors. Additionally, existing methods require the spatial information to be present in the data and lack the capacity to extrapolate/generalize. Thus, in this paper we focus, first, on showing that generalizing spatial arrangements to unseen objects and object-relation-object combinations is possible and, second, on ensuring that the predicted templates are accurate by performing several quantitative and qualitative evaluations.", "weight": 0.87}, {"from": "Results and discussion", "to": "3 Proposed task and model", "weight": 0.87}, {"from": "Results and discussion", "to": "To learn spatial templates, we propose the task of predicting the 2D relative spatial arrangement of two objects under a relationship given a structured text input of the form (Subject, Relationship, Object)-henceforth abbreviated as (S, R, O). Let us denote the 2D coordinates of the center (\"c\") of the Object\u0027s box as", "weight": 0.87}, {"from": "Results and discussion", "to": "y \u2208 R are the horizontal and vertical components respectively. Let", "weight": 0.87}, {"from": "Results and discussion", "to": "x , O b y ] \u2208 R 2 be half of the width (O b x ) and half of the height (O b y ) of the Object\u0027s box (\"b\"). We employ a similar notation for the Subject (S c , S b ), and model predictions are denoted with a hat O c , O b . The task consists in predicting the Object\u0027s location and size", "weight": 0.87}, {"from": "Results and discussion", "to": "given the structured text input (S, R, O) and the location S c and size S b of the Subject ", "weight": 0.87}, {"from": "Results and discussion", "to": "2 Crucially, we notice that knowing the Subject\u0027s coordinates is not a requirement for generating templates (but only for evaluating against the ground truth) since inputting arbitrary coordinates (e.g., S c =[0.5, 0.5]) still enables visualizing relative object locations. Additionally, we input the Subject\u0027s size in order to provide a reference size to the model. However, we find that without this input, the model still learns to predict an \"average size\" for each Object.", "weight": 0.87}, {"from": "Results and discussion", "to": "Hence, the goal is to answer common sense spatial questions such as \"if a man is feeding a horse, where is the man relative to the horse?\" or \"where would a child wear her shoes?\"", "weight": 0.87}, {"from": "Results and discussion", "to": "To build a mapping from the input to the output in our task (Sect. 3.1) we propose two simple neural models ( ", "weight": 0.87}, {"from": "Results and discussion", "to": "are one-hot encodings of S, R, O (a.k.a. one-of-k encoding, i.e., a sparse vector with 0 everywhere except for a 1 at the position of the k-th word) and", "weight": 0.87}, {"from": "Results and discussion", "to": "This layer represents objects and relationships as continuous features, enabling thus to introduce external knowledge of unseen objects as features. The embeddings are then concatenated with the Subject center S c and size", "weight": 0.87}, {"from": "Results and discussion", "to": "which is inputted to a stack of hidden layers that compose S, R and O into a joint hidden representation z h :", "weight": 0.87}, {"from": "Results and discussion", "to": "where f (\u2022) is an element-wise non-linear function and W h and b h are the weight matrix and bias respectively. 3 (ii) Output and loss. We consider two different possibilities for the output\u0177 that follows immediately after the last layer: ", "weight": 0.87}, {"from": "Results and discussion", "to": "These models are conceptually different and have different capabilities. While the REG model outputs \"crisp\" pointwise predictions, PIX can model more diffuse spatial templates where the location of the object has more variability, e.g., in (man, flying, kite) the \"kite\" can easily move around. Notice that in contrast with convolutional neural networks (CNNs) our approach does not make use of the image pixels ( ", "weight": 0.87}, {"from": "Results and discussion", "to": "We employ a 10-fold cross-validation (CV) setting. Data are randomly split into 10 disjoint parts and 10% is employed for testing and 90% for training, repeating this for each of the 10 folds. Reported results are averages over the 10 folds.", "weight": 0.87}, {"from": "Results and discussion", "to": "We use the Visual Genome dataset ", "weight": 0.87}, {"from": "Results and discussion", "to": "dog, catches, frisbee boy, feeds, giraffe man, throws, frisbee cat, wears glasses ", "weight": 0.87}, {"from": "Results and discussion", "to": "We consider the following subsets of the Visual Genome data to evaluate performance. (i) Raw data: Simply the unfiltered instances from the Visual Genome data (Sect. 4.1). This set contains a substantial proportion of meaningless (e.g., (nose, almost, touching)) and irrelevant (e.g., (sign, says, gate 2)) instances. (ii) Generalized Triplets: We pick at random 100 combinations (S, R, O) among the 1,000 most frequent implicit combinations in Visual Genome. This yields \u223c25K instances such as (person, holding, racket), (man, flying, kite), etc. 4 (iii) Generalized Words: We randomly choose 25 objects (e.g., \"woman\", \"apple\", etc.) 5 among the 100 most frequent objects in Visual Genome and take all the instances (\u223c130K) that contain any of these words. For example, since \"apple\" is in our list, e.g., (cat, sniffing, apple) is kept. ", "weight": 0.87}, {"from": "Results and discussion", "to": "When enforcing generalization conditions in our experiments, all combinations from sets (ii) and (iii) are removed from the training data to prevent the model from seeing them. 7 Even without imposing generalization conditions (or when testing with Raw data), reported results are always on unseen instances-yet the combinations (S, R, O) may have been seen during training (e.g., in different images). All sets above contain exclusively implicit spatial language, although an analogous version of the Raw set where R are explicit spatial prepositions is also considered in our experiments.", "weight": 0.87}, {"from": "Results and discussion", "to": "The coordinates of bounding boxes in the images are normalized by the width and height of the image. Thus, S c , O c \u2208 [0, 1] 2 . Additionally, we notice that the distinction between left and right is arbitrary regarding the semantics of the image ", "weight": 0.87}, {"from": "Results and discussion", "to": "The REG model directly outputs Object coordinates, while PIX outputs 2D heatmaps. We however enable evaluating the PIX model with regression/classification metrics by taking the point of maximum activation (or their average, if there are many) as the Object center O c . The predicted Object size O b is not estimated. We use the following performance metrics.", "weight": 0.87}, {"from": "Results and discussion", "to": "(A) Intersection over Union ", "weight": 0.87}, {"from": "Results and discussion", "to": "where B O and B O are the predicted and ground truth Object bounding boxes, respectively. If the IoU is larger than 50%, the prediction is counted as correct. It must be noted that our setting is not comparable to object detection (nor our results) since we do not employ the image as input (but text) and thus we cannot leverage the pixels to predict the Object\u0027s location.", "weight": 0.87}, {"from": "Results and discussion", "to": "(B) Regression. We consider standard regression metrics. ", "weight": 0.87}, {"from": "Results and discussion", "to": "(D) Pixel (macro) accuracy. The IoU on pixels is equivalent to binary pixel accuracy. However, this is not a good measure here, where class 1 (Object box) comprises, on average, only 5% of the pixels. Thus, a constant prediction of zeros everywhere would obtain 95% accuracy. Hence, we consider macro-averaged pixel accuracy, a.k.a. mean IoU (mIoU) ", "weight": 0.87}, {"from": "Results and discussion", "to": "We use 300-dimensional GloVe word embeddings (Pennington, Socher, and Manning 2014) pre-trained on the Common Crawl corpus (consisting of 840B-tokens), which we obtain from the authors\u0027 website. 8", "weight": 0.87}, {"from": "Results and discussion", "to": "Our experiments are implemented in Python 2.7 and we use Keras deep learning framework for our models ", "weight": 0.87}, {"from": "Results and discussion", "to": "We consider the evaluation sets from Sect. 4.2 and the following variations of PIX and REG models (Sect. 3.2). The subindex EMB denotes a model that employs GloVe embeddings and RND a model with embeddings randomly drawn from a dimension-wise normal distribution of mean (\u03bc) and standard deviation (\u03c3) equal to those of the GloVe embeddings, preserving the original dimensionality (d=300). A third type employs one-hot vectors (1H). We additionally  consider a control method (ctrl) that outputs random normal predictions of \u03bc and \u03c3 equal to the dimension-wise mean and standard deviation of the training targets. We test statistical significance with a Friedman rank test and post hoc Nemenyi tests on the results of the 10 folds. We indicate with an asterisk * in the tables when a method is significantly better than the rest (p \u003c 0.01) within the same model (PIX or REG). ", "weight": 0.87}, {"from": "Results and discussion", "to": "To ensure that model predictions are meaningful and interpretable, we further validate the quantitative results above with a qualitative evaluation. Notice that all plots in ", "weight": 0.87}, {"from": "Results and discussion", "to": "We study how the weights of the model provide insight into the spatial properties of words. To obtain more interpretable weights, we learn a REG 9 model without hidden layers, resulting in only an embedding layer followed by a linear output layer\u0177 = W out u + b out , where", "weight": 0.87}, {"from": "Results and discussion", "to": "By using one-hot encodings w S , w R , w O , the concatenation layer u becomes of size |V S | + |V R | + |V O | + 2 + 2. E.g., if \"wearing\" has one-hot index j in the Relationships\u0027 vocabulary (V R ), its index in the concatenation layer is |V S | + j. The product W out u is a 4-dimensional vector, where its i-th component is the product of the i-th row of W out with the vector u. Thus, the component |V S |+j of the i-th row of W out gives us the influence of the j-th relationship (i.e., \"wearing\") on the i-th dimension of the output\u0177 =   ", "weight": 0.87}, {"from": "Results and discussion", "to": "Overall, this paper provides insight into the fundamental differences between implicit and explicit spatial language and extends the concept of spatial templates to implicit spatial language, the understanding of which requires common sense spatial knowledge about objects and actions. We define the task of predicting relative spatial arrangements of two objects under a relationship and present two embedding-based neural models that attain promising performance, proving that spatial templates can be accurately predicted from implicit spatial language. Remarkably, our models generalize well, predicting correctly unseen (generalized) object-relationshipobject combinations. Furthermore, the acquired common sense spatial knowledge-aided with word embeddingsallows the model to correctly predict templates for unseen words. Finally, we show that the weights of the model provide great insight into the spatial connotations of words.", "weight": 0.87}, {"from": "Results and discussion", "to": "A first limitation of our approach is the fully supervised setting where the models are trained using images with detected ground truth objects and parsed text-which aims at keeping the design clean in this first study on implicit spatial templates. Notice however that methods to automatically parse images and text exist. In future work, we aim at implementing our approach in a weakly supervised setting. A second limitation is the 2D spatial treatment of the actual 3D world. It is worth noting however, that our models (PIX and REG) and setting trivially generalize to 3D if appropriate data are available.", "weight": 0.87}, {"from": "Your Visual Research Paper", "to": "Evaluation with raw data", "weight": 0.97}, {"from": "Evaluation with raw data", "to": "Spatial understanding is a fundamental problem with widereaching real-world applications. The representation of spatial knowledge is often modeled with spatial templates, i.e., regions of acceptability of two objects under an explicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with prior work that restricts spatial templates to explicit spatial prepositions (e.g., \"glass on table\"), here we extend this concept to implicit spatial language, i.e., those relationships (generally actions) for which the spatial arrangement of the objects is only implicitly implied (e.g., \"man riding horse\"). In contrast with explicit relationships, predicting spatial arrangements from implicit spatial language requires significant common sense spatial understanding. Here, we introduce the task of predicting spatial templates for two objects under a relationship, which can be seen as a spatial question-answering task with a (2D) continuous output (\"where is the man w.r.t. a horse when the man is walking the horse?\"). We present two simple neural-based models that leverage annotated images and structured text to learn this task. The good performance of these models reveals that spatial locations are to a large extent predictable from implicit spatial language. Crucially, the models attain similar performance in a challenging generalized setting, where the object-relation-object combinations (e.g.,\"man walking dog\") have never been seen before. Next, we go one step further by presenting the models with unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging word embeddings enables the models to output accurate spatial predictions, proving that the models acquire solid common sense spatial knowledge allowing for such generalization.", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "To provide machines with common sense is one of the major long term goals of artificial intelligence. Common sense knowledge regards knowledge that humans have acquired through a lifetime of experiences. It is crucial in language understanding because a lot of content needed for correct understanding is not expressed explicitly but resides in the mind of communicator and audience. In addition, humans rely on their common sense knowledge when performing a variety of tasks including interpreting images, navigation and reasoning, to name a few. Representing and understanding spatial knowledge are in fact imperative for any agent (human, animal or robot) that navigates in a physical world. In this paper, we are interested in acquiring spatial commonsense knowledge from language paired with visual data.", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "Computational and cognitive models often handle spatial representations as spatial templates or regions of acceptability for two objects under an explicit (a.k.a. deictic) spatial preposition such as \"on\", \"below\" or \"left\" ", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "Predicting spatial templates for implicit relationships is notably more challenging than for explicit relationships. Firstly, whereas there are only a few tens of explicit spatial prepositions, there exist thousands of actions, entailing thus a drastic increase in the sparsity of (object 1 , relationship, object 2 ) combinations. Secondly, the complexity of the task radically increases in implicit language. More precisely, while explicit spatial prepositions 1 are highly deterministic about the spatial arrangements (e.g., (object 1 , below, object 2 ) unequivocally implies that object 1 is relatively lower than object 2 ), actions generally are not. E.g., the relative spatial configuration of \"man\" and the object is clearly distinct in (man, pulling, kite) than in (man, pulling, luggage) yet the action is the same. Contrarily, other relationships such as \"jumping\" are highly informative about the spatial template, i.e., in (object 1 , jumping, object 2 ), object 2 is in a lower position than object 1 . Hence, unlike explicit relationships, predicting spatial layouts from implicit spatial language requires spatial common sense knowledge about the objects, actions and their interaction, which suggests the need of learning to compose the triplet (Subject, Relationship, Object) as a whole instead of learning a template for each Relationship.", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "To systematically study these questions, we propose the task of predicting the relative spatial locations of two objects given a structured text input (Subject, Relationship, Object). We introduce two simple neural-based models trained from annotated images that successfully address the two challenges of implicit spatial language discussed above. Our quantitative evaluation reveals that spatial templates can be reliably predicted from implicit spatial language-as accurately as from explicit spatial language. We also show that our models generalize well to templates of unseen combinations, e.g., predicting (man, riding, elephant) without having been exposed to such scene before, tackling thus the challenge of sparsity. Furthermore, by leveraging word embeddings, the models can correctly generalize to spatial templates with unseen words, e.g., predicting (man, riding, elephant) without having ever seen an \"elephant\" before. Since word embeddings capture attributes of objects ", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "The rest of the paper is organized as follows. In Sect. 2 we review related research. In Sect. 3 we first introduce the task of predicting spatial templates and then present two simple neural models. Then, in Sect. 4, we describe our experimental setup. In Sect. 5 we present and discuss our results. Finally, in Sect. 6 we summarize the contributions of this article.", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "Spatial processing has drawn significant attention from the cognitive ", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "Spatial templates. Earlier approaches have predominantly considered rule-based spatial representations ", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "Additionally, while they build a spatial template for each (explicit) Relationship, we build a template for each (Subject, Relationship, Object) combination, allowing the template to be determined by the interaction/composition of the Subject, Relationship and Object instead of the Relationship alone. Additionally, the model from ", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "Leveraging spatial knowledge in tasks. It has been shown that knowledge of the spatial structure in images improves the task of image captioning ", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "Common sense spatial knowledge. ", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "Image generation. Although models that generate images from text exist (e.g., DRAW model ", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "As discussed, spatial knowledge can improve a wide range of tasks ", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "Obj.", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "Subj.", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "x y Elliott and Keller 2013). This suggests that the predictions of our models can be used as spatial common sense input for methods that rely on good spatial priors. Additionally, existing methods require the spatial information to be present in the data and lack the capacity to extrapolate/generalize. Thus, in this paper we focus, first, on showing that generalizing spatial arrangements to unseen objects and object-relation-object combinations is possible and, second, on ensuring that the predicted templates are accurate by performing several quantitative and qualitative evaluations.", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "3 Proposed task and model", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "To learn spatial templates, we propose the task of predicting the 2D relative spatial arrangement of two objects under a relationship given a structured text input of the form (Subject, Relationship, Object)-henceforth abbreviated as (S, R, O). Let us denote the 2D coordinates of the center (\"c\") of the Object\u0027s box as", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "y \u2208 R are the horizontal and vertical components respectively. Let", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "x , O b y ] \u2208 R 2 be half of the width (O b x ) and half of the height (O b y ) of the Object\u0027s box (\"b\"). We employ a similar notation for the Subject (S c , S b ), and model predictions are denoted with a hat O c , O b . The task consists in predicting the Object\u0027s location and size", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "given the structured text input (S, R, O) and the location S c and size S b of the Subject ", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "2 Crucially, we notice that knowing the Subject\u0027s coordinates is not a requirement for generating templates (but only for evaluating against the ground truth) since inputting arbitrary coordinates (e.g., S c =[0.5, 0.5]) still enables visualizing relative object locations. Additionally, we input the Subject\u0027s size in order to provide a reference size to the model. However, we find that without this input, the model still learns to predict an \"average size\" for each Object.", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "Hence, the goal is to answer common sense spatial questions such as \"if a man is feeding a horse, where is the man relative to the horse?\" or \"where would a child wear her shoes?\"", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "To build a mapping from the input to the output in our task (Sect. 3.1) we propose two simple neural models ( ", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "are one-hot encodings of S, R, O (a.k.a. one-of-k encoding, i.e., a sparse vector with 0 everywhere except for a 1 at the position of the k-th word) and", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "This layer represents objects and relationships as continuous features, enabling thus to introduce external knowledge of unseen objects as features. The embeddings are then concatenated with the Subject center S c and size", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "which is inputted to a stack of hidden layers that compose S, R and O into a joint hidden representation z h :", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "where f (\u2022) is an element-wise non-linear function and W h and b h are the weight matrix and bias respectively. 3 (ii) Output and loss. We consider two different possibilities for the output\u0177 that follows immediately after the last layer: ", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "These models are conceptually different and have different capabilities. While the REG model outputs \"crisp\" pointwise predictions, PIX can model more diffuse spatial templates where the location of the object has more variability, e.g., in (man, flying, kite) the \"kite\" can easily move around. Notice that in contrast with convolutional neural networks (CNNs) our approach does not make use of the image pixels ( ", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "We employ a 10-fold cross-validation (CV) setting. Data are randomly split into 10 disjoint parts and 10% is employed for testing and 90% for training, repeating this for each of the 10 folds. Reported results are averages over the 10 folds.", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "We use the Visual Genome dataset ", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "dog, catches, frisbee boy, feeds, giraffe man, throws, frisbee cat, wears glasses ", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "We consider the following subsets of the Visual Genome data to evaluate performance. (i) Raw data: Simply the unfiltered instances from the Visual Genome data (Sect. 4.1). This set contains a substantial proportion of meaningless (e.g., (nose, almost, touching)) and irrelevant (e.g., (sign, says, gate 2)) instances. (ii) Generalized Triplets: We pick at random 100 combinations (S, R, O) among the 1,000 most frequent implicit combinations in Visual Genome. This yields \u223c25K instances such as (person, holding, racket), (man, flying, kite), etc. 4 (iii) Generalized Words: We randomly choose 25 objects (e.g., \"woman\", \"apple\", etc.) 5 among the 100 most frequent objects in Visual Genome and take all the instances (\u223c130K) that contain any of these words. For example, since \"apple\" is in our list, e.g., (cat, sniffing, apple) is kept. ", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "When enforcing generalization conditions in our experiments, all combinations from sets (ii) and (iii) are removed from the training data to prevent the model from seeing them. 7 Even without imposing generalization conditions (or when testing with Raw data), reported results are always on unseen instances-yet the combinations (S, R, O) may have been seen during training (e.g., in different images). All sets above contain exclusively implicit spatial language, although an analogous version of the Raw set where R are explicit spatial prepositions is also considered in our experiments.", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "The coordinates of bounding boxes in the images are normalized by the width and height of the image. Thus, S c , O c \u2208 [0, 1] 2 . Additionally, we notice that the distinction between left and right is arbitrary regarding the semantics of the image ", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "The REG model directly outputs Object coordinates, while PIX outputs 2D heatmaps. We however enable evaluating the PIX model with regression/classification metrics by taking the point of maximum activation (or their average, if there are many) as the Object center O c . The predicted Object size O b is not estimated. We use the following performance metrics.", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "(A) Intersection over Union ", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "where B O and B O are the predicted and ground truth Object bounding boxes, respectively. If the IoU is larger than 50%, the prediction is counted as correct. It must be noted that our setting is not comparable to object detection (nor our results) since we do not employ the image as input (but text) and thus we cannot leverage the pixels to predict the Object\u0027s location.", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "(B) Regression. We consider standard regression metrics. ", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "(D) Pixel (macro) accuracy. The IoU on pixels is equivalent to binary pixel accuracy. However, this is not a good measure here, where class 1 (Object box) comprises, on average, only 5% of the pixels. Thus, a constant prediction of zeros everywhere would obtain 95% accuracy. Hence, we consider macro-averaged pixel accuracy, a.k.a. mean IoU (mIoU) ", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "We use 300-dimensional GloVe word embeddings (Pennington, Socher, and Manning 2014) pre-trained on the Common Crawl corpus (consisting of 840B-tokens), which we obtain from the authors\u0027 website. 8", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "Our experiments are implemented in Python 2.7 and we use Keras deep learning framework for our models ", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "We consider the evaluation sets from Sect. 4.2 and the following variations of PIX and REG models (Sect. 3.2). The subindex EMB denotes a model that employs GloVe embeddings and RND a model with embeddings randomly drawn from a dimension-wise normal distribution of mean (\u03bc) and standard deviation (\u03c3) equal to those of the GloVe embeddings, preserving the original dimensionality (d=300). A third type employs one-hot vectors (1H). We additionally  consider a control method (ctrl) that outputs random normal predictions of \u03bc and \u03c3 equal to the dimension-wise mean and standard deviation of the training targets. We test statistical significance with a Friedman rank test and post hoc Nemenyi tests on the results of the 10 folds. We indicate with an asterisk * in the tables when a method is significantly better than the rest (p \u003c 0.01) within the same model (PIX or REG). ", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "To ensure that model predictions are meaningful and interpretable, we further validate the quantitative results above with a qualitative evaluation. Notice that all plots in ", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "We study how the weights of the model provide insight into the spatial properties of words. To obtain more interpretable weights, we learn a REG 9 model without hidden layers, resulting in only an embedding layer followed by a linear output layer\u0177 = W out u + b out , where", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "By using one-hot encodings w S , w R , w O , the concatenation layer u becomes of size |V S | + |V R | + |V O | + 2 + 2. E.g., if \"wearing\" has one-hot index j in the Relationships\u0027 vocabulary (V R ), its index in the concatenation layer is |V S | + j. The product W out u is a 4-dimensional vector, where its i-th component is the product of the i-th row of W out with the vector u. Thus, the component |V S |+j of the i-th row of W out gives us the influence of the j-th relationship (i.e., \"wearing\") on the i-th dimension of the output\u0177 =   ", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "Overall, this paper provides insight into the fundamental differences between implicit and explicit spatial language and extends the concept of spatial templates to implicit spatial language, the understanding of which requires common sense spatial knowledge about objects and actions. We define the task of predicting relative spatial arrangements of two objects under a relationship and present two embedding-based neural models that attain promising performance, proving that spatial templates can be accurately predicted from implicit spatial language. Remarkably, our models generalize well, predicting correctly unseen (generalized) object-relationshipobject combinations. Furthermore, the acquired common sense spatial knowledge-aided with word embeddingsallows the model to correctly predict templates for unseen words. Finally, we show that the weights of the model provide great insight into the spatial connotations of words.", "weight": 0.87}, {"from": "Evaluation with raw data", "to": "A first limitation of our approach is the fully supervised setting where the models are trained using images with detected ground truth objects and parsed text-which aims at keeping the design clean in this first study on implicit spatial templates. Notice however that methods to automatically parse images and text exist. In future work, we aim at implementing our approach in a weakly supervised setting. A second limitation is the 2D spatial treatment of the actual 3D world. It is worth noting however, that our models (PIX and REG) and setting trivially generalize to 3D if appropriate data are available.", "weight": 0.87}, {"from": "Your Visual Research Paper", "to": "Generalized evaluations", "weight": 0.97}, {"from": "Generalized evaluations", "to": "Spatial understanding is a fundamental problem with widereaching real-world applications. The representation of spatial knowledge is often modeled with spatial templates, i.e., regions of acceptability of two objects under an explicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with prior work that restricts spatial templates to explicit spatial prepositions (e.g., \"glass on table\"), here we extend this concept to implicit spatial language, i.e., those relationships (generally actions) for which the spatial arrangement of the objects is only implicitly implied (e.g., \"man riding horse\"). In contrast with explicit relationships, predicting spatial arrangements from implicit spatial language requires significant common sense spatial understanding. Here, we introduce the task of predicting spatial templates for two objects under a relationship, which can be seen as a spatial question-answering task with a (2D) continuous output (\"where is the man w.r.t. a horse when the man is walking the horse?\"). We present two simple neural-based models that leverage annotated images and structured text to learn this task. The good performance of these models reveals that spatial locations are to a large extent predictable from implicit spatial language. Crucially, the models attain similar performance in a challenging generalized setting, where the object-relation-object combinations (e.g.,\"man walking dog\") have never been seen before. Next, we go one step further by presenting the models with unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging word embeddings enables the models to output accurate spatial predictions, proving that the models acquire solid common sense spatial knowledge allowing for such generalization.", "weight": 0.87}, {"from": "Generalized evaluations", "to": "To provide machines with common sense is one of the major long term goals of artificial intelligence. Common sense knowledge regards knowledge that humans have acquired through a lifetime of experiences. It is crucial in language understanding because a lot of content needed for correct understanding is not expressed explicitly but resides in the mind of communicator and audience. In addition, humans rely on their common sense knowledge when performing a variety of tasks including interpreting images, navigation and reasoning, to name a few. Representing and understanding spatial knowledge are in fact imperative for any agent (human, animal or robot) that navigates in a physical world. In this paper, we are interested in acquiring spatial commonsense knowledge from language paired with visual data.", "weight": 0.87}, {"from": "Generalized evaluations", "to": "Computational and cognitive models often handle spatial representations as spatial templates or regions of acceptability for two objects under an explicit (a.k.a. deictic) spatial preposition such as \"on\", \"below\" or \"left\" ", "weight": 0.87}, {"from": "Generalized evaluations", "to": "Predicting spatial templates for implicit relationships is notably more challenging than for explicit relationships. Firstly, whereas there are only a few tens of explicit spatial prepositions, there exist thousands of actions, entailing thus a drastic increase in the sparsity of (object 1 , relationship, object 2 ) combinations. Secondly, the complexity of the task radically increases in implicit language. More precisely, while explicit spatial prepositions 1 are highly deterministic about the spatial arrangements (e.g., (object 1 , below, object 2 ) unequivocally implies that object 1 is relatively lower than object 2 ), actions generally are not. E.g., the relative spatial configuration of \"man\" and the object is clearly distinct in (man, pulling, kite) than in (man, pulling, luggage) yet the action is the same. Contrarily, other relationships such as \"jumping\" are highly informative about the spatial template, i.e., in (object 1 , jumping, object 2 ), object 2 is in a lower position than object 1 . Hence, unlike explicit relationships, predicting spatial layouts from implicit spatial language requires spatial common sense knowledge about the objects, actions and their interaction, which suggests the need of learning to compose the triplet (Subject, Relationship, Object) as a whole instead of learning a template for each Relationship.", "weight": 0.87}, {"from": "Generalized evaluations", "to": "To systematically study these questions, we propose the task of predicting the relative spatial locations of two objects given a structured text input (Subject, Relationship, Object). We introduce two simple neural-based models trained from annotated images that successfully address the two challenges of implicit spatial language discussed above. Our quantitative evaluation reveals that spatial templates can be reliably predicted from implicit spatial language-as accurately as from explicit spatial language. We also show that our models generalize well to templates of unseen combinations, e.g., predicting (man, riding, elephant) without having been exposed to such scene before, tackling thus the challenge of sparsity. Furthermore, by leveraging word embeddings, the models can correctly generalize to spatial templates with unseen words, e.g., predicting (man, riding, elephant) without having ever seen an \"elephant\" before. Since word embeddings capture attributes of objects ", "weight": 0.87}, {"from": "Generalized evaluations", "to": "The rest of the paper is organized as follows. In Sect. 2 we review related research. In Sect. 3 we first introduce the task of predicting spatial templates and then present two simple neural models. Then, in Sect. 4, we describe our experimental setup. In Sect. 5 we present and discuss our results. Finally, in Sect. 6 we summarize the contributions of this article.", "weight": 0.87}, {"from": "Generalized evaluations", "to": "Spatial processing has drawn significant attention from the cognitive ", "weight": 0.87}, {"from": "Generalized evaluations", "to": "Spatial templates. Earlier approaches have predominantly considered rule-based spatial representations ", "weight": 0.87}, {"from": "Generalized evaluations", "to": "Additionally, while they build a spatial template for each (explicit) Relationship, we build a template for each (Subject, Relationship, Object) combination, allowing the template to be determined by the interaction/composition of the Subject, Relationship and Object instead of the Relationship alone. Additionally, the model from ", "weight": 0.87}, {"from": "Generalized evaluations", "to": "Leveraging spatial knowledge in tasks. It has been shown that knowledge of the spatial structure in images improves the task of image captioning ", "weight": 0.87}, {"from": "Generalized evaluations", "to": "Common sense spatial knowledge. ", "weight": 0.87}, {"from": "Generalized evaluations", "to": "Image generation. Although models that generate images from text exist (e.g., DRAW model ", "weight": 0.87}, {"from": "Generalized evaluations", "to": "As discussed, spatial knowledge can improve a wide range of tasks ", "weight": 0.87}, {"from": "Generalized evaluations", "to": "Obj.", "weight": 0.87}, {"from": "Generalized evaluations", "to": "Subj.", "weight": 0.87}, {"from": "Generalized evaluations", "to": "x y Elliott and Keller 2013). This suggests that the predictions of our models can be used as spatial common sense input for methods that rely on good spatial priors. Additionally, existing methods require the spatial information to be present in the data and lack the capacity to extrapolate/generalize. Thus, in this paper we focus, first, on showing that generalizing spatial arrangements to unseen objects and object-relation-object combinations is possible and, second, on ensuring that the predicted templates are accurate by performing several quantitative and qualitative evaluations.", "weight": 0.87}, {"from": "Generalized evaluations", "to": "3 Proposed task and model", "weight": 0.87}, {"from": "Generalized evaluations", "to": "To learn spatial templates, we propose the task of predicting the 2D relative spatial arrangement of two objects under a relationship given a structured text input of the form (Subject, Relationship, Object)-henceforth abbreviated as (S, R, O). Let us denote the 2D coordinates of the center (\"c\") of the Object\u0027s box as", "weight": 0.87}, {"from": "Generalized evaluations", "to": "y \u2208 R are the horizontal and vertical components respectively. Let", "weight": 0.87}, {"from": "Generalized evaluations", "to": "x , O b y ] \u2208 R 2 be half of the width (O b x ) and half of the height (O b y ) of the Object\u0027s box (\"b\"). We employ a similar notation for the Subject (S c , S b ), and model predictions are denoted with a hat O c , O b . The task consists in predicting the Object\u0027s location and size", "weight": 0.87}, {"from": "Generalized evaluations", "to": "given the structured text input (S, R, O) and the location S c and size S b of the Subject ", "weight": 0.87}, {"from": "Generalized evaluations", "to": "2 Crucially, we notice that knowing the Subject\u0027s coordinates is not a requirement for generating templates (but only for evaluating against the ground truth) since inputting arbitrary coordinates (e.g., S c =[0.5, 0.5]) still enables visualizing relative object locations. Additionally, we input the Subject\u0027s size in order to provide a reference size to the model. However, we find that without this input, the model still learns to predict an \"average size\" for each Object.", "weight": 0.87}, {"from": "Generalized evaluations", "to": "Hence, the goal is to answer common sense spatial questions such as \"if a man is feeding a horse, where is the man relative to the horse?\" or \"where would a child wear her shoes?\"", "weight": 0.87}, {"from": "Generalized evaluations", "to": "To build a mapping from the input to the output in our task (Sect. 3.1) we propose two simple neural models ( ", "weight": 0.87}, {"from": "Generalized evaluations", "to": "are one-hot encodings of S, R, O (a.k.a. one-of-k encoding, i.e., a sparse vector with 0 everywhere except for a 1 at the position of the k-th word) and", "weight": 0.87}, {"from": "Generalized evaluations", "to": "This layer represents objects and relationships as continuous features, enabling thus to introduce external knowledge of unseen objects as features. The embeddings are then concatenated with the Subject center S c and size", "weight": 0.87}, {"from": "Generalized evaluations", "to": "which is inputted to a stack of hidden layers that compose S, R and O into a joint hidden representation z h :", "weight": 0.87}, {"from": "Generalized evaluations", "to": "where f (\u2022) is an element-wise non-linear function and W h and b h are the weight matrix and bias respectively. 3 (ii) Output and loss. We consider two different possibilities for the output\u0177 that follows immediately after the last layer: ", "weight": 0.87}, {"from": "Generalized evaluations", "to": "These models are conceptually different and have different capabilities. While the REG model outputs \"crisp\" pointwise predictions, PIX can model more diffuse spatial templates where the location of the object has more variability, e.g., in (man, flying, kite) the \"kite\" can easily move around. Notice that in contrast with convolutional neural networks (CNNs) our approach does not make use of the image pixels ( ", "weight": 0.87}, {"from": "Generalized evaluations", "to": "We employ a 10-fold cross-validation (CV) setting. Data are randomly split into 10 disjoint parts and 10% is employed for testing and 90% for training, repeating this for each of the 10 folds. Reported results are averages over the 10 folds.", "weight": 0.87}, {"from": "Generalized evaluations", "to": "We use the Visual Genome dataset ", "weight": 0.87}, {"from": "Generalized evaluations", "to": "dog, catches, frisbee boy, feeds, giraffe man, throws, frisbee cat, wears glasses ", "weight": 0.87}, {"from": "Generalized evaluations", "to": "We consider the following subsets of the Visual Genome data to evaluate performance. (i) Raw data: Simply the unfiltered instances from the Visual Genome data (Sect. 4.1). This set contains a substantial proportion of meaningless (e.g., (nose, almost, touching)) and irrelevant (e.g., (sign, says, gate 2)) instances. (ii) Generalized Triplets: We pick at random 100 combinations (S, R, O) among the 1,000 most frequent implicit combinations in Visual Genome. This yields \u223c25K instances such as (person, holding, racket), (man, flying, kite), etc. 4 (iii) Generalized Words: We randomly choose 25 objects (e.g., \"woman\", \"apple\", etc.) 5 among the 100 most frequent objects in Visual Genome and take all the instances (\u223c130K) that contain any of these words. For example, since \"apple\" is in our list, e.g., (cat, sniffing, apple) is kept. ", "weight": 0.87}, {"from": "Generalized evaluations", "to": "When enforcing generalization conditions in our experiments, all combinations from sets (ii) and (iii) are removed from the training data to prevent the model from seeing them. 7 Even without imposing generalization conditions (or when testing with Raw data), reported results are always on unseen instances-yet the combinations (S, R, O) may have been seen during training (e.g., in different images). All sets above contain exclusively implicit spatial language, although an analogous version of the Raw set where R are explicit spatial prepositions is also considered in our experiments.", "weight": 0.87}, {"from": "Generalized evaluations", "to": "The coordinates of bounding boxes in the images are normalized by the width and height of the image. Thus, S c , O c \u2208 [0, 1] 2 . Additionally, we notice that the distinction between left and right is arbitrary regarding the semantics of the image ", "weight": 0.87}, {"from": "Generalized evaluations", "to": "The REG model directly outputs Object coordinates, while PIX outputs 2D heatmaps. We however enable evaluating the PIX model with regression/classification metrics by taking the point of maximum activation (or their average, if there are many) as the Object center O c . The predicted Object size O b is not estimated. We use the following performance metrics.", "weight": 0.87}, {"from": "Generalized evaluations", "to": "(A) Intersection over Union ", "weight": 0.87}, {"from": "Generalized evaluations", "to": "where B O and B O are the predicted and ground truth Object bounding boxes, respectively. If the IoU is larger than 50%, the prediction is counted as correct. It must be noted that our setting is not comparable to object detection (nor our results) since we do not employ the image as input (but text) and thus we cannot leverage the pixels to predict the Object\u0027s location.", "weight": 0.87}, {"from": "Generalized evaluations", "to": "(B) Regression. We consider standard regression metrics. ", "weight": 0.87}, {"from": "Generalized evaluations", "to": "(D) Pixel (macro) accuracy. The IoU on pixels is equivalent to binary pixel accuracy. However, this is not a good measure here, where class 1 (Object box) comprises, on average, only 5% of the pixels. Thus, a constant prediction of zeros everywhere would obtain 95% accuracy. Hence, we consider macro-averaged pixel accuracy, a.k.a. mean IoU (mIoU) ", "weight": 0.87}, {"from": "Generalized evaluations", "to": "We use 300-dimensional GloVe word embeddings (Pennington, Socher, and Manning 2014) pre-trained on the Common Crawl corpus (consisting of 840B-tokens), which we obtain from the authors\u0027 website. 8", "weight": 0.87}, {"from": "Generalized evaluations", "to": "Our experiments are implemented in Python 2.7 and we use Keras deep learning framework for our models ", "weight": 0.87}, {"from": "Generalized evaluations", "to": "We consider the evaluation sets from Sect. 4.2 and the following variations of PIX and REG models (Sect. 3.2). The subindex EMB denotes a model that employs GloVe embeddings and RND a model with embeddings randomly drawn from a dimension-wise normal distribution of mean (\u03bc) and standard deviation (\u03c3) equal to those of the GloVe embeddings, preserving the original dimensionality (d=300). A third type employs one-hot vectors (1H). We additionally  consider a control method (ctrl) that outputs random normal predictions of \u03bc and \u03c3 equal to the dimension-wise mean and standard deviation of the training targets. We test statistical significance with a Friedman rank test and post hoc Nemenyi tests on the results of the 10 folds. We indicate with an asterisk * in the tables when a method is significantly better than the rest (p \u003c 0.01) within the same model (PIX or REG). ", "weight": 0.87}, {"from": "Generalized evaluations", "to": "To ensure that model predictions are meaningful and interpretable, we further validate the quantitative results above with a qualitative evaluation. Notice that all plots in ", "weight": 0.87}, {"from": "Generalized evaluations", "to": "We study how the weights of the model provide insight into the spatial properties of words. To obtain more interpretable weights, we learn a REG 9 model without hidden layers, resulting in only an embedding layer followed by a linear output layer\u0177 = W out u + b out , where", "weight": 0.87}, {"from": "Generalized evaluations", "to": "By using one-hot encodings w S , w R , w O , the concatenation layer u becomes of size |V S | + |V R | + |V O | + 2 + 2. E.g., if \"wearing\" has one-hot index j in the Relationships\u0027 vocabulary (V R ), its index in the concatenation layer is |V S | + j. The product W out u is a 4-dimensional vector, where its i-th component is the product of the i-th row of W out with the vector u. Thus, the component |V S |+j of the i-th row of W out gives us the influence of the j-th relationship (i.e., \"wearing\") on the i-th dimension of the output\u0177 =   ", "weight": 0.87}, {"from": "Generalized evaluations", "to": "Overall, this paper provides insight into the fundamental differences between implicit and explicit spatial language and extends the concept of spatial templates to implicit spatial language, the understanding of which requires common sense spatial knowledge about objects and actions. We define the task of predicting relative spatial arrangements of two objects under a relationship and present two embedding-based neural models that attain promising performance, proving that spatial templates can be accurately predicted from implicit spatial language. Remarkably, our models generalize well, predicting correctly unseen (generalized) object-relationshipobject combinations. Furthermore, the acquired common sense spatial knowledge-aided with word embeddingsallows the model to correctly predict templates for unseen words. Finally, we show that the weights of the model provide great insight into the spatial connotations of words.", "weight": 0.87}, {"from": "Generalized evaluations", "to": "A first limitation of our approach is the fully supervised setting where the models are trained using images with detected ground truth objects and parsed text-which aims at keeping the design clean in this first study on implicit spatial templates. Notice however that methods to automatically parse images and text exist. In future work, we aim at implementing our approach in a weakly supervised setting. A second limitation is the 2D spatial treatment of the actual 3D world. It is worth noting however, that our models (PIX and REG) and setting trivially generalize to 3D if appropriate data are available.", "weight": 0.87}, {"from": "Your Visual Research Paper", "to": "Qualitative evaluation (spatial templates)", "weight": 0.97}, {"from": "Qualitative evaluation (spatial templates)", "to": "Spatial understanding is a fundamental problem with widereaching real-world applications. The representation of spatial knowledge is often modeled with spatial templates, i.e., regions of acceptability of two objects under an explicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with prior work that restricts spatial templates to explicit spatial prepositions (e.g., \"glass on table\"), here we extend this concept to implicit spatial language, i.e., those relationships (generally actions) for which the spatial arrangement of the objects is only implicitly implied (e.g., \"man riding horse\"). In contrast with explicit relationships, predicting spatial arrangements from implicit spatial language requires significant common sense spatial understanding. Here, we introduce the task of predicting spatial templates for two objects under a relationship, which can be seen as a spatial question-answering task with a (2D) continuous output (\"where is the man w.r.t. a horse when the man is walking the horse?\"). We present two simple neural-based models that leverage annotated images and structured text to learn this task. The good performance of these models reveals that spatial locations are to a large extent predictable from implicit spatial language. Crucially, the models attain similar performance in a challenging generalized setting, where the object-relation-object combinations (e.g.,\"man walking dog\") have never been seen before. Next, we go one step further by presenting the models with unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging word embeddings enables the models to output accurate spatial predictions, proving that the models acquire solid common sense spatial knowledge allowing for such generalization.", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "To provide machines with common sense is one of the major long term goals of artificial intelligence. Common sense knowledge regards knowledge that humans have acquired through a lifetime of experiences. It is crucial in language understanding because a lot of content needed for correct understanding is not expressed explicitly but resides in the mind of communicator and audience. In addition, humans rely on their common sense knowledge when performing a variety of tasks including interpreting images, navigation and reasoning, to name a few. Representing and understanding spatial knowledge are in fact imperative for any agent (human, animal or robot) that navigates in a physical world. In this paper, we are interested in acquiring spatial commonsense knowledge from language paired with visual data.", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "Computational and cognitive models often handle spatial representations as spatial templates or regions of acceptability for two objects under an explicit (a.k.a. deictic) spatial preposition such as \"on\", \"below\" or \"left\" ", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "Predicting spatial templates for implicit relationships is notably more challenging than for explicit relationships. Firstly, whereas there are only a few tens of explicit spatial prepositions, there exist thousands of actions, entailing thus a drastic increase in the sparsity of (object 1 , relationship, object 2 ) combinations. Secondly, the complexity of the task radically increases in implicit language. More precisely, while explicit spatial prepositions 1 are highly deterministic about the spatial arrangements (e.g., (object 1 , below, object 2 ) unequivocally implies that object 1 is relatively lower than object 2 ), actions generally are not. E.g., the relative spatial configuration of \"man\" and the object is clearly distinct in (man, pulling, kite) than in (man, pulling, luggage) yet the action is the same. Contrarily, other relationships such as \"jumping\" are highly informative about the spatial template, i.e., in (object 1 , jumping, object 2 ), object 2 is in a lower position than object 1 . Hence, unlike explicit relationships, predicting spatial layouts from implicit spatial language requires spatial common sense knowledge about the objects, actions and their interaction, which suggests the need of learning to compose the triplet (Subject, Relationship, Object) as a whole instead of learning a template for each Relationship.", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "To systematically study these questions, we propose the task of predicting the relative spatial locations of two objects given a structured text input (Subject, Relationship, Object). We introduce two simple neural-based models trained from annotated images that successfully address the two challenges of implicit spatial language discussed above. Our quantitative evaluation reveals that spatial templates can be reliably predicted from implicit spatial language-as accurately as from explicit spatial language. We also show that our models generalize well to templates of unseen combinations, e.g., predicting (man, riding, elephant) without having been exposed to such scene before, tackling thus the challenge of sparsity. Furthermore, by leveraging word embeddings, the models can correctly generalize to spatial templates with unseen words, e.g., predicting (man, riding, elephant) without having ever seen an \"elephant\" before. Since word embeddings capture attributes of objects ", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "The rest of the paper is organized as follows. In Sect. 2 we review related research. In Sect. 3 we first introduce the task of predicting spatial templates and then present two simple neural models. Then, in Sect. 4, we describe our experimental setup. In Sect. 5 we present and discuss our results. Finally, in Sect. 6 we summarize the contributions of this article.", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "Spatial processing has drawn significant attention from the cognitive ", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "Spatial templates. Earlier approaches have predominantly considered rule-based spatial representations ", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "Additionally, while they build a spatial template for each (explicit) Relationship, we build a template for each (Subject, Relationship, Object) combination, allowing the template to be determined by the interaction/composition of the Subject, Relationship and Object instead of the Relationship alone. Additionally, the model from ", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "Leveraging spatial knowledge in tasks. It has been shown that knowledge of the spatial structure in images improves the task of image captioning ", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "Common sense spatial knowledge. ", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "Image generation. Although models that generate images from text exist (e.g., DRAW model ", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "As discussed, spatial knowledge can improve a wide range of tasks ", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "Obj.", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "Subj.", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "x y Elliott and Keller 2013). This suggests that the predictions of our models can be used as spatial common sense input for methods that rely on good spatial priors. Additionally, existing methods require the spatial information to be present in the data and lack the capacity to extrapolate/generalize. Thus, in this paper we focus, first, on showing that generalizing spatial arrangements to unseen objects and object-relation-object combinations is possible and, second, on ensuring that the predicted templates are accurate by performing several quantitative and qualitative evaluations.", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "3 Proposed task and model", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "To learn spatial templates, we propose the task of predicting the 2D relative spatial arrangement of two objects under a relationship given a structured text input of the form (Subject, Relationship, Object)-henceforth abbreviated as (S, R, O). Let us denote the 2D coordinates of the center (\"c\") of the Object\u0027s box as", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "y \u2208 R are the horizontal and vertical components respectively. Let", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "x , O b y ] \u2208 R 2 be half of the width (O b x ) and half of the height (O b y ) of the Object\u0027s box (\"b\"). We employ a similar notation for the Subject (S c , S b ), and model predictions are denoted with a hat O c , O b . The task consists in predicting the Object\u0027s location and size", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "given the structured text input (S, R, O) and the location S c and size S b of the Subject ", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "2 Crucially, we notice that knowing the Subject\u0027s coordinates is not a requirement for generating templates (but only for evaluating against the ground truth) since inputting arbitrary coordinates (e.g., S c =[0.5, 0.5]) still enables visualizing relative object locations. Additionally, we input the Subject\u0027s size in order to provide a reference size to the model. However, we find that without this input, the model still learns to predict an \"average size\" for each Object.", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "Hence, the goal is to answer common sense spatial questions such as \"if a man is feeding a horse, where is the man relative to the horse?\" or \"where would a child wear her shoes?\"", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "To build a mapping from the input to the output in our task (Sect. 3.1) we propose two simple neural models ( ", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "are one-hot encodings of S, R, O (a.k.a. one-of-k encoding, i.e., a sparse vector with 0 everywhere except for a 1 at the position of the k-th word) and", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "This layer represents objects and relationships as continuous features, enabling thus to introduce external knowledge of unseen objects as features. The embeddings are then concatenated with the Subject center S c and size", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "which is inputted to a stack of hidden layers that compose S, R and O into a joint hidden representation z h :", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "where f (\u2022) is an element-wise non-linear function and W h and b h are the weight matrix and bias respectively. 3 (ii) Output and loss. We consider two different possibilities for the output\u0177 that follows immediately after the last layer: ", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "These models are conceptually different and have different capabilities. While the REG model outputs \"crisp\" pointwise predictions, PIX can model more diffuse spatial templates where the location of the object has more variability, e.g., in (man, flying, kite) the \"kite\" can easily move around. Notice that in contrast with convolutional neural networks (CNNs) our approach does not make use of the image pixels ( ", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "We employ a 10-fold cross-validation (CV) setting. Data are randomly split into 10 disjoint parts and 10% is employed for testing and 90% for training, repeating this for each of the 10 folds. Reported results are averages over the 10 folds.", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "We use the Visual Genome dataset ", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "dog, catches, frisbee boy, feeds, giraffe man, throws, frisbee cat, wears glasses ", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "We consider the following subsets of the Visual Genome data to evaluate performance. (i) Raw data: Simply the unfiltered instances from the Visual Genome data (Sect. 4.1). This set contains a substantial proportion of meaningless (e.g., (nose, almost, touching)) and irrelevant (e.g., (sign, says, gate 2)) instances. (ii) Generalized Triplets: We pick at random 100 combinations (S, R, O) among the 1,000 most frequent implicit combinations in Visual Genome. This yields \u223c25K instances such as (person, holding, racket), (man, flying, kite), etc. 4 (iii) Generalized Words: We randomly choose 25 objects (e.g., \"woman\", \"apple\", etc.) 5 among the 100 most frequent objects in Visual Genome and take all the instances (\u223c130K) that contain any of these words. For example, since \"apple\" is in our list, e.g., (cat, sniffing, apple) is kept. ", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "When enforcing generalization conditions in our experiments, all combinations from sets (ii) and (iii) are removed from the training data to prevent the model from seeing them. 7 Even without imposing generalization conditions (or when testing with Raw data), reported results are always on unseen instances-yet the combinations (S, R, O) may have been seen during training (e.g., in different images). All sets above contain exclusively implicit spatial language, although an analogous version of the Raw set where R are explicit spatial prepositions is also considered in our experiments.", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "The coordinates of bounding boxes in the images are normalized by the width and height of the image. Thus, S c , O c \u2208 [0, 1] 2 . Additionally, we notice that the distinction between left and right is arbitrary regarding the semantics of the image ", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "The REG model directly outputs Object coordinates, while PIX outputs 2D heatmaps. We however enable evaluating the PIX model with regression/classification metrics by taking the point of maximum activation (or their average, if there are many) as the Object center O c . The predicted Object size O b is not estimated. We use the following performance metrics.", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "(A) Intersection over Union ", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "where B O and B O are the predicted and ground truth Object bounding boxes, respectively. If the IoU is larger than 50%, the prediction is counted as correct. It must be noted that our setting is not comparable to object detection (nor our results) since we do not employ the image as input (but text) and thus we cannot leverage the pixels to predict the Object\u0027s location.", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "(B) Regression. We consider standard regression metrics. ", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "(D) Pixel (macro) accuracy. The IoU on pixels is equivalent to binary pixel accuracy. However, this is not a good measure here, where class 1 (Object box) comprises, on average, only 5% of the pixels. Thus, a constant prediction of zeros everywhere would obtain 95% accuracy. Hence, we consider macro-averaged pixel accuracy, a.k.a. mean IoU (mIoU) ", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "We use 300-dimensional GloVe word embeddings (Pennington, Socher, and Manning 2014) pre-trained on the Common Crawl corpus (consisting of 840B-tokens), which we obtain from the authors\u0027 website. 8", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "Our experiments are implemented in Python 2.7 and we use Keras deep learning framework for our models ", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "We consider the evaluation sets from Sect. 4.2 and the following variations of PIX and REG models (Sect. 3.2). The subindex EMB denotes a model that employs GloVe embeddings and RND a model with embeddings randomly drawn from a dimension-wise normal distribution of mean (\u03bc) and standard deviation (\u03c3) equal to those of the GloVe embeddings, preserving the original dimensionality (d=300). A third type employs one-hot vectors (1H). We additionally  consider a control method (ctrl) that outputs random normal predictions of \u03bc and \u03c3 equal to the dimension-wise mean and standard deviation of the training targets. We test statistical significance with a Friedman rank test and post hoc Nemenyi tests on the results of the 10 folds. We indicate with an asterisk * in the tables when a method is significantly better than the rest (p \u003c 0.01) within the same model (PIX or REG). ", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "To ensure that model predictions are meaningful and interpretable, we further validate the quantitative results above with a qualitative evaluation. Notice that all plots in ", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "We study how the weights of the model provide insight into the spatial properties of words. To obtain more interpretable weights, we learn a REG 9 model without hidden layers, resulting in only an embedding layer followed by a linear output layer\u0177 = W out u + b out , where", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "By using one-hot encodings w S , w R , w O , the concatenation layer u becomes of size |V S | + |V R | + |V O | + 2 + 2. E.g., if \"wearing\" has one-hot index j in the Relationships\u0027 vocabulary (V R ), its index in the concatenation layer is |V S | + j. The product W out u is a 4-dimensional vector, where its i-th component is the product of the i-th row of W out with the vector u. Thus, the component |V S |+j of the i-th row of W out gives us the influence of the j-th relationship (i.e., \"wearing\") on the i-th dimension of the output\u0177 =   ", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "Overall, this paper provides insight into the fundamental differences between implicit and explicit spatial language and extends the concept of spatial templates to implicit spatial language, the understanding of which requires common sense spatial knowledge about objects and actions. We define the task of predicting relative spatial arrangements of two objects under a relationship and present two embedding-based neural models that attain promising performance, proving that spatial templates can be accurately predicted from implicit spatial language. Remarkably, our models generalize well, predicting correctly unseen (generalized) object-relationshipobject combinations. Furthermore, the acquired common sense spatial knowledge-aided with word embeddingsallows the model to correctly predict templates for unseen words. Finally, we show that the weights of the model provide great insight into the spatial connotations of words.", "weight": 0.87}, {"from": "Qualitative evaluation (spatial templates)", "to": "A first limitation of our approach is the fully supervised setting where the models are trained using images with detected ground truth objects and parsed text-which aims at keeping the design clean in this first study on implicit spatial templates. Notice however that methods to automatically parse images and text exist. In future work, we aim at implementing our approach in a weakly supervised setting. A second limitation is the 2D spatial treatment of the actual 3D world. It is worth noting however, that our models (PIX and REG) and setting trivially generalize to 3D if appropriate data are available.", "weight": 0.87}, {"from": "Your Visual Research Paper", "to": "Interpretation of model weights", "weight": 0.97}, {"from": "Interpretation of model weights", "to": "Spatial understanding is a fundamental problem with widereaching real-world applications. The representation of spatial knowledge is often modeled with spatial templates, i.e., regions of acceptability of two objects under an explicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with prior work that restricts spatial templates to explicit spatial prepositions (e.g., \"glass on table\"), here we extend this concept to implicit spatial language, i.e., those relationships (generally actions) for which the spatial arrangement of the objects is only implicitly implied (e.g., \"man riding horse\"). In contrast with explicit relationships, predicting spatial arrangements from implicit spatial language requires significant common sense spatial understanding. Here, we introduce the task of predicting spatial templates for two objects under a relationship, which can be seen as a spatial question-answering task with a (2D) continuous output (\"where is the man w.r.t. a horse when the man is walking the horse?\"). We present two simple neural-based models that leverage annotated images and structured text to learn this task. The good performance of these models reveals that spatial locations are to a large extent predictable from implicit spatial language. Crucially, the models attain similar performance in a challenging generalized setting, where the object-relation-object combinations (e.g.,\"man walking dog\") have never been seen before. Next, we go one step further by presenting the models with unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging word embeddings enables the models to output accurate spatial predictions, proving that the models acquire solid common sense spatial knowledge allowing for such generalization.", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "To provide machines with common sense is one of the major long term goals of artificial intelligence. Common sense knowledge regards knowledge that humans have acquired through a lifetime of experiences. It is crucial in language understanding because a lot of content needed for correct understanding is not expressed explicitly but resides in the mind of communicator and audience. In addition, humans rely on their common sense knowledge when performing a variety of tasks including interpreting images, navigation and reasoning, to name a few. Representing and understanding spatial knowledge are in fact imperative for any agent (human, animal or robot) that navigates in a physical world. In this paper, we are interested in acquiring spatial commonsense knowledge from language paired with visual data.", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "Computational and cognitive models often handle spatial representations as spatial templates or regions of acceptability for two objects under an explicit (a.k.a. deictic) spatial preposition such as \"on\", \"below\" or \"left\" ", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "Predicting spatial templates for implicit relationships is notably more challenging than for explicit relationships. Firstly, whereas there are only a few tens of explicit spatial prepositions, there exist thousands of actions, entailing thus a drastic increase in the sparsity of (object 1 , relationship, object 2 ) combinations. Secondly, the complexity of the task radically increases in implicit language. More precisely, while explicit spatial prepositions 1 are highly deterministic about the spatial arrangements (e.g., (object 1 , below, object 2 ) unequivocally implies that object 1 is relatively lower than object 2 ), actions generally are not. E.g., the relative spatial configuration of \"man\" and the object is clearly distinct in (man, pulling, kite) than in (man, pulling, luggage) yet the action is the same. Contrarily, other relationships such as \"jumping\" are highly informative about the spatial template, i.e., in (object 1 , jumping, object 2 ), object 2 is in a lower position than object 1 . Hence, unlike explicit relationships, predicting spatial layouts from implicit spatial language requires spatial common sense knowledge about the objects, actions and their interaction, which suggests the need of learning to compose the triplet (Subject, Relationship, Object) as a whole instead of learning a template for each Relationship.", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "To systematically study these questions, we propose the task of predicting the relative spatial locations of two objects given a structured text input (Subject, Relationship, Object). We introduce two simple neural-based models trained from annotated images that successfully address the two challenges of implicit spatial language discussed above. Our quantitative evaluation reveals that spatial templates can be reliably predicted from implicit spatial language-as accurately as from explicit spatial language. We also show that our models generalize well to templates of unseen combinations, e.g., predicting (man, riding, elephant) without having been exposed to such scene before, tackling thus the challenge of sparsity. Furthermore, by leveraging word embeddings, the models can correctly generalize to spatial templates with unseen words, e.g., predicting (man, riding, elephant) without having ever seen an \"elephant\" before. Since word embeddings capture attributes of objects ", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "The rest of the paper is organized as follows. In Sect. 2 we review related research. In Sect. 3 we first introduce the task of predicting spatial templates and then present two simple neural models. Then, in Sect. 4, we describe our experimental setup. In Sect. 5 we present and discuss our results. Finally, in Sect. 6 we summarize the contributions of this article.", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "Spatial processing has drawn significant attention from the cognitive ", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "Spatial templates. Earlier approaches have predominantly considered rule-based spatial representations ", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "Additionally, while they build a spatial template for each (explicit) Relationship, we build a template for each (Subject, Relationship, Object) combination, allowing the template to be determined by the interaction/composition of the Subject, Relationship and Object instead of the Relationship alone. Additionally, the model from ", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "Leveraging spatial knowledge in tasks. It has been shown that knowledge of the spatial structure in images improves the task of image captioning ", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "Common sense spatial knowledge. ", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "Image generation. Although models that generate images from text exist (e.g., DRAW model ", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "As discussed, spatial knowledge can improve a wide range of tasks ", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "Obj.", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "Subj.", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "x y Elliott and Keller 2013). This suggests that the predictions of our models can be used as spatial common sense input for methods that rely on good spatial priors. Additionally, existing methods require the spatial information to be present in the data and lack the capacity to extrapolate/generalize. Thus, in this paper we focus, first, on showing that generalizing spatial arrangements to unseen objects and object-relation-object combinations is possible and, second, on ensuring that the predicted templates are accurate by performing several quantitative and qualitative evaluations.", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "3 Proposed task and model", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "To learn spatial templates, we propose the task of predicting the 2D relative spatial arrangement of two objects under a relationship given a structured text input of the form (Subject, Relationship, Object)-henceforth abbreviated as (S, R, O). Let us denote the 2D coordinates of the center (\"c\") of the Object\u0027s box as", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "y \u2208 R are the horizontal and vertical components respectively. Let", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "x , O b y ] \u2208 R 2 be half of the width (O b x ) and half of the height (O b y ) of the Object\u0027s box (\"b\"). We employ a similar notation for the Subject (S c , S b ), and model predictions are denoted with a hat O c , O b . The task consists in predicting the Object\u0027s location and size", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "given the structured text input (S, R, O) and the location S c and size S b of the Subject ", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "2 Crucially, we notice that knowing the Subject\u0027s coordinates is not a requirement for generating templates (but only for evaluating against the ground truth) since inputting arbitrary coordinates (e.g., S c =[0.5, 0.5]) still enables visualizing relative object locations. Additionally, we input the Subject\u0027s size in order to provide a reference size to the model. However, we find that without this input, the model still learns to predict an \"average size\" for each Object.", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "Hence, the goal is to answer common sense spatial questions such as \"if a man is feeding a horse, where is the man relative to the horse?\" or \"where would a child wear her shoes?\"", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "To build a mapping from the input to the output in our task (Sect. 3.1) we propose two simple neural models ( ", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "are one-hot encodings of S, R, O (a.k.a. one-of-k encoding, i.e., a sparse vector with 0 everywhere except for a 1 at the position of the k-th word) and", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "This layer represents objects and relationships as continuous features, enabling thus to introduce external knowledge of unseen objects as features. The embeddings are then concatenated with the Subject center S c and size", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "which is inputted to a stack of hidden layers that compose S, R and O into a joint hidden representation z h :", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "where f (\u2022) is an element-wise non-linear function and W h and b h are the weight matrix and bias respectively. 3 (ii) Output and loss. We consider two different possibilities for the output\u0177 that follows immediately after the last layer: ", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "These models are conceptually different and have different capabilities. While the REG model outputs \"crisp\" pointwise predictions, PIX can model more diffuse spatial templates where the location of the object has more variability, e.g., in (man, flying, kite) the \"kite\" can easily move around. Notice that in contrast with convolutional neural networks (CNNs) our approach does not make use of the image pixels ( ", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "We employ a 10-fold cross-validation (CV) setting. Data are randomly split into 10 disjoint parts and 10% is employed for testing and 90% for training, repeating this for each of the 10 folds. Reported results are averages over the 10 folds.", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "We use the Visual Genome dataset ", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "dog, catches, frisbee boy, feeds, giraffe man, throws, frisbee cat, wears glasses ", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "We consider the following subsets of the Visual Genome data to evaluate performance. (i) Raw data: Simply the unfiltered instances from the Visual Genome data (Sect. 4.1). This set contains a substantial proportion of meaningless (e.g., (nose, almost, touching)) and irrelevant (e.g., (sign, says, gate 2)) instances. (ii) Generalized Triplets: We pick at random 100 combinations (S, R, O) among the 1,000 most frequent implicit combinations in Visual Genome. This yields \u223c25K instances such as (person, holding, racket), (man, flying, kite), etc. 4 (iii) Generalized Words: We randomly choose 25 objects (e.g., \"woman\", \"apple\", etc.) 5 among the 100 most frequent objects in Visual Genome and take all the instances (\u223c130K) that contain any of these words. For example, since \"apple\" is in our list, e.g., (cat, sniffing, apple) is kept. ", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "When enforcing generalization conditions in our experiments, all combinations from sets (ii) and (iii) are removed from the training data to prevent the model from seeing them. 7 Even without imposing generalization conditions (or when testing with Raw data), reported results are always on unseen instances-yet the combinations (S, R, O) may have been seen during training (e.g., in different images). All sets above contain exclusively implicit spatial language, although an analogous version of the Raw set where R are explicit spatial prepositions is also considered in our experiments.", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "The coordinates of bounding boxes in the images are normalized by the width and height of the image. Thus, S c , O c \u2208 [0, 1] 2 . Additionally, we notice that the distinction between left and right is arbitrary regarding the semantics of the image ", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "The REG model directly outputs Object coordinates, while PIX outputs 2D heatmaps. We however enable evaluating the PIX model with regression/classification metrics by taking the point of maximum activation (or their average, if there are many) as the Object center O c . The predicted Object size O b is not estimated. We use the following performance metrics.", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "(A) Intersection over Union ", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "where B O and B O are the predicted and ground truth Object bounding boxes, respectively. If the IoU is larger than 50%, the prediction is counted as correct. It must be noted that our setting is not comparable to object detection (nor our results) since we do not employ the image as input (but text) and thus we cannot leverage the pixels to predict the Object\u0027s location.", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "(B) Regression. We consider standard regression metrics. ", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "(D) Pixel (macro) accuracy. The IoU on pixels is equivalent to binary pixel accuracy. However, this is not a good measure here, where class 1 (Object box) comprises, on average, only 5% of the pixels. Thus, a constant prediction of zeros everywhere would obtain 95% accuracy. Hence, we consider macro-averaged pixel accuracy, a.k.a. mean IoU (mIoU) ", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "We use 300-dimensional GloVe word embeddings (Pennington, Socher, and Manning 2014) pre-trained on the Common Crawl corpus (consisting of 840B-tokens), which we obtain from the authors\u0027 website. 8", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "Our experiments are implemented in Python 2.7 and we use Keras deep learning framework for our models ", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "We consider the evaluation sets from Sect. 4.2 and the following variations of PIX and REG models (Sect. 3.2). The subindex EMB denotes a model that employs GloVe embeddings and RND a model with embeddings randomly drawn from a dimension-wise normal distribution of mean (\u03bc) and standard deviation (\u03c3) equal to those of the GloVe embeddings, preserving the original dimensionality (d=300). A third type employs one-hot vectors (1H). We additionally  consider a control method (ctrl) that outputs random normal predictions of \u03bc and \u03c3 equal to the dimension-wise mean and standard deviation of the training targets. We test statistical significance with a Friedman rank test and post hoc Nemenyi tests on the results of the 10 folds. We indicate with an asterisk * in the tables when a method is significantly better than the rest (p \u003c 0.01) within the same model (PIX or REG). ", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "To ensure that model predictions are meaningful and interpretable, we further validate the quantitative results above with a qualitative evaluation. Notice that all plots in ", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "We study how the weights of the model provide insight into the spatial properties of words. To obtain more interpretable weights, we learn a REG 9 model without hidden layers, resulting in only an embedding layer followed by a linear output layer\u0177 = W out u + b out , where", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "By using one-hot encodings w S , w R , w O , the concatenation layer u becomes of size |V S | + |V R | + |V O | + 2 + 2. E.g., if \"wearing\" has one-hot index j in the Relationships\u0027 vocabulary (V R ), its index in the concatenation layer is |V S | + j. The product W out u is a 4-dimensional vector, where its i-th component is the product of the i-th row of W out with the vector u. Thus, the component |V S |+j of the i-th row of W out gives us the influence of the j-th relationship (i.e., \"wearing\") on the i-th dimension of the output\u0177 =   ", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "Overall, this paper provides insight into the fundamental differences between implicit and explicit spatial language and extends the concept of spatial templates to implicit spatial language, the understanding of which requires common sense spatial knowledge about objects and actions. We define the task of predicting relative spatial arrangements of two objects under a relationship and present two embedding-based neural models that attain promising performance, proving that spatial templates can be accurately predicted from implicit spatial language. Remarkably, our models generalize well, predicting correctly unseen (generalized) object-relationshipobject combinations. Furthermore, the acquired common sense spatial knowledge-aided with word embeddingsallows the model to correctly predict templates for unseen words. Finally, we show that the weights of the model provide great insight into the spatial connotations of words.", "weight": 0.87}, {"from": "Interpretation of model weights", "to": "A first limitation of our approach is the fully supervised setting where the models are trained using images with detected ground truth objects and parsed text-which aims at keeping the design clean in this first study on implicit spatial templates. Notice however that methods to automatically parse images and text exist. In future work, we aim at implementing our approach in a weakly supervised setting. A second limitation is the 2D spatial treatment of the actual 3D world. It is worth noting however, that our models (PIX and REG) and setting trivially generalize to 3D if appropriate data are available.", "weight": 0.87}, {"from": "Your Visual Research Paper", "to": "Conclusions", "weight": 0.97}, {"from": "Conclusions", "to": "Spatial understanding is a fundamental problem with widereaching real-world applications. The representation of spatial knowledge is often modeled with spatial templates, i.e., regions of acceptability of two objects under an explicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with prior work that restricts spatial templates to explicit spatial prepositions (e.g., \"glass on table\"), here we extend this concept to implicit spatial language, i.e., those relationships (generally actions) for which the spatial arrangement of the objects is only implicitly implied (e.g., \"man riding horse\"). In contrast with explicit relationships, predicting spatial arrangements from implicit spatial language requires significant common sense spatial understanding. Here, we introduce the task of predicting spatial templates for two objects under a relationship, which can be seen as a spatial question-answering task with a (2D) continuous output (\"where is the man w.r.t. a horse when the man is walking the horse?\"). We present two simple neural-based models that leverage annotated images and structured text to learn this task. The good performance of these models reveals that spatial locations are to a large extent predictable from implicit spatial language. Crucially, the models attain similar performance in a challenging generalized setting, where the object-relation-object combinations (e.g.,\"man walking dog\") have never been seen before. Next, we go one step further by presenting the models with unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging word embeddings enables the models to output accurate spatial predictions, proving that the models acquire solid common sense spatial knowledge allowing for such generalization.", "weight": 0.87}, {"from": "Conclusions", "to": "To provide machines with common sense is one of the major long term goals of artificial intelligence. Common sense knowledge regards knowledge that humans have acquired through a lifetime of experiences. It is crucial in language understanding because a lot of content needed for correct understanding is not expressed explicitly but resides in the mind of communicator and audience. In addition, humans rely on their common sense knowledge when performing a variety of tasks including interpreting images, navigation and reasoning, to name a few. Representing and understanding spatial knowledge are in fact imperative for any agent (human, animal or robot) that navigates in a physical world. In this paper, we are interested in acquiring spatial commonsense knowledge from language paired with visual data.", "weight": 0.87}, {"from": "Conclusions", "to": "Computational and cognitive models often handle spatial representations as spatial templates or regions of acceptability for two objects under an explicit (a.k.a. deictic) spatial preposition such as \"on\", \"below\" or \"left\" ", "weight": 0.87}, {"from": "Conclusions", "to": "Predicting spatial templates for implicit relationships is notably more challenging than for explicit relationships. Firstly, whereas there are only a few tens of explicit spatial prepositions, there exist thousands of actions, entailing thus a drastic increase in the sparsity of (object 1 , relationship, object 2 ) combinations. Secondly, the complexity of the task radically increases in implicit language. More precisely, while explicit spatial prepositions 1 are highly deterministic about the spatial arrangements (e.g., (object 1 , below, object 2 ) unequivocally implies that object 1 is relatively lower than object 2 ), actions generally are not. E.g., the relative spatial configuration of \"man\" and the object is clearly distinct in (man, pulling, kite) than in (man, pulling, luggage) yet the action is the same. Contrarily, other relationships such as \"jumping\" are highly informative about the spatial template, i.e., in (object 1 , jumping, object 2 ), object 2 is in a lower position than object 1 . Hence, unlike explicit relationships, predicting spatial layouts from implicit spatial language requires spatial common sense knowledge about the objects, actions and their interaction, which suggests the need of learning to compose the triplet (Subject, Relationship, Object) as a whole instead of learning a template for each Relationship.", "weight": 0.87}, {"from": "Conclusions", "to": "To systematically study these questions, we propose the task of predicting the relative spatial locations of two objects given a structured text input (Subject, Relationship, Object). We introduce two simple neural-based models trained from annotated images that successfully address the two challenges of implicit spatial language discussed above. Our quantitative evaluation reveals that spatial templates can be reliably predicted from implicit spatial language-as accurately as from explicit spatial language. We also show that our models generalize well to templates of unseen combinations, e.g., predicting (man, riding, elephant) without having been exposed to such scene before, tackling thus the challenge of sparsity. Furthermore, by leveraging word embeddings, the models can correctly generalize to spatial templates with unseen words, e.g., predicting (man, riding, elephant) without having ever seen an \"elephant\" before. Since word embeddings capture attributes of objects ", "weight": 0.87}, {"from": "Conclusions", "to": "The rest of the paper is organized as follows. In Sect. 2 we review related research. In Sect. 3 we first introduce the task of predicting spatial templates and then present two simple neural models. Then, in Sect. 4, we describe our experimental setup. In Sect. 5 we present and discuss our results. Finally, in Sect. 6 we summarize the contributions of this article.", "weight": 0.87}, {"from": "Conclusions", "to": "Spatial processing has drawn significant attention from the cognitive ", "weight": 0.87}, {"from": "Conclusions", "to": "Spatial templates. Earlier approaches have predominantly considered rule-based spatial representations ", "weight": 0.87}, {"from": "Conclusions", "to": "Additionally, while they build a spatial template for each (explicit) Relationship, we build a template for each (Subject, Relationship, Object) combination, allowing the template to be determined by the interaction/composition of the Subject, Relationship and Object instead of the Relationship alone. Additionally, the model from ", "weight": 0.87}, {"from": "Conclusions", "to": "Leveraging spatial knowledge in tasks. It has been shown that knowledge of the spatial structure in images improves the task of image captioning ", "weight": 0.87}, {"from": "Conclusions", "to": "Common sense spatial knowledge. ", "weight": 0.87}, {"from": "Conclusions", "to": "Image generation. Although models that generate images from text exist (e.g., DRAW model ", "weight": 0.87}, {"from": "Conclusions", "to": "As discussed, spatial knowledge can improve a wide range of tasks ", "weight": 0.87}, {"from": "Conclusions", "to": "Obj.", "weight": 0.87}, {"from": "Conclusions", "to": "Subj.", "weight": 0.87}, {"from": "Conclusions", "to": "x y Elliott and Keller 2013). This suggests that the predictions of our models can be used as spatial common sense input for methods that rely on good spatial priors. Additionally, existing methods require the spatial information to be present in the data and lack the capacity to extrapolate/generalize. Thus, in this paper we focus, first, on showing that generalizing spatial arrangements to unseen objects and object-relation-object combinations is possible and, second, on ensuring that the predicted templates are accurate by performing several quantitative and qualitative evaluations.", "weight": 0.87}, {"from": "Conclusions", "to": "3 Proposed task and model", "weight": 0.87}, {"from": "Conclusions", "to": "To learn spatial templates, we propose the task of predicting the 2D relative spatial arrangement of two objects under a relationship given a structured text input of the form (Subject, Relationship, Object)-henceforth abbreviated as (S, R, O). Let us denote the 2D coordinates of the center (\"c\") of the Object\u0027s box as", "weight": 0.87}, {"from": "Conclusions", "to": "y \u2208 R are the horizontal and vertical components respectively. Let", "weight": 0.87}, {"from": "Conclusions", "to": "x , O b y ] \u2208 R 2 be half of the width (O b x ) and half of the height (O b y ) of the Object\u0027s box (\"b\"). We employ a similar notation for the Subject (S c , S b ), and model predictions are denoted with a hat O c , O b . The task consists in predicting the Object\u0027s location and size", "weight": 0.87}, {"from": "Conclusions", "to": "given the structured text input (S, R, O) and the location S c and size S b of the Subject ", "weight": 0.87}, {"from": "Conclusions", "to": "2 Crucially, we notice that knowing the Subject\u0027s coordinates is not a requirement for generating templates (but only for evaluating against the ground truth) since inputting arbitrary coordinates (e.g., S c =[0.5, 0.5]) still enables visualizing relative object locations. Additionally, we input the Subject\u0027s size in order to provide a reference size to the model. However, we find that without this input, the model still learns to predict an \"average size\" for each Object.", "weight": 0.87}, {"from": "Conclusions", "to": "Hence, the goal is to answer common sense spatial questions such as \"if a man is feeding a horse, where is the man relative to the horse?\" or \"where would a child wear her shoes?\"", "weight": 0.87}, {"from": "Conclusions", "to": "To build a mapping from the input to the output in our task (Sect. 3.1) we propose two simple neural models ( ", "weight": 0.87}, {"from": "Conclusions", "to": "are one-hot encodings of S, R, O (a.k.a. one-of-k encoding, i.e., a sparse vector with 0 everywhere except for a 1 at the position of the k-th word) and", "weight": 0.87}, {"from": "Conclusions", "to": "This layer represents objects and relationships as continuous features, enabling thus to introduce external knowledge of unseen objects as features. The embeddings are then concatenated with the Subject center S c and size", "weight": 0.87}, {"from": "Conclusions", "to": "which is inputted to a stack of hidden layers that compose S, R and O into a joint hidden representation z h :", "weight": 0.87}, {"from": "Conclusions", "to": "where f (\u2022) is an element-wise non-linear function and W h and b h are the weight matrix and bias respectively. 3 (ii) Output and loss. We consider two different possibilities for the output\u0177 that follows immediately after the last layer: ", "weight": 0.87}, {"from": "Conclusions", "to": "These models are conceptually different and have different capabilities. While the REG model outputs \"crisp\" pointwise predictions, PIX can model more diffuse spatial templates where the location of the object has more variability, e.g., in (man, flying, kite) the \"kite\" can easily move around. Notice that in contrast with convolutional neural networks (CNNs) our approach does not make use of the image pixels ( ", "weight": 0.87}, {"from": "Conclusions", "to": "We employ a 10-fold cross-validation (CV) setting. Data are randomly split into 10 disjoint parts and 10% is employed for testing and 90% for training, repeating this for each of the 10 folds. Reported results are averages over the 10 folds.", "weight": 0.87}, {"from": "Conclusions", "to": "We use the Visual Genome dataset ", "weight": 0.87}, {"from": "Conclusions", "to": "dog, catches, frisbee boy, feeds, giraffe man, throws, frisbee cat, wears glasses ", "weight": 0.87}, {"from": "Conclusions", "to": "We consider the following subsets of the Visual Genome data to evaluate performance. (i) Raw data: Simply the unfiltered instances from the Visual Genome data (Sect. 4.1). This set contains a substantial proportion of meaningless (e.g., (nose, almost, touching)) and irrelevant (e.g., (sign, says, gate 2)) instances. (ii) Generalized Triplets: We pick at random 100 combinations (S, R, O) among the 1,000 most frequent implicit combinations in Visual Genome. This yields \u223c25K instances such as (person, holding, racket), (man, flying, kite), etc. 4 (iii) Generalized Words: We randomly choose 25 objects (e.g., \"woman\", \"apple\", etc.) 5 among the 100 most frequent objects in Visual Genome and take all the instances (\u223c130K) that contain any of these words. For example, since \"apple\" is in our list, e.g., (cat, sniffing, apple) is kept. ", "weight": 0.87}, {"from": "Conclusions", "to": "When enforcing generalization conditions in our experiments, all combinations from sets (ii) and (iii) are removed from the training data to prevent the model from seeing them. 7 Even without imposing generalization conditions (or when testing with Raw data), reported results are always on unseen instances-yet the combinations (S, R, O) may have been seen during training (e.g., in different images). All sets above contain exclusively implicit spatial language, although an analogous version of the Raw set where R are explicit spatial prepositions is also considered in our experiments.", "weight": 0.87}, {"from": "Conclusions", "to": "The coordinates of bounding boxes in the images are normalized by the width and height of the image. Thus, S c , O c \u2208 [0, 1] 2 . Additionally, we notice that the distinction between left and right is arbitrary regarding the semantics of the image ", "weight": 0.87}, {"from": "Conclusions", "to": "The REG model directly outputs Object coordinates, while PIX outputs 2D heatmaps. We however enable evaluating the PIX model with regression/classification metrics by taking the point of maximum activation (or their average, if there are many) as the Object center O c . The predicted Object size O b is not estimated. We use the following performance metrics.", "weight": 0.87}, {"from": "Conclusions", "to": "(A) Intersection over Union ", "weight": 0.87}, {"from": "Conclusions", "to": "where B O and B O are the predicted and ground truth Object bounding boxes, respectively. If the IoU is larger than 50%, the prediction is counted as correct. It must be noted that our setting is not comparable to object detection (nor our results) since we do not employ the image as input (but text) and thus we cannot leverage the pixels to predict the Object\u0027s location.", "weight": 0.87}, {"from": "Conclusions", "to": "(B) Regression. We consider standard regression metrics. ", "weight": 0.87}, {"from": "Conclusions", "to": "(D) Pixel (macro) accuracy. The IoU on pixels is equivalent to binary pixel accuracy. However, this is not a good measure here, where class 1 (Object box) comprises, on average, only 5% of the pixels. Thus, a constant prediction of zeros everywhere would obtain 95% accuracy. Hence, we consider macro-averaged pixel accuracy, a.k.a. mean IoU (mIoU) ", "weight": 0.87}, {"from": "Conclusions", "to": "We use 300-dimensional GloVe word embeddings (Pennington, Socher, and Manning 2014) pre-trained on the Common Crawl corpus (consisting of 840B-tokens), which we obtain from the authors\u0027 website. 8", "weight": 0.87}, {"from": "Conclusions", "to": "Our experiments are implemented in Python 2.7 and we use Keras deep learning framework for our models ", "weight": 0.87}, {"from": "Conclusions", "to": "We consider the evaluation sets from Sect. 4.2 and the following variations of PIX and REG models (Sect. 3.2). The subindex EMB denotes a model that employs GloVe embeddings and RND a model with embeddings randomly drawn from a dimension-wise normal distribution of mean (\u03bc) and standard deviation (\u03c3) equal to those of the GloVe embeddings, preserving the original dimensionality (d=300). A third type employs one-hot vectors (1H). We additionally  consider a control method (ctrl) that outputs random normal predictions of \u03bc and \u03c3 equal to the dimension-wise mean and standard deviation of the training targets. We test statistical significance with a Friedman rank test and post hoc Nemenyi tests on the results of the 10 folds. We indicate with an asterisk * in the tables when a method is significantly better than the rest (p \u003c 0.01) within the same model (PIX or REG). ", "weight": 0.87}, {"from": "Conclusions", "to": "To ensure that model predictions are meaningful and interpretable, we further validate the quantitative results above with a qualitative evaluation. Notice that all plots in ", "weight": 0.87}, {"from": "Conclusions", "to": "We study how the weights of the model provide insight into the spatial properties of words. To obtain more interpretable weights, we learn a REG 9 model without hidden layers, resulting in only an embedding layer followed by a linear output layer\u0177 = W out u + b out , where", "weight": 0.87}, {"from": "Conclusions", "to": "By using one-hot encodings w S , w R , w O , the concatenation layer u becomes of size |V S | + |V R | + |V O | + 2 + 2. E.g., if \"wearing\" has one-hot index j in the Relationships\u0027 vocabulary (V R ), its index in the concatenation layer is |V S | + j. The product W out u is a 4-dimensional vector, where its i-th component is the product of the i-th row of W out with the vector u. Thus, the component |V S |+j of the i-th row of W out gives us the influence of the j-th relationship (i.e., \"wearing\") on the i-th dimension of the output\u0177 =   ", "weight": 0.87}, {"from": "Conclusions", "to": "Overall, this paper provides insight into the fundamental differences between implicit and explicit spatial language and extends the concept of spatial templates to implicit spatial language, the understanding of which requires common sense spatial knowledge about objects and actions. We define the task of predicting relative spatial arrangements of two objects under a relationship and present two embedding-based neural models that attain promising performance, proving that spatial templates can be accurately predicted from implicit spatial language. Remarkably, our models generalize well, predicting correctly unseen (generalized) object-relationshipobject combinations. Furthermore, the acquired common sense spatial knowledge-aided with word embeddingsallows the model to correctly predict templates for unseen words. Finally, we show that the weights of the model provide great insight into the spatial connotations of words.", "weight": 0.87}, {"from": "Conclusions", "to": "A first limitation of our approach is the fully supervised setting where the models are trained using images with detected ground truth objects and parsed text-which aims at keeping the design clean in this first study on implicit spatial templates. Notice however that methods to automatically parse images and text exist. In future work, we aim at implementing our approach in a weakly supervised setting. A second limitation is the 2D spatial treatment of the actual 3D world. It is worth noting however, that our models (PIX and REG) and setting trivially generalize to 3D if appropriate data are available.", "weight": 0.87}]);

        // adding nodes and edges to the graph
        data = {nodes: nodes, edges: edges};

        var options = {
    "configure": {
        "enabled": true,
        "filter": [
            "physics"
        ]
    },
    "edges": {
        "color": {
            "inherit": true
        },
        "smooth": {
            "enabled": false,
            "type": "continuous"
        }
    },
    "interaction": {
        "dragNodes": true,
        "hideEdgesOnDrag": false,
        "hideNodesOnDrag": false
    },
    "physics": {
        "enabled": true,
        "stabilization": {
            "enabled": true,
            "fit": true,
            "iterations": 1000,
            "onlyDynamicEdges": false,
            "updateInterval": 50
        }
    }
};
        
        

        
        // if this network requires displaying the configure window,
        // put it in its div
        options.configure["container"] = document.getElementById("config");
        

        network = new vis.Network(container, data, options);

        


        

        return network;

    }

    drawGraph();

</script>
</body>
</html>