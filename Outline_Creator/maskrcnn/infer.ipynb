{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from utils import (\n",
    "    overlay_ann,\n",
    "    overlay_mask,\n",
    "    show,\n",
    "    extract_elements\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "CATEGORIES2LABELS = {\n",
    "    0: \"bg\",\n",
    "    1: \"text\",\n",
    "    2: \"title\",\n",
    "    3: \"list\",\n",
    "    4: \"table\",\n",
    "    5: \"figure\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instance_segmentation_model(num_classes):\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
    "        in_features_mask,\n",
    "        hidden_layer,\n",
    "        num_classes\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_elements(image_path):\n",
    "    # model \n",
    "    num_classes = 6\n",
    "    model = get_instance_segmentation_model(num_classes)\n",
    "    model.cuda()\n",
    "\n",
    "    if os.path.exists('model_196000.pth'):\n",
    "        checkpoint_path = \"model_196000.pth\"\n",
    "    else:\n",
    "        checkpoint_path = \"../../../Downloads/model_196000.pth\"\n",
    "\n",
    "    assert os.path.exists(checkpoint_path)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    model.eval()\n",
    "    \n",
    "    # NOTE: custom  image\n",
    "    assert os.path.exists(image_path)\n",
    "    image_name = (os.path.basename(image_path)).split('.')[0]\n",
    "    print(image_name)\n",
    "    elements_path = f'{os.path.dirname(image_path)}/{image_name}_elements'\n",
    "    try:\n",
    "        os.mkdir(elements_path)\n",
    "    except:\n",
    "        print (\"Creation of the directory %s failed\" % elements_path)\n",
    "    else:\n",
    "        print(\"Successfully created the directory %s \" % elements_path)\n",
    "        \n",
    "        \n",
    "    image = cv2.imread(image_path)\n",
    "    rat = 1300 / image.shape[0]\n",
    "    image = cv2.resize(image, None, fx=rat, fy=rat)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    image = transform(image)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model([image.cuda()])\n",
    "\n",
    "    image = torch.squeeze(image, 0).permute(1, 2, 0).mul(255).numpy().astype(np.uint8)\n",
    "    \n",
    "    ROI_number = 0 \n",
    "    for pred in prediction:\n",
    "        for idx, mask in enumerate(pred['masks']):\n",
    "            if pred['scores'][idx].item() < 0.7:\n",
    "                continue\n",
    "\n",
    "            m = mask[0].mul(255).byte().cpu().numpy()\n",
    "            box = list(map(int, pred[\"boxes\"][idx].tolist()))\n",
    "            label = CATEGORIES2LABELS[pred[\"labels\"][idx].item()]\n",
    "\n",
    "            score = pred[\"scores\"][idx].item()\n",
    "\n",
    "            # image = overlay_mask(image, m)\n",
    "            extract_elements(image, box, label, ROI_number, elements_path)\n",
    "            image = overlay_ann(image, m, box, label, score)\n",
    "            ROI_number += 1\n",
    "    image_save_path = (f'./{elements_path}/masked_{os.path.basename(image_path)}')\n",
    "    cv2.imwrite(image_save_path, image)\n",
    "    # show(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(argv):\n",
    "    if len(argv) > 0 and os.path.exists(argv[0]):\n",
    "        file_path = argv[0]\n",
    "    else:\n",
    "        file_path = 'CVPR2017.pdf'\n",
    "    \n",
    "    pages = convert_from_path(file_path, dpi=200)\n",
    "    file_name = file_path.split('.')[0]\n",
    "    try:\n",
    "        os.mkdir(f'../tmp/images/{file_name}')\n",
    "    except:\n",
    "        print (\"Creation of the directory %s failed\" % f'../tmp/images/{file_name}')\n",
    "    else:\n",
    "        print(\"Successfully created the directory %s \" % f'../tmp/images/{file_name}')\n",
    "    \n",
    "    for idx,page in enumerate(pages):\n",
    "        page.save(f\"../tmp/images/CVPR2017/{idx}.png\", 'PNG')\n",
    "        image_path = f'../tmp/images/CVPR2017/{idx}.png'\n",
    "        get_page_elements(image_path)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the directory ../tmp/images/CVPR2017 \n",
      "0\n",
      "Successfully created the directory ../tmp/images/CVPR2017/0_elements \n",
      "Document semantic structure extraction (DSSE) is an\n",
      "ctively-researched area dedicated to understanding images\n",
      "f documents. The goal is to split a document image into re-\n",
      "ions of interest and to recognize the role of each region. It\n",
      "s usually done in two steps: the first step, often referred to\n",
      "s page segmentation, is appearance-based and attempts to\n",
      "istinguish text regions from regions like figures, tables and\n",
      "ine segments. The second step, often referred to as logical\n",
      "tructure analysis, is semantics-based and categorizes each\n",
      "egion into semantically-relevant classes like paragraph and\n",
      "aption\n",
      "\f",
      "\n",
      "Label:\n",
      "text0\n",
      "We present an end-to-end, multimodal, fully convolu-\n",
      "tional network for extracting semantic structures from doc-\n",
      "ument images. We consider document semantic structure\n",
      "extraction as a pixel-wise segmentation task, and propose a\n",
      "unified model that classifies pixels based not only on their\n",
      "visual appearance, as in the traditional page segmentation\n",
      "task, but also on the content of underlying text. Moreover,\n",
      "we propose an efficient synthetic document generation pro-\n",
      "cess that we use to generate pretraining data for our net-\n",
      "work. Once the network is trained on a large set of synthetic\n",
      "documents, we fine-tune the network on unlabeled real doc-\n",
      "uments using a semi-supervised approach. We systemati-\n",
      "cally study the optimum network architecture and show that\n",
      "both our multimodal approach and the synthetic data pre-\n",
      "training significantly boost the performance.\n",
      "\f",
      "\n",
      "Label:\n",
      "text1\n",
      "volutional network (MFCN) that simultaneously identifies\n",
      "both appearance-based and semantics-based classes. Itis a\n",
      "generalized page segmentation model that additionally per-\n",
      "forms fine-grained recognition on text regions: text regions\n",
      "are assigned specific labels based on their semantic func\n",
      "tionality in the document. Our approach simplifies DSSE\n",
      "and better supports document image understanding.\n",
      "\f",
      "\n",
      "Label:\n",
      "text2\n",
      "In many cases, regions like section headings or captions\n",
      "can be visually identified. In Fig. 1 (a), one can easily ree-\n",
      "ognize the different roles of the same name, However, a\n",
      "robust DSSE system needs the semantic information of the\n",
      "text to disambiguate possible false identifications. For ex-\n",
      "ample, in Fig. | (b), the text in the large font might look like\n",
      "section heading, but it does not function that way; the lines\n",
      "beginning with dashes might be mislabeled as a list.\n",
      "\f",
      "\n",
      "Label:\n",
      "text3\n",
      "Learning to Extract Semantic Structure from Documents\n",
      "Using Multimodal Fully Convolutional Neural Networks\n",
      "\f",
      "\n",
      "Label:\n",
      "title4\n",
      "fo this end, our multimodal fully Convolutional network\n",
      "is designed to leverage the textual information in the docu-\n",
      "ment as well. To incorporate textual information in a CNN-\n",
      "based architecture, we build a text embedding map and feed\n",
      "it to our MFCN. More specifically, we embed each sentence\n",
      "and map the embedding to the corresponding pixels where\n",
      "the sentence is represented in the document. Fig. 2 summa-\n",
      "rizes the architecture of the proposed MFCN model. Our\n",
      "\f",
      "\n",
      "Label:\n",
      "text5\n",
      "We consider DSSE as a pixel-wise segmentation prob-\n",
      "jem: each pixel is labeled as background, figure, table,\n",
      "\f",
      "\n",
      "Label:\n",
      "text6\n",
      "Teiaconciedsanwhibs eng BC Ushene\n",
      "SS 2\n",
      "\n",
      "(a)\n",
      "seg ‘hie itn ee ps\n",
      "\f",
      "\n",
      "Label:\n",
      "figure7\n",
      "Figure 1: (a) Examples that are difficult to identify if only\n",
      "based on text. The same name can be a title, an author or\n",
      "a figure caption, (b) Examples that are difficult to identify\n",
      "if only based on visual appearance. Text in the large font\n",
      "might be mislabeled as a section heading. Text with dashes\n",
      "might be mislabeled as a list.\n",
      "\f",
      "\n",
      "Label:\n",
      "text8\n",
      "Abstract\n",
      "\f",
      "\n",
      "Label:\n",
      "title9\n",
      "paragraph, section heading, list, caption, etc. We show\n",
      "that our MFCN model trained in an end-to-end, pixels-to-\n",
      "pixels manner on document images exceeds the state-of-\n",
      "the-art significantly. It eliminates the need to design com-\n",
      "plex heuristic rules and extract hand-crafted features [30,\n",
      "22.21, 46, 4].\n",
      "\f",
      "\n",
      "Label:\n",
      "text10\n",
      "‘iao Yang', Ersin Yumer', Paul Asente', Mike Kraley', Daniel Kifer’, C. Lee Giles\n",
      "'The Pennsylvania State University ‘Adobe Research\n",
      "\f",
      "\n",
      "Label:\n",
      "text11\n",
      "|. Introduction\n",
      "\f",
      "\n",
      "Label:\n",
      "title12\n",
      "1\n",
      "Successfully created the directory ../tmp/images/CVPR2017/1_elements \n",
      "a — Bridge Output Segmentatio\n",
      "Lee.\n",
      "m Pure as\n",
      "a x 8 8 L\n",
      "=a Pepe be =\n",
      "— Peps\n",
      "a crcoser bier = 3\n",
      "put Image Decoder\n",
      "[EDI Auntiary decoder\n",
      "\f",
      "\n",
      "Label:\n",
      "figure0\n",
      "© We propose an end-to-end, unified network to address\n",
      "document semantic structure extraction. Unlike pre-\n",
      "vious two-step processes, we simultaneously identify\n",
      "both appearance-based and semantics-based classes.\n",
      "\n",
      "© Our network supports both supervised training on im-\n",
      "age and text of documents, as well as unsupervised\n",
      "auxiliary training for better representation learning.\n",
      "\n",
      "# We propose a synthetic data generation process and use\n",
      "it to synthesize a large-scale dataset for training the\n",
      "‘ennervised part of our deen MECN model\n",
      "\f",
      "\n",
      "Label:\n",
      "list1\n",
      "Une OF the vomlenecss in Wailing rully comvoumonal\n",
      "networks is the need for pixel-wise ground truth data, Pre-\n",
      "vious document understanding datasets (31, 44, 50, 6] are\n",
      "limited by both their small size and the lack of fine-grained\n",
      "semantic labels such as section headings, lists, or figure and\n",
      "table captions. To address these issues, we propose an ef-\n",
      "ficient synthetic document generation process and use it to\n",
      "generate large-scale pretraining data for our network. Fur-\n",
      "thermore, we propose two unsupervised tasks for better gen-\n",
      "eralization to real documents: reconstruction and consis-\n",
      "tency tasks. The former enables better representation learn-\n",
      "ing by reconstructing the input image, whereas the latter en-\n",
      "courages pixels belonging to the same regions have similar\n",
      "\f",
      "\n",
      "Label:\n",
      "text2\n",
      "model consists of four parts: an encoder that learns a hier-\n",
      "archy of feature representations, a decoder that outputs seg-\n",
      "mentation masks, an auxiliary decoder for reconstruction\n",
      "during training, and a bridge that merges visual representa-\n",
      "tions and textual representations. We assume that the docu-\n",
      "ment text has been pre-extracted. For document images this\n",
      "can be done with modern OCR engines [47, 1, 2].\n",
      "\f",
      "\n",
      "Label:\n",
      "text3\n",
      "Page Segmentation. Most earlier works on page seg-\n",
      "mentation [30, 22, 21, 46, 4, 45] fall into two cate-\n",
      "gories: bottom-up and top-down approaches. Bottom-up\n",
      "approaches [30, 46, 4] first detect words based on local fea-\n",
      "tures (white/black pixels or connected components), then\n",
      "sequentially group words into text lines and paragraphs.\n",
      "However, such approaches suffer from the identification and\n",
      "grouping of connected components being time-consuming.\n",
      "Top-down approaches [22, 21] iteratively split a page into\n",
      "columns, blocks, text lines and words. With both of these\n",
      "approaches it is difficult to correctly segment documents\n",
      "with complex layout, for example a document with non-\n",
      "rectanoular figures [38]\n",
      "\f",
      "\n",
      "Label:\n",
      "text4\n",
      "Figure 2: The architecture of the proposed multimodal fully convolutional neural network. It consists of four parts: an\n",
      "encoder that learns a hierarchy of feature representations, a decoder that outputs segmentation masks, an auxiliary decoder\n",
      "for unsupervised reconstruction, and a bridge that merges visual representations and textual representations. The auxiliary\n",
      "decoder only exists during training.\n",
      "\f",
      "\n",
      "Label:\n",
      "text5\n",
      "Logical Structure Analysis. Logical structure is de-\n",
      "ined as a hierarchy of logical components in documents,\n",
      "such as section headings, paragraphs and lists [38]. Early\n",
      "work in logical structure discovery [18, 29, 24, 14] focused\n",
      "on using a set of heuristic rules based on the location, font\n",
      "and text of each sentence. Shilman et al. [45] modeled doc-\n",
      "ment layout as a grammar and used machine learning to\n",
      "ninimize the cost of a invalid parsing. Luong et al. [35]\n",
      "sroposed using a conditional random fields model to jointly\n",
      "\f",
      "\n",
      "Label:\n",
      "text6\n",
      "Our main contributions are summarized as follows:\n",
      "\f",
      "\n",
      "Label:\n",
      "text7\n",
      "‘With recent advances in deep convolutional neural net-\n",
      "works, several neural-based models have been proposed.\n",
      "Chen et al. [12] applied a convolutional auto-encoder to\n",
      "learn features from cropped document image patches, then\n",
      "use these features to train a SVM [15] classifier, Vo et\n",
      "al. [52] proposed using FCN to detect lines in handwritten\n",
      "document images. However, these methods are strictly re-\n",
      "stricted to visual cues, and thus are not able to discover the\n",
      "semantic meaning of the underlying text.\n",
      "\f",
      "\n",
      "Label:\n",
      "text8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Background\n",
      "\f",
      "\n",
      "Label:\n",
      "title9\n",
      "2\n",
      "Successfully created the directory ../tmp/images/CVPR2017/2_elements \n",
      "Unsupervised Learning. Several methods have been\n",
      "proposed to use unsupervised learning to improve super-\n",
      "vised learning tasks. Mairal et al. [36] proposed a sparse\n",
      "coding method that leans sparse local features by sparsity-\n",
      "constrained reconstruction loss functions. Zhao et al. [58]\n",
      "proposed a Stacked What-Where Auto-Encoder that uses\n",
      "unpooling during reconstruction, By injecting noise into the\n",
      "input and the middle features, a denoising auto-encoder [51]\n",
      "can learn robust filters that recover uncorrupted input. The\n",
      "main focus in unsupervised learning has been image-level\n",
      "classification and generative approaches, whereas in this pa-\n",
      "per we explore the potential of such methods for pixel-wise\n",
      "semantic segmentation.\n",
      "\f",
      "\n",
      "Label:\n",
      "text0\n",
      "As shown In Fig. 2, our MPCN mode! has tour parts:\n",
      "an encoder, two decoders and a bridge. The encoder and\n",
      "Jecoder parts roughly follow the architecture guidelines set\n",
      "forth by Noh et al. [11]. However, several changes have\n",
      "been made to better address document segmentation.\n",
      "\f",
      "\n",
      "Label:\n",
      "text1\n",
      "‘Wen et al. [53] recently proposed a center loss that en-\n",
      "-ourages data samples with the same label to have a similar\n",
      "visual representation. Similarly, we introduce an intra-class\n",
      "consistency constraint. However, the “center” for each class\n",
      "n their loss is determined by data samples across the whole\n",
      "Jataset, while in our case the “center” is locally determined\n",
      ">y pixels within the same region in each image.\n",
      "\f",
      "\n",
      "Label:\n",
      "text2\n",
      "label each sentence based on several hand-crafted features.\n",
      "However, the performance of these methods is limited by\n",
      "their reliance on hand-crafted features, which cannot cap-\n",
      "ture the highly semantic context.\n",
      "\f",
      "\n",
      "Label:\n",
      "text3\n",
      "Language and Vision, Several joint learning tasks\n",
      "such as image captioning [16, 28], visual question answer-\n",
      "ing [5, 20, 37}, and one-shot learning [19, 48, 1] have\n",
      "demonstrated the significant impact of using textual and\n",
      "visual representations in a joint framework. Our work is\n",
      "unique in that we use textual embedding directly for a seg-\n",
      "\f",
      "\n",
      "Label:\n",
      "text4\n",
      "such as section heading and caption usually occupy rela-\n",
      "ively small areas, Moreover, correctly identifying certain\n",
      "egions often relies on small visual cues, like lists being\n",
      "ddentified by small bullets or numbers in front of each item.\n",
      "This suggests that low-level features need to be used. How-\n",
      "ever, because max-pooling naturally loses information dur-\n",
      "ing downsampling, FCN often performs poorly for small\n",
      "sbjects. Long et al. [33] attempt to avoid this problem us-\n",
      "ng skip connections, However, simply averaging indepen-\n",
      "Jent predictions based on features at different scales does\n",
      "rot provide a satisfying solution. Low-level representations,\n",
      "imited by the local receptive field, are not aware of object-\n",
      "evel semantic information; on the other hand, high-level\n",
      "jeatures are not necessarily aligned consistently with object\n",
      "oundaries because CNN models are invariant to transla-\n",
      "ion, We propose an alternative skip connection implemen-\n",
      "ation, illustrated by the blue arrows in Fig. 2, similar to that\n",
      "ised in the independent work SharpMask [43]. However,\n",
      "hey use bilinear upsampling after skip connection while we\n",
      "ise unpooling to preserve more spatial information.\n",
      "\f",
      "\n",
      "Label:\n",
      "text5\n",
      "Collecting pixel-wise annotations for thousands or mil-\n",
      "lions of images requires massive labor and cost. To this end,\n",
      "several methods [42, 56, 34] have been proposed to harness\n",
      "weak annotations (bounding-box level or image level anno-\n",
      "tations) in neural network training. Our consistency loss re-\n",
      "lies on similar intuition but does not require a “class label”\n",
      "for each bounding box.\n",
      "\f",
      "\n",
      "Label:\n",
      "text6\n",
      "We also notice that broader context information is\n",
      "needed to identify certain objects. For an instance, it is\n",
      "often difficult to tell the difference between a list and sev-\n",
      "eral paragraphs by only looking at parts of them. In Fig. 3,\n",
      "© correctly segment the right part of the list, the receptive\n",
      "fields must be large enough to capture the bullets on the\n",
      "left. Inspired by the Inception architecture [49] and dilated\n",
      "convolution [5-1], we propose a dilated convolution block,\n",
      "which is illustrated in Fig. 4 (left). Each dilated convolu-\n",
      "ion block consists of 5 dilated convolutions with a 3 x 3\n",
      "kernel size and a dilation d = 1,2,4,8, 16.\n",
      "\f",
      "\n",
      "Label:\n",
      "text7\n",
      "Our method does supervised training for pixel-wise seg-\n",
      "mentation with a specialized multimodal fully convolu-\n",
      "tional network that uses a text embedding map jointly\n",
      "with the visual cues. Moreover, our MFCN architecture\n",
      "also supports two unsupervised learning tasks to improve\n",
      "the learned document representation: a reconstruction task\n",
      "based on an auxiliary decoder and a consistency task eval-\n",
      "uated in the main decoder branch along with the per-pixel\n",
      "segmentation loss.\n",
      "\f",
      "\n",
      "Label:\n",
      "text8\n",
      "mentation task for the first time, and we show that our ap-\n",
      "proach improves the results of traditional segmentation ap-\n",
      "proaches that only use visual cues.\n",
      "\f",
      "\n",
      "Label:\n",
      "text9\n",
      "Semantic Segmentation. Large-scale annotations [32]\n",
      "and the development of deep neural network approaches\n",
      "such as the fully convolutional network (FCN) [33] have led\n",
      "to rapid improvement of the accuracy of semantic segmen-\n",
      "tation [13, 42, 41, 54]. However, the originally proposed\n",
      "FCN model has several limitations, such as ignoring small\n",
      "objects and mislabeling large objects due to the fixed recep-\n",
      "tive field size. To address this issue, Noh et al. [41] pro-\n",
      "posed using unpooling, a technique that reuses the pooled\n",
      "“location” at the up-sampling stage, Pinheiro et al. [43]\n",
      "attempted to use skip connections to refine segmentation\n",
      "boundaries. Our model addresses this issue by using a di-\n",
      "lated block, inspired by dilated convolutions [54] and recent\n",
      "work [49, 23] that groups several layers together . We fur-\n",
      "ther investigate the effectiveness of different approaches to\n",
      "optimize our network architecture.\n",
      "\f",
      "\n",
      "Label:\n",
      "text10\n",
      "5. Method\n",
      "\f",
      "\n",
      "Label:\n",
      "title11\n",
      "3.1. Multimodal Fully Convolutional Network\n",
      "\f",
      "\n",
      "Label:\n",
      "title12\n",
      "3\n",
      "Successfully created the directory ../tmp/images/CVPR2017/3_elements \n",
      "\f",
      "\n",
      "Label:\n",
      "figure0\n",
      "Specifically, our word embedding 1s learned using the\n",
      "skip-gram model (39, 40]. Fig. 4 (right) shows the basic\n",
      "diagram. Let V be the number of words in a vocabulary\n",
      "and w be a V-dimensional one-hot vector representing a\n",
      "word, The training objective is to find a N-dimensional\n",
      "(N << V) vector representation for each word that is useful\n",
      "for predicting the neighboring words, More formally, given\n",
      "a sequence of words [w1,w2,--» wr], we maximize the\n",
      "averave log probability\n",
      "\f",
      "\n",
      "Label:\n",
      "text1\n",
      "Figure 3: A cropped document image and its segmentation\n",
      "mask generated by our model. Note that the top-right corner\n",
      "of the list is yellow instead of cyan, indicating that it has\n",
      "been mislabeled as a paragraph.\n",
      "\f",
      "\n",
      "Label:\n",
      "text2\n",
      "‘We treat a sentence as the minimum unit that conveys\n",
      "certain semantic meanings, and represent it using a low-\n",
      "dimensional vector. Our sentence embedding is built by\n",
      "averaging embeddings for individual words. This is a sim-\n",
      "ple yet effective method that has been shown to be useful\n",
      "in many applications, including sentiment analysis [26] and\n",
      "text classification [27]. Using such embeddings, we cre-\n",
      "ate a text embedding map as follows: for each pixel inside\n",
      "the area of a sentence, we use the corresponding sentence\n",
      "embedding as the input. Pixels that belong to the same sen-\n",
      "tence thus share the same embedding. Pixels that do not\n",
      "belong to any sentences will be filled with zero vectors. For\n",
      "a document image of size H x W, this process results in\n",
      "an embedding map of size N x H x W if the learned sen-\n",
      "tence embeddings are V-dimensional vectors. The embed-\n",
      "ding map is later concatenated with a feature response along\n",
      "the number-of-channel dimensions (see Fig. 2).\n",
      "\f",
      "\n",
      "Label:\n",
      "text3\n",
      "A= PB\n",
      "\f",
      "\n",
      "Label:\n",
      "figure4\n",
      "where T is the length of the sequence and C' is the size of\n",
      "the context window. The probability of outputting a word\n",
      "\f",
      "\n",
      "Label:\n",
      "text5\n",
      "Traditional image semantic segmentation models learn\n",
      "the semantic meanings of objects from a visual perspective\n",
      "Our task, however, also requires understanding the text in\n",
      "images from a linguistic perspective. Therefore, we build a\n",
      "text embedding map and feed it to our multimodal model to\n",
      "make use of both visual and textual representations.\n",
      "\f",
      "\n",
      "Label:\n",
      "text6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 4: Left: A dilated block that contains 5 dilated\n",
      "convolutional layers with different dilation d.  Batch-\n",
      "Normalization and non-linearity are not shown for brevity.\n",
      "Right: The skip-gram model for word embeddings.\n",
      "\f",
      "\n",
      "Label:\n",
      "text7\n",
      "3.2. Text Embedding Map\n",
      "\f",
      "\n",
      "Label:\n",
      "title8\n",
      "Although our synthetic documents (Sec. 4) provide a\n",
      "jarge amount of labeled data for training, they are limited\n",
      "in the Variations of their layouts. To this end, we define two\n",
      "unsupervised loss functions to make use of real documents\n",
      "and to encourage better representation learning.\n",
      "\f",
      "\n",
      "Label:\n",
      "text9\n",
      "ROCORSTUCHION PRsk, st has Ocen shown Mat recon-\n",
      "struction can help learning better representations and there-\n",
      "fore improves performance for supervised tasks [58, 57]\n",
      "We thus introduce a second decoder pathway (Fig. 2 - axil-\n",
      "lary decoder), denoted as Dee, and define a reconstruction\n",
      "loss at intermediate features. This auxiliary decoder only\n",
      "exists during the training phase.\n",
      "\f",
      "\n",
      "Label:\n",
      "text10\n",
      "W. given an input word w; 1s defined using softmax:\n",
      "\f",
      "\n",
      "Label:\n",
      "text11\n",
      "Consistency 1asK. Fixei-wis€ annotations are labor-\n",
      "intensive to obtain, however it is relatively easy to get a set\n",
      "of bounding boxes for detected objects in a document. For\n",
      "documents in PDF format, one can find bounding boxes by\n",
      "analyzing the rendering commands in the PDF files (See\n",
      "our supplementary document for typical examples). Even\n",
      "if their labels remain unknown, these bounding boxes are\n",
      "still beneficial: they provide knowledge of which parts of a\n",
      "document belongs to the same objects and thus should not\n",
      "be segmented into different fragments.\n",
      "\f",
      "\n",
      "Label:\n",
      "text12\n",
      "where v, and v,, are the “input” and “output” N-\n",
      "Jimensional vector representations of w.\n",
      "\f",
      "\n",
      "Label:\n",
      "text13\n",
      "PACE ETS By Bey 8 Bes OR EN CON EONS OREN BAYT ON\n",
      "the encoder, and ag be the input image. For a feed-forward\n",
      "\n",
      "convolutional network, a, is a feature map of size Ci x Hix\n",
      "\n",
      "Wi. Our auxiliary decoder Dyce attempts to reconstruct a\n",
      "hierarchy of feature maps {i}. Reconstruction loss L'\").\n",
      "\n",
      "for a specific 1 is therefore defined as\n",
      "\f",
      "\n",
      "Label:\n",
      "text14\n",
      "4\n",
      "Successfully created the directory ../tmp/images/CVPR2017/4_elements \n",
      "‘we perrorm per-cnanne: mean subtraction and resize\n",
      "each input image so that its longer side is less than 384\n",
      "pixels. No other pre-processing is applied. We use\n",
      "Adadelta [55] with a mini-batch size of 2. During semi-\n",
      "supervised training, mini-batches of synthetic and real\n",
      "documents are used alternatively, For synthetic docu-\n",
      "ments, both per-pixel classification loss and the unsuper-\n",
      "vised losses are active at back-propagation, while for real\n",
      "documents, only the unsupervised losses are active. Since\n",
      "the labels are unbalanced (e.g. the area of paragraphs is\n",
      "\f",
      "\n",
      "Label:\n",
      "text0\n",
      "¢ For paragraphs, we randomly sample sentences trom a\n",
      "2016 English Wikipedia dump [3]\n",
      "\n",
      "* For section headings, we only sample sentences and\n",
      "phrases that are section or subsection headings in the\n",
      "“Contents” block in a Wikipedia page\n",
      "\n",
      "¢ For lists, we ensure that all items in a list come from\n",
      "the same Wikipedia page.\n",
      "\n",
      "¢ For captions, we either use the associated caption (for\n",
      "images from MS COCO) or the title of the image in\n",
      "web image search, which can be found in the span with\n",
      "class name “ire_pt”.\n",
      "\f",
      "\n",
      "Label:\n",
      "list1\n",
      "An total, our synthetic dataset contains 155,000 document\n",
      "mages, Examples of our synthetic documents are shown\n",
      "n Fig, 5. Please refer to our supplementary document for\n",
      "nore examples of synthetic documents and individual ele-\n",
      "nents used in the generation process.\n",
      "\f",
      "\n",
      "Label:\n",
      "text2\n",
      "Since our MFCN aims to generate a segmentation mask\n",
      "of the whole document image, pixel-wise annotations are\n",
      "required for the supervised task. While there are several\n",
      "publicly available datasets for page segmentation [44, 50,\n",
      "6), there are only a few hundred to a few thousand pages\n",
      "in each. Furthermore, the types of labels are limited, for\n",
      "example to text, figure and table, however our goal is to\n",
      "perform a much more granular segmentation.\n",
      "\f",
      "\n",
      "Label:\n",
      "text3\n",
      "Fig. 2 summarizes the architecture of our model. The\n",
      "auxiliary decoder only exists in the training phase. All con-\n",
      "volutional layers have a 3 x 3 kernel size and a stride of\n",
      "1. The pooling (in the encoders) and unpooling (in the de-\n",
      "coders) have a kernel size of 2 x 2. We adopt batch normal-\n",
      "ization [25] immediately after each convolution and before\n",
      "all non-linear functions,\n",
      "\f",
      "\n",
      "Label:\n",
      "text4\n",
      "‘To further increase the complexity of the generated docu-\n",
      "ment layouts, we collected and labeled 271 documents with\n",
      "varied, complicated layouts. We then randomly replaced\n",
      "each element with a standalone paragraph, figure, table,\n",
      "caption, section heading ot list generated as stated above.\n",
      "\f",
      "\n",
      "Label:\n",
      "text5\n",
      "triple-column PDFs. Candidate figures include academic-\n",
      "style figures and graphic drawings downloaded using web\n",
      "image search, and natural images from MS COCO [32],\n",
      "which associates each image with several captions. Candi-\n",
      "date tables are downloaded using web image search. Var-\n",
      "ious queries are used to increase the diversity of down-\n",
      "loaded tables. Since our MFCN model relies on the seman-\n",
      "tic meaning of text to make prediction, the content of text\n",
      "regions (paragraph, section heading, list, caption) must be\n",
      "carefully selected:\n",
      "\f",
      "\n",
      "Label:\n",
      "text6\n",
      "Our synthetic document engine uses two methods to gen-\n",
      "srate documents. The first produces completely automated\n",
      "and random layout of partial data scraped from the web.\n",
      "More specifically, we generate LaTeX source files in which\n",
      "paragraphs, figures, tables, captions, section headings and\n",
      "ists are randomly arranged to make up single, double, or\n",
      "\f",
      "\n",
      "Label:\n",
      "text7\n",
      "to address these issues, we created a synthetic data en-\n",
      "gine, capable of generating large-scale, pixel-wise anno-\n",
      "tated documents.\n",
      "\f",
      "\n",
      "Label:\n",
      "text8\n",
      "By building on the intuition that regions belonging to\n",
      "same objects should have similar feature representations,\n",
      "we define the consistency task loss Leons as follows. Let\n",
      "pii,j) (i = 1.2,+* Hj = 1,2,--- W) be activations at lo-\n",
      "cation (i, j) in a feature map of size C x H x Wand b be\n",
      "the rectangular area in a bounding box. Let each rectangu-\n",
      "lar area b is of size H, x Wy. Then, for each b € By Leons\n",
      "will be given by\n",
      "\f",
      "\n",
      "Label:\n",
      "text9\n",
      "5. Implementation Details\n",
      "\f",
      "\n",
      "Label:\n",
      "title10\n",
      "We use the unsupervised consistency loss, Leong, a8 a loss\n",
      "layer, that is evaluated at the main decoder branch (blue\n",
      "branch in Fig. 2) along with supervised segmentation loss.\n",
      "\f",
      "\n",
      "Label:\n",
      "text11\n",
      "The consistency loss Leons is differentiable and can be\n",
      "optimized using stochastic gradient descent. ‘The gradient\n",
      "of Leone With respect to p;; » is\n",
      "\f",
      "\n",
      "Label:\n",
      "text12\n",
      "Minimizing consistency loss Leo, encourages intra-region\n",
      "consistency.\n",
      "\f",
      "\n",
      "Label:\n",
      "text13\n",
      "4. Synthetic Document Data\n",
      "\f",
      "\n",
      "Label:\n",
      "title14\n",
      "1 2\n",
      "Looms = Fa, Do Iran ~ P|,\n",
      "(ised\n",
      "1\n",
      "po ~ AW, > Pug)\n",
      "{ijjeb\n",
      "\f",
      "\n",
      "Label:\n",
      "figure15\n",
      "ince HW, > 1, for efficiency it can be approximated by:\n",
      "\f",
      "\n",
      "Label:\n",
      "text16\n",
      "5\n",
      "Successfully created the directory ../tmp/images/CVPR2017/5_elements \n",
      "We further evaluate the effectiveness of replacing Duin-\n",
      "car upsampling with unpooling, giving Model3. All up-\n",
      "sampling layers in Model2 are replaced by unpooling while\n",
      "other parts are kept unchanged. Doing so results in a signif-\n",
      "icant improvement for mean IoU (65.4% vs. 71.2%). This\n",
      "suggests that the pooled index should not be discarded dur-\n",
      "ing decoding. These indexes are helpful to disambiguate\n",
      "the location information when constructing the segmenta-\n",
      "tion mask in the decoder.\n",
      "\f",
      "\n",
      "Label:\n",
      "text0\n",
      "‘a - =\n",
      "\f",
      "\n",
      "Label:\n",
      "figure1\n",
      "‘We used three datasets for evaluations: ICDAR2015 [6],\n",
      "SectLabel [35] and our new dataset named DSSE-200.\n",
      "ICDAR2015 [6] is a dataset used in the biennial IC-\n",
      "DAR page segmentation competitions [7] focusing more\n",
      "on appearance-based regions. The evaluation set of IC-\n",
      "DAR2015 consists of 70 sampled pages from contemporary\n",
      "magazines and technical articles. SectL.abel [35] consists\n",
      "of 40 academic papers with 347 pages in the field of com-\n",
      "puter science. Each text line in these papers is manually\n",
      "assigned a semantics-based label such as text, section head-\n",
      "ing or list item. In addition to these two datasets, we in-\n",
      "troduce DSSE-200!, which provides both appearance-based\n",
      "and semantics-based labels. DSSE-200 contains 200 pages\n",
      "from magazines and academic papers. Regions in a page are\n",
      "assigned labels from the following dictionary: figure, table,\n",
      "section, caption, list and paragraph. Note that DSSE-200\n",
      "has a more granular segmentation than previously released.\n",
      "benchmark datasets.\n",
      "\f",
      "\n",
      "Label:\n",
      "text2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENR I ERIE SINE COME LIOIES LOBE TORE BEOMINEN SET\n",
      "Model2. Note that this model is similar to the SharpMask\n",
      "model. We observe a mean IoU of 65.4%, 4% better than\n",
      "the base model. The improvements are even more signifi-\n",
      "cant for small objects like captions.\n",
      "\f",
      "\n",
      "Label:\n",
      "text3\n",
      "intersection-over-union (1oU), which 1s standard in seman-\n",
      "‘ic segmentation tasks. We optimize the architecture of\n",
      "our MFCN model based on the DSSE-200 dataset since\n",
      "i contains both appearance-based and semantics-based la-\n",
      "pels. Sec. 6.4 compares our results to state-of-the-art meth-\n",
      "ods on the ICDAR2015 and SectLabel datasets.\n",
      "\f",
      "\n",
      "Label:\n",
      "text4\n",
      "Finally, we investigate the use of dilated convolutions.\n",
      "Model3 is equivalent to using dilated convolution when\n",
      "d = 1. Modeld sets d = 8 while Model5 uses the di-\n",
      "lated block illustrated in Fig. 4 (left). The number of output\n",
      "channels are adjusted such that the total number of parame-\n",
      "\f",
      "\n",
      "Label:\n",
      "text5\n",
      "We first systematically evaluate the effectiveness of dif-\n",
      "ferent network architectures. Results are shown in Table 1.\n",
      "Note that these results do not incorporate textual informa-\n",
      "tion or unsupervised learning tasks, The purpose of this\n",
      "experiment is to find the best “base” architecture to be used\n",
      "in the following experiments. All models are trained from\n",
      "scratch and evaluated on the DSSE-200 dataset.\n",
      "\f",
      "\n",
      "Label:\n",
      "text6\n",
      "AS a simple baseline (lable | Modell), we train a plain\n",
      "encoder-decoder style model for document segmentation.\n",
      "It consists of a feed-forward convolutional network as an\n",
      "encoder, and a decoder implemented by a fully convolu-\n",
      "tional network. Upsampling is done by bilinear interpola-\n",
      "tion, This model achieves a mean IoU of 61.4%.\n",
      "\f",
      "\n",
      "Label:\n",
      "text7\n",
      "POT Text EMDECEINE, We represent cach WOTG as a beo~\n",
      "dimensional vector and train a skip-gram model on the\n",
      "2016 English Wikipedia dump [3]. Embeddings for out-\n",
      "of-dictionary words are obtained following Bojanowski et\n",
      "al. [9]. We use Tesseract [47] as our OCR engine.\n",
      "\f",
      "\n",
      "Label:\n",
      "text8\n",
      "Fost-processing. We apply an optional post-processing:\n",
      "step as a cleanup strategy for segment masks. For docu-\n",
      "ments in PDF format, we obtain a set of candidate bounding\n",
      "boxes by analyzing the PDF format to find element boxes.\n",
      "\n",
      "We then refine the segmentation masks by first calculat-\n",
      "ing the average class probability for pixels belonging to the\n",
      "same box, followed by assigning the most likely label to\n",
      "these pixels.\n",
      "\f",
      "\n",
      "Label:\n",
      "text9\n",
      "much larger than that of caption), class weights for the per-\n",
      "pixel classification loss are set differently according to the\n",
      "total number of pixels in each class in the training set.\n",
      "\f",
      "\n",
      "Label:\n",
      "text10\n",
      "The performance 1s measured in terms of pixel-wise\n",
      "\f",
      "\n",
      "Label:\n",
      "text11\n",
      "1, ADlation Expermment on Viodel Architecture\n",
      "\f",
      "\n",
      "Label:\n",
      "title12\n",
      "hb. Experiments\n",
      "\f",
      "\n",
      "Label:\n",
      "title13\n",
      "igure 5: Example synthetic documents, raw segmentations and results after optional post-processing (Sec. 5). Se\n",
      "\f",
      "\n",
      "Label:\n",
      "text14\n",
      "Figt0.5 Example synthetic documents, raw segmentations and results after optional post-processing (Sec. 5). Segmentation\n",
      "label colors are: (BUR. EENC). RRRCny teeing. |. list and paragraph .\n",
      "\f",
      "\n",
      "Label:\n",
      "text15\n",
      "http: //personal .psu.edu/xuyl11/projects/\n",
      "vpr2017_doc. html.\n",
      "\f",
      "\n",
      "Label:\n",
      "text16\n",
      "6\n",
      "Successfully created the directory ../tmp/images/CVPR2017/6_elements \n",
      "S64 ET = fo\n",
      ": Bnnn6 = :\n",
      "—— |? . ae\n",
      "oa oi Le LL\n",
      "li =\n",
      "\f",
      "\n",
      "Label:\n",
      "figure0\n",
      "‘We now investigate the importance of textual informa-\n",
      "tion in our multimodal model. We take the best architec-\n",
      "ture, Model5, as our vision-only model, and incorporate a\n",
      "text embedding map via a bridge module depicted in Fig. 2.\n",
      "This combined model is fine-tuned on our synthetic docu-\n",
      "ments, As shown in Table 2, using text as well improves\n",
      "the performance for textual classes. The accuracy for sec-\n",
      "tion heading, caption, list and paragraph is boosted by 1.1%,\n",
      "0.1%, 1.7% and 2.2%, respectively.\n",
      "\f",
      "\n",
      "Label:\n",
      "text1\n",
      "We rely on existing OCR engines [47] to extract text, but\n",
      "hey are not always reliable for scanned documents of low\n",
      "juality. To quantitatively analyze the effects of using ex-\n",
      "racted text, we compare the performance of using extracted\n",
      "ext versus real text. The comparison is conducted on a sub-\n",
      "‘et of our synthetic dataset (200 images), since ground-truth\n",
      "ext is naturally available. As shown in Table 2, using real\n",
      "ext leads to a remarkable improvement (6.4%) for mean\n",
      "[oU, suggesting the effectiveness of incorporating textual\n",
      "information. Using OCR extracted text is not as effective,\n",
      "out still results in 2.6% improvement. It is better than the\n",
      ").3% improvement on DSSE-200 dataset; we attribute this\n",
      "© our synthetic data not being as complicated as DSSE-200,\n",
      "0 extracting text becomes easier.\n",
      "\f",
      "\n",
      "Label:\n",
      "text2\n",
      "eS\n",
      "Model# [| dilation upsampling skip | bkg figure table section caption list paragraph [| mean\n",
      "T T bilinear no | 80.3 754 627 500 338 S73 704 614\n",
      "2 1 bilinear yes | 821 767 744 SIS 424 58.7 744 |) 65.4\n",
      "3 1 unpooling yes | 84.1 81.2 776 546 60.3 65.9 748 nN2\n",
      "4 8 unpooling yes | 83.9 74.9 69.7 572 60.2 64.6 = 76.1 69.5\n",
      "5 block unpooling yes | 84.6 833 794 583 61.0 66.7 77.1 BO\n",
      "\f",
      "\n",
      "Label:\n",
      "table3\n",
      "Figure 6: Example real documents and their corresponding segmentation. Top: DSSE-200. Middle: ICDAR2015. Bottom:\n",
      "\n",
      "SectLabel. Since these documents are not in PDF format, the simple post-processing in Sec. 5 can not be applied. One may\n",
      "\n",
      "consider exploiting a CRF [13] to refine the segmentation, but that is beyond the main focus of this paper. Segmentation label,\n",
      "eee tO Tene Bae Scommentalion, Out mat rs beyond he 1\n",
      "\f",
      "\n",
      "Label:\n",
      "text4\n",
      "Table 1: Ablation experiments on DSSE-200 dataset. The architecture of each model is characterized by the dilation in\n",
      "convolution layers, the way of upsampling and the use of skip connection. IoU scores (%) are reported.\n",
      "\f",
      "\n",
      "Label:\n",
      "text5\n",
      "6.2. Adding Textual Information\n",
      "\f",
      "\n",
      "Label:\n",
      "title6\n",
      "ers are similar. Comparing the results for these three mod-\n",
      "‘Is, we can see that the IoU of Model4 for each class is on\n",
      "ar with or worse than Model3, while Model5 is better than\n",
      "oth Model3 and Model4 for all classes.\n",
      "\f",
      "\n",
      "Label:\n",
      "text7\n",
      "7\n",
      "Successfully created the directory ../tmp/images/CVPR2017/7_elements \n",
      "Here, we examine how the proposed two unsupervised\n",
      "learning tasks — reconstruction and consistency tasks —\n",
      "can complement the pixel-wise classification during train-\n",
      "ing. We take the best model in Sec. 6.2, and only change the\n",
      "training objectives. Our model is then fine-tuned in a semi-\n",
      "supervised manner as described in Sec. 5. The results are\n",
      "shown in Table 3. Adding the reconstruction task slightly\n",
      "improves the mean IoU by 0.6%, while adding the consis-\n",
      "tency task leads to a boost of 1.9%. These results justify our\n",
      "hypothesis that harnessing region information is beneficial.\n",
      "Combining both tasks results in a mean IoU of 75.9%.\n",
      "\f",
      "\n",
      "Label:\n",
      "text0\n",
      "Comparisons on SectLabel dataset (Table 5). Luong et\n",
      "at. [35] first use Omnipage [2] to localize and recognize text\n",
      "lines, then predict the semantics-based label for each line.\n",
      "The FI score for each class was reported, For fair compar-\n",
      "ison, we use the same set of text line bounding boxes, and\n",
      "use the averaged pixel-wise prediction as the label for each\n",
      "text line. Our model achieves better Fl scores for section\n",
      "heading (0.919 VS 0.916), caption (0.893 VS 0.781) and\n",
      "list (0.793 VS 0.712), while being capable of identifying\n",
      "figures and tables.\n",
      "\f",
      "\n",
      "Label:\n",
      "text1\n",
      "Comparisons on ICDAR2015 dataset (Table 4). Pre-\n",
      "vious pixel-wise page segmentation models usually solve a\n",
      "pinary segmentation problem and do not make predictions\n",
      "for fine-grained classes. For fair comparison, we change\n",
      "the number of output channels of the last layer to 3 (back-\n",
      "sround, figure and text) and fine-tune this last layer. Our bi-\n",
      "nary MECN model achieves 94.5%, 91.0% and 77.1% ToU\n",
      "scores for non-text (background and figure), text and figure\n",
      "regions, outperforming other models.\n",
      "\f",
      "\n",
      "Label:\n",
      "text2\n",
      "‘Table 4 and 5 present comparisons with several meth-\n",
      "ods that have previously reported performance on the IC-\n",
      "DAR2015 and SectLabel datasets. It is worth emphasiz-\n",
      "\f",
      "\n",
      "Label:\n",
      "text3\n",
      "WO PROPOSS? 8 TMCS SURY CORVIRARI EWU\n",
      "(MFCN) for document semantic structure extraction, The\n",
      "proposed model uses both visual and textual information\n",
      "Moreover, we propose an efficient synthetic data generation\n",
      "method that yields per-pixel ground-truth. Our unsuper-\n",
      "vised auxiliary tasks help boost performance tapping into\n",
      "unlabeled real documents, facilitating better representation\n",
      "learning. We showed that both the multimodal approach\n",
      "and unsupervised tasks can help improve performance. Our\n",
      "results indicate that we have improved the state of the art\n",
      "on previously established benchmarks. In addition, we\n",
      "are publicly providing the large synthetic dataset (135,000\n",
      "sanes) as well ac a new benchmark dataset: DSSE-200.\n",
      "\f",
      "\n",
      "Label:\n",
      "text4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This work started during Xiao Yang’s internship at\n",
      "Adobe Research. This work was supported by NSF grant\n",
      "CCF 1317560 and Adobe Systems Inc.\n",
      "\f",
      "\n",
      "Label:\n",
      "text5\n",
      "Table 4: IoU scores (%) for page segmentation on the\n",
      "ICDAR2015 dataset. For comparison purpose, only IoU\n",
      "scores for non-text, text and figure are shown. However our\n",
      "model can make fine-grained predictions as well.\n",
      "\f",
      "\n",
      "Label:\n",
      "text6\n",
      "ing that our MFCN model simultaneously predicts both\n",
      "appearance-based and semantics-based classes while other\n",
      "methods can not,\n",
      "\f",
      "\n",
      "Label:\n",
      "text7\n",
      "| Methods | non-text | text |\n",
      "Teptonica [8] MT [868\n",
      "Bukhari et al. [10] 90.6 | 90.3\n",
      "Ours (binary) 945 | 91.0\n",
      "Fernandez et al. {17} | 70.1 | 85.8\n",
      "Ours (binary) 771 | 91.0\n",
      "\f",
      "\n",
      "Label:\n",
      "table8\n",
      "fable 5: Fl scores on the SectLabel dataset. Note that our\n",
      "model can also identify non-text classes such as figures and\n",
      "ables.\n",
      "\f",
      "\n",
      "Label:\n",
      "text9\n",
      "7. Conclusion:\n",
      "\f",
      "\n",
      "Label:\n",
      "title10\n",
      "Acknowledgment\n",
      "\f",
      "\n",
      "Label:\n",
      "title11\n",
      "Table 3: IoU scores (%) when using different training ob-\n",
      "jectives on DSSE-200 dataset. c/s: pixel-wise classification\n",
      "teak, rec: reconstruction task and cons: consistency teak.\n",
      "\f",
      "\n",
      "Label:\n",
      "text12\n",
      "Table 2: IoU scores (%) on the DSSE-200 (D) and synthetic dataset (S) using text embedding map. On synthetic dataset, we\n",
      "further investigate the effects of using extracted text versus real text when building the text embedding map.\n",
      "\f",
      "\n",
      "Label:\n",
      "text13\n",
      "[base |] dataset__text_][ bkg figure table __section caption _ist_para. |] mean |\n",
      "Model5 D none 84.6 83.3 79.4 58.3 61.0 66.7 77.1 73.0\n",
      "Model5S D extract |} 83.9 83.7 79.7 59.4 611 68.4 793 | 73.3\n",
      "Model5 Ss none 87.7 83.1 84.3 70.8 70.9 82.30 83.1 79.6\n",
      "ModelS: s extract || 88.8 85.4 86.6 73.1 712 83.6 87.2 82.2\n",
      "Model5S Ss real 91.2 90.3 89.0 78.4 75.3 87.5 89.6 86.0\n",
      "\f",
      "\n",
      "Label:\n",
      "table14\n",
      "Methods section | caption | list para.\n",
      "Tuonget al. [55] | 0.916 | 0.781 | 0.712 | 0.969\n",
      "Ours 0.919 | 0,893 | 0.793 0.969\n",
      "\f",
      "\n",
      "Label:\n",
      "table15\n",
      "6.4. Comparisons with Prior Art\n",
      "\f",
      "\n",
      "Label:\n",
      "title16\n",
      "my TABe || dataset___text_ || bkg figure table section caption __list__para. || mean\n",
      "Model5S D none 84.6 83.3 79.4 58.3 61.0 66.7) 77.1 73.0\n",
      "Model5 || D extract | 83.9 83.7 79.7594 6LL_—«68.4._—-79.3:'|: 73.3.\n",
      "Model5 Ss none 87.7 83.1 84.3 70.8 70.9 82.3 83.1 79.6\n",
      "Models || S extract || 888 854 86.6 73.1 71.2 83.687. |) 82.2\n",
      "Model5 Ss real 91.2 90.3 89.0 78.4 75.3 87.5 89.6 86.0\n",
      "\f",
      "\n",
      "Label:\n",
      "figure17\n",
      "8\n",
      "Successfully created the directory ../tmp/images/CVPR2017/8_elements \n",
      "[2] Omnipage. https: //goo.g1/nDOEpC. 2,8\n",
      "\n",
      "[3] Wikipedia, bets: //dumps .wikimedia.org/. 5,6\n",
      "\n",
      "[4] A. Amin and R. Shiu, Page segmentation and classification\n",
      "utilizing bottom-up approach. International Journal of Im-\n",
      "age and Graphics, \\(02):345-361, 2001. 1,2\n",
      "\n",
      "[5] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra,\n",
      "C. Lawrence Zitnick, and D. Parikh. Vga: Visual question\n",
      "answering. In Proceedings of the IEEE International Con-\n",
      "ference on Computer Vision, pages 2425-2433, 2015. 3\n",
      "\n",
      "[6] A. Antonacopoulos, D. Bridson, C. Papadopoulos, and\n",
      "S. Pletschacher. A realistic dataset for performance evalua-\n",
      "tion of document layout analysis. In 2009 /0th International\n",
      "Conference on Document Analysis and Recognition, pages\n",
      "296-300. IEEE, 2009. 2, 5,6\n",
      "\n",
      "[7] A. Antonacopoulos, C. Clausner, C. Papadopoulos, and\n",
      "S. Pletschacher. Iedar2015 competition on recognition of\n",
      "documents with complex layouts-rdel2015, In Document\n",
      "Analysis and Recognition (ICDAR), 2015 13th International\n",
      "Conference on, pages 1151-1155. IEEE, 2015. 6\n",
      "\n",
      "[8] D. S. Bloomberg and L. Vincent. Document image applica-\n",
      "tions. Morphologie Mathmatique, 2007. 8\n",
      "\n",
      "[9] P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov. Enrich-\n",
      "ing word vectors with subword information. arXiv preprint\n",
      "arXiv:1607.04606, 2016. 6\n",
      "\n",
      "[10] S. S. Bukhari, F. Shafait, and T. M. Breuel. Improved\n",
      "document image segmentation algorithm using multiresolu-\n",
      "tion morphology. In IS&T/SPIE Electronic Imaging, pages\n",
      "78740D-78740D. International Society for Optics and Pho-\n",
      "tonics, 2011. 8\n",
      "\n",
      "[11] S. Changpinyo, W-L. Chao, B. Gong, and F. Sha. Syn-\n",
      "thesized classifiers for zero-shot learning. arXiv preprint\n",
      "arXiv:1603.00550, 2016. 3\n",
      "\n",
      "[12] K. Chen, M. Seuret, M. Liwicki, J. Hennebert, and R. In-\n",
      "gold. Page segmentation of historical document images\n",
      "with convolutional autoencoders. In Document Analysis and\n",
      "Recognition (ICDAR), 2015 13th International Conference\n",
      "on, pages 1011-1015. IEEE, 2015. 2\n",
      "\n",
      "[13] LC. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and\n",
      "A.L. Yuille. Semantic image segmentation with deep con-\n",
      "volutional nets and fully connected erfs. arXiv preprint\n",
      "arXiv:1412.7062, 2014. 3,7\n",
      "\n",
      "[14] A. Conway. Page grammars and page parsing. a syntactic ap-\n",
      "proach to document layout recognition, In Document Analy-\n",
      "siv and Recognition, 1993., Proceedings of the Second Inter-\n",
      "national Conference on, pages 761-764. IEEE, 1993. 2\n",
      "\n",
      "[15] C. Cortes and V. Vapnik. Support-veetor networks. Machine\n",
      "learning, 20(3):273-297, 1995. 2\n",
      "\n",
      "[16] J. Donahue, L. Anne Hendricks, S. Guadarrama,\n",
      "M. Rohrbach, S. Venugopalan, K. Saenko, and T. Dar-\n",
      "ell, Long-term recurrent convolutional networks for visual\n",
      "recognition and description. In Proceedings of the IEEE\n",
      "Conference on Computer Vision and Pattern Recognition,\n",
      "pages 2625-2634, 2015. 3\n",
      "\n",
      "[17] F.C, Feméndez and O. R. Terrades. Document segmenta-\n",
      "\f",
      "\n",
      "Label:\n",
      "list0\n",
      "(ICPR), 2012 21st International Conference on, pages 1562—\n",
      "1565. IEEE, 2012. 8\n",
      "\n",
      "[18] J.L. Fisher. Logical structure descriptions of segmented doc-\n",
      "ument images. Proceedings of International Con ference on\n",
      "Document Analysis and Recognition, pages 302-310, 1991.\n",
      "2\n",
      "\n",
      "[19] A. Frome, G. $. Corrado, J, Shlens, S. Bengio, J. Dean,\n",
      "TT. Mikoloy, et al. Devise: A deep visual-semantic embed-\n",
      "ding model. In Advances in neural information processing\n",
      "systems, pages 2121-2129, 2013. 3\n",
      "\n",
      "[20] H. Gao, J. Mao, J. Zhou, Z. Huang, L. Wang, and W. Xu.\n",
      "Are you talking to a machine? dataset and methods for mul-\n",
      "tilingual image question. In Advances in Neural Information\n",
      "Processing Systems, pages 2296-2304, 2015. 3\n",
      "\n",
      "[21] J. Ha, R. M. Haralick, and I. T. Phillips. Document page\n",
      "decomposition by the bounding-box project. In Document\n",
      "Analysis and Recognition, 1995., Proceedings of the Third\n",
      "International Conference on, volume 2, pages 1119-1122.\n",
      "TEBE, 1995. 1,2\n",
      "\n",
      "[22] J. Ha, R. M. Haralick, and 1. T. Phillips. Recursive xy cut\n",
      "using bounding boxes of connected components. In Doc-\n",
      "ument Analysis and Recognition, 1995., Proceedings of the\n",
      "Third International Conference on, volume 2, pages 952\n",
      "955, IEEE, 1995. 1, 2\n",
      "\n",
      "[23] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-\n",
      "ing for image recognition. arXiv preprint arXiv:1512.03385,\n",
      "2015. 3\n",
      "\n",
      "[24] R. Ingold and D, Armangil. A top-down document analysis,\n",
      "method for logical structure recognition. In Proceedings of\n",
      "International Conference on Document Analysis and Recog-\n",
      "nition, pages 41-49, 1991. 2\n",
      "\n",
      "[25] S. loffe and C. Szegedy. Batch normalization: Accelerating\n",
      "deep network training by reducing internal covariate shift.\n",
      "arXiv preprint arXiv:1502.03167, 2015. 5\n",
      "\n",
      "[26] M. Iyyer, V. Manjunatha, J. Boyd-Graber, and H. Daumé Il,\n",
      "Deep unordered composition rivals syntactic methods for\n",
      "text classification, In Proceedings of the Association for\n",
      "Computational Linguistics, 2015. 4\n",
      "\n",
      "[27] A. Joulin, E. Grave, P. Bojanowski, and T. Mikolov. Bag\n",
      "of tricks for efficient text classification. arXiv preprint\n",
      "arXiv:1607.01759, 2016. 4\n",
      "\n",
      "[28] A. Karpathy and L. Fei-Fei. Deep visual-semantic align-\n",
      "ments for generating image descriptions. In Proceedings\n",
      "of the IEEE Conference on Computer Vision and Pattern\n",
      "Recognition, pages 3128-3137, 2015. 3\n",
      "\n",
      "[29] M. Krishnamoorthy, G. Nagy, S. Seth, and M. Viswanathan.\n",
      "Syntactic segmentation and labeling of digitized pages trom\n",
      "technical journals. JEEE Transactions on Pattern Analysis\n",
      "and Machine Intelligence, 15(7):737-741, 1993. 2\n",
      "\n",
      "[30] F. Lebourgeois, Z. Bublinski, and H. Emptoz. A fast and\n",
      "efficient method for extracting text paragraphs and graph-\n",
      "ies from unconstrained documents. In Pattern Recognition,\n",
      "1992. Vol. Il. Conference B: Pattern Recognition Method-\n",
      "ology and Systems, Proceedings., Ith IAPR International\n",
      "Conference on, pages 272-276. IEEE, 1992. 1,2\n",
      "\n",
      "[31] J. Liang, R. Rogers, R. M. Haralick, and I. , Phillips. Uw-\n",
      "isl document image analysis toolbox: An experimental en-\n",
      "vironment. In Document Analysis and Recognition, 1997.,\n",
      "\f",
      "\n",
      "Label:\n",
      "list1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "References\n",
      "\f",
      "\n",
      "Label:\n",
      "title2\n",
      "9\n",
      "Successfully created the directory ../tmp/images/CVPR2017/9_elements \n",
      "Proceedings of ine Fourth internationat Conjerence on, vol-\n",
      "ume 2, pages 984-988. IEEE, 1997. 2\n",
      "\n",
      "[32] T-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-\n",
      "manan, P, Dollar, and C. L. Zitnick. Microsoft coco: Com-\n",
      "mon objects in context. In European Conference on Com-\n",
      "puter Vision, pages 740-755. Springer, 2014. 3, 5\n",
      "\n",
      "[33] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional\n",
      "networks for semantic segmentation, In Proceedings of the\n",
      "IEEE Conference on Computer Vision and Pattern Recogni-\n",
      "tion, pages 3431-3440, 2015. 3\n",
      "\n",
      "[34] Z. Lu, Z. Fu, T. Xiang, P. Han, L. Wang, and X. Gao. Learn-\n",
      "ing from weak and noisy labels for semantic segmentation,\n",
      "2016. 3\n",
      "\n",
      "[35] M.-T, Luong, T. D. Nguyen, and M.-Y. Kan, Logical struc-\n",
      "ture recovery in scholarly articles with rich document fea-\n",
      "tures, Multimedia Storage and Retrieval Innovations for Dig-\n",
      "ital Library Systems, 270, 2012. 2, 6,8\n",
      "\n",
      "[36] J. Mairal, J. Ponce, G. Sapiro, A. Zisserman, and F. R. Bach.\n",
      "Supervised dictionary learning. In Advances in neural infor-\n",
      "‘mation processing systems, pages 1033-1040, 2009. 3\n",
      "\n",
      "[37] M. Malinowski, M. Rohrbach, and M. Fritz, Ask your neu-\n",
      "rons: A neural-based approach to answering questions about\n",
      "images. In Proceedings of the IEEE International Confer-\n",
      "ence on Computer Vision, pages 1-9, 2015. 3\n",
      "\n",
      "[38] S. Mao, A. Rosenfeld, and T. Kanungo. Document structure\n",
      "analysis algorithms: a literature survey. In Electronic Imag-\n",
      "ing 2003, pages 197-207. International Society for Optics\n",
      "and Photonics, 2003. 2\n",
      "\n",
      "[39] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient\n",
      "estimation of word representations in vector space. arXiv\n",
      "preprint arXiv:1301.3781, 2013. 4\n",
      "\n",
      "[40] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and\n",
      "J. Dean, Distributed representations of words and phrases\n",
      "and their compositionality. In Advances in neural informa-\n",
      "tion processing systems, pages 3111-3119, 2013. 4\n",
      "\n",
      "[41] H. Noh, S. Hong, and B. Han, Learning deconvolution net-\n",
      "work for semantic segmentation. In Proceedings of the IEEE\n",
      "International Conference on Computer Vision, pages 1520-\n",
      "1528, 2015. 3\n",
      "\n",
      "[42] G. Papandreou, L.-C. Chen, K. Murphy, and A. L. Yuille.\n",
      "‘Weakly-and semi-supervised learning of a denn for seman-\n",
      "tic image segmentation. arXiv preprint arXiv: 1502.02734,\n",
      "2015. 3\n",
      "\n",
      "[43] P.O. Pinheiro, T-Y. Lin, R. Collobert, and P. Dolliér. Leam-\n",
      "ing to refine object segments. In Proceedings of the Euro-\n",
      "pean Conference on Computer Vision (ECV), 2016. 3\n",
      "\n",
      "[44] J. Sauvola and H. Kauniskangas. Mediateam document\n",
      "database ii, A CD-ROM collection of document images, Uni-\n",
      "versity of Oulu Finland, 1999. 2, 5\n",
      "\n",
      "[45] M. Shilman, P. Liang, and P, Viola. Learning nongenerative\n",
      "grammatical models for document analysis. In Tenth IEEE\n",
      "International Conference on Computer Vision (ICCV'05)\n",
      "Volume 1, volume 2, pages 962-969. IEE, 2005. 2\n",
      "\n",
      "[46] A. Simon, J.-C. Pret, and A. P. Johnson, A fast algorithm for\n",
      "bottom-up document layout analysis. IEEE Transactions on\n",
      "Pattern Analysis and Machine Intelligence, 19(3):273-277,\n",
      "\f",
      "\n",
      "Label:\n",
      "list0\n",
      "67\n",
      "\n",
      "48] R. Socher, M. Ganjoo, C.D. Manning, and A. Ng. Zero-shot\n",
      "earning through cross-modal transfer. In Advances in neural\n",
      "information processing systems, pages 935-943, 2013. 3\n",
      "\n",
      "49] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S, Reed,\n",
      "D. Angueloy, D. Erhan, V. Vanhoucke, and A. Rabinovich,\n",
      "Going deeper with convolutions. In Proceedings of the IEEE\n",
      "Conference on Computer Vision and Pattern Recognition,\n",
      "pages 1-9, 2015. 3\n",
      "\n",
      "50] L. Todoran, M. Worring, and A. W. Smeulders. The uva\n",
      "color document dataset. International Journal of Document\n",
      "Analysis and Recognition (1IDAR), 1(4):228-240, 2005. 2,\n",
      "5\n",
      "\n",
      "51] P. Vincent, H. Larochelle, Y. Bengio, and P-A. Manzagol.\n",
      "Extracting and composing robust features with denoising au-\n",
      "toencoders. In Proceedings of the 25th international confer-\n",
      "ence on Machine learning, pages 1096-1103. ACM, 2008.\n",
      "3\n",
      "\n",
      "52] Q.N. Vo and G. Lee. Dense prediction for text line segmen-\n",
      "tation in handwritten document images. In Image Process-\n",
      "ing (ICIP), 2016 IEEE International Conference on, pages\n",
      "3264-3268. IEF, 2016. 2\n",
      "\n",
      "53] Y. Wen, K. Zhang, Z. Li, and Y. Qiao. A discrimina-\n",
      "tive feature leaming approach for deep face recognition, In\n",
      "European Conference on Computer Vision, pages 499-515.\n",
      "Springer, 2016. 3\n",
      "\n",
      "$4] F. Yu and V. Koltun, Multi-scale context aggregation by di-\n",
      "lated convolutions, arXiv preprint arXiv:1511.07122, 2015.\n",
      "3\n",
      "\n",
      "55] M.D. Zeiler. Adadelta: an adaptive learning rate method,\n",
      "arXiv preprint arXiv:1212.5701, 2012. 5\n",
      "\n",
      "56] W. Zhang, S. Zeng, D. Wang, and X. Xue, Weakly super\n",
      "vised semantic segmentation for social images. In Proceed-\n",
      "ings of the IEEE Conference on Computer Vision and Pattern\n",
      "Recognition, pages 2718-2726, 2015. 3\n",
      "\n",
      "57] Y. Zhang, B. K. Lee, B. H. Lee, and U, EDU, Augmenting\n",
      "supervised neural networks with unsupervised objectives for\n",
      "large-scale image classification. 4\n",
      "\n",
      "58] J. Zhao, M. Mathieu, R. Goroshin, and Y. Lecun.\n",
      "Stacked what-where auto-encoders. arXiv preprint\n",
      "Se pene mane; aye 2 4\n",
      "\f",
      "\n",
      "Label:\n",
      "list1\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    argv = sys.argv[1:]\n",
    "    main(argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
