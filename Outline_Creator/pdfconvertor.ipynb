{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Requirement already satisfied: pdfminer in /Users/irfhanazakirhussain/opt/anaconda3/lib/python3.7/site-packages (20191125)\nRequirement already satisfied: pycryptodome in /Users/irfhanazakirhussain/opt/anaconda3/lib/python3.7/site-packages (from pdfminer) (3.9.8)\n"
    }
   ],
   "source": [
    "!pip install pdfminer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfminer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#converts pdf into xml file\n",
    "!pdf2txt.py -o output.xml CVPR2017.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make the xml file indented\n",
    "from xml.dom.minidom import parse\n",
    "dom1 = parse('output.xml')\n",
    "xmlstr = dom1.toprettyxml()\n",
    "with open('output.xml', 'bw') as fp:\n",
    "    fp.write(xmlstr.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "LearningtoExtractSemanticStructurefromDocumentsUsingMultimodalFullyConvolutionalNeuralNetworksXiaoYang‡,ErsinYumer†,PaulAsente†,MikeKraley†,DanielKifer‡,C.LeeGiles‡‡ThePennsylvaniaStateUniversity†AdobeResearchxuy111@psu.edu{yumer,asente,mkraley}@adobe.comdkifer@cse.psu.edugiles@ist.psu.eduAbstractWepresentanend-to-end,multimodal,fullyconvolu-tionalnetworkforextractingsemanticstructuresfromdoc-umentimages.Weconsiderdocumentsemanticstructureextractionasapixel-wisesegmentationtask,andproposeauniﬁedmodelthatclassiﬁespixelsbasednotonlyontheirvisualappearance,asinthetraditionalpagesegmentationtask,butalsoonthecontentofunderlyingtext.Moreover,weproposeanefﬁcientsyntheticdocumentgenerationpro-cessthatweusetogeneratepretrainingdataforournet-work.Oncethenetworkistrainedonalargesetofsyntheticdocuments,weﬁne-tunethenetworkonunlabeledrealdoc-umentsusingasemi-supervisedapproach.Wesystemati-callystudytheoptimumnetworkarchitectureandshowthatbothourmultimodalapproachandthesyntheticdatapre-trainingsigniﬁcantlyboosttheperformance.1.IntroductionDocumentsemanticstructureextraction(DSSE)isanactively-researchedareadedicatedtounderstandingimagesofdocuments.Thegoalistosplitadocumentimageintore-gionsofinterestandtorecognizetheroleofeachregion.Itisusuallydoneintwosteps:theﬁrststep,oftenreferredtoaspagesegmentation,isappearance-basedandattemptstodistinguishtextregionsfromregionslikeﬁgures,tablesandlinesegments.Thesecondstep,oftenreferredtoaslogicalstructureanalysis,issemantics-basedandcategorizeseachregionintosemantically-relevantclasseslikeparagraphandcaption.Inthiswork,weproposeauniﬁedmultimodalfullycon-volutionalnetwork(MFCN)thatsimultaneouslyidentiﬁesbothappearance-basedandsemantics-basedclasses.Itisageneralizedpagesegmentationmodelthatadditionallyper-formsﬁne-grainedrecognitionontextregions:textregionsareassignedspeciﬁclabelsbasedontheirsemanticfunc-tionalityinthedocument.OurapproachsimpliﬁesDSSEandbettersupportsdocumentimageunderstanding.WeconsiderDSSEasapixel-wisesegmentationprob-lem:eachpixelislabeledasbackground,ﬁgure,table,Figure1:(a)Examplesthataredifﬁculttoidentifyifonlybasedontext.Thesamenamecanbeatitle,anauthororaﬁgurecaption.(b)Examplesthataredifﬁculttoidentifyifonlybasedonvisualappearance.Textinthelargefontmightbemislabeledasasectionheading.Textwithdashesmightbemislabeledasalist.paragraph,sectionheading,list,caption,etc.WeshowthatourMFCNmodeltrainedinanend-to-end,pixels-to-pixelsmannerondocumentimagesexceedsthestate-of-the-artsigniﬁcantly.Iteliminatestheneedtodesigncom-plexheuristicrulesandextracthand-craftedfeatures[30,22,21,46,4].Inmanycases,regionslikesectionheadingsorcaptionscanbevisuallyidentiﬁed.InFig.1(a),onecaneasilyrec-ognizethedifferentrolesofthesamename.However,arobustDSSEsystemneedsthesemanticinformationofthetexttodisambiguatepossiblefalseidentiﬁcations.Forex-ample,inFig.1(b),thetextinthelargefontmightlooklikesectionheading,butitdoesnotfunctionthatway;thelinesbeginningwithdashesmightbemislabeledasalist.Tothisend,ourmultimodalfullyconvolutionalnetworkisdesignedtoleveragethetextualinformationinthedocu-mentaswell.ToincorporatetextualinformationinaCNN-basedarchitecture,webuildatextembeddingmapandfeedittoourMFCN.Morespeciﬁcally,weembedeachsentenceandmaptheembeddingtothecorrespondingpixelswherethesentenceisrepresentedinthedocument.Fig.2summa-rizesthearchitectureoftheproposedMFCNmodel.Our15315Figure2:Thearchitectureoftheproposedmultimodalfullyconvolutionalneuralnetwork.Itconsistsoffourparts:anencoderthatlearnsahierarchyoffeaturerepresentations,adecoderthatoutputssegmentationmasks,anauxiliarydecoderforunsupervisedreconstruction,andabridgethatmergesvisualrepresentationsandtextualrepresentations.Theauxiliarydecoderonlyexistsduringtraining.modelconsistsoffourparts:anencoderthatlearnsahier-archyoffeaturerepresentations,adecoderthatoutputsseg-mentationmasks,anauxiliarydecoderforreconstructionduringtraining,andabridgethatmergesvisualrepresenta-tionsandtextualrepresentations.Weassumethatthedocu-menttexthasbeenpre-extracted.FordocumentimagesthiscanbedonewithmodernOCRengines[47,1,2].Oneofthebottlenecksintrainingfullyconvolutionalnetworksistheneedforpixel-wisegroundtruthdata.Pre-viousdocumentunderstandingdatasets[31,44,50,6]arelimitedbyboththeirsmallsizeandthelackofﬁne-grainedsemanticlabelssuchassectionheadings,lists,orﬁgureandtablecaptions.Toaddresstheseissues,weproposeanef-ﬁcientsyntheticdocumentgenerationprocessanduseittogeneratelarge-scalepretrainingdataforournetwork.Fur-thermore,weproposetwounsupervisedtasksforbettergen-eralizationtorealdocuments:reconstructionandconsis-tencytasks.Theformerenablesbetterrepresentationlearn-ingbyreconstructingtheinputimage,whereasthelatteren-couragespixelsbelongingtothesameregionshavesimilarrepresentation.Ourmaincontributionsaresummarizedasfollows:•Weproposeanend-to-end,uniﬁednetworktoaddressdocumentsemanticstructureextraction.Unlikepre-vioustwo-stepprocesses,wesimultaneouslyidentifybothappearance-basedandsemantics-basedclasses.•Ournetworksupportsbothsupervisedtrainingonim-ageandtextofdocuments,aswellasunsupervisedauxiliarytrainingforbetterrepresentationlearning.•Weproposeasyntheticdatagenerationprocessanduseittosynthesizealarge-scaledatasetfortrainingthesupervisedpartofourdeepMFCNmodel.2.BackgroundPageSegmentation.Mostearlierworksonpageseg-mentation[30,22,21,46,4,45]fallintotwocate-gories:bottom-upandtop-downapproaches.Bottom-upapproaches[30,46,4]ﬁrstdetectwordsbasedonlocalfea-tures(white/blackpixelsorconnectedcomponents),thensequentiallygroupwordsintotextlinesandparagraphs.However,suchapproachessufferfromtheidentiﬁcationandgroupingofconnectedcomponentsbeingtime-consuming.Top-downapproaches[22,21]iterativelysplitapageintocolumns,blocks,textlinesandwords.Withbothoftheseapproachesitisdifﬁculttocorrectlysegmentdocumentswithcomplexlayout,forexampleadocumentwithnon-rectangularﬁgures[38].Withrecentadvancesindeepconvolutionalneuralnet-works,severalneural-basedmodelshavebeenproposed.Chenetal.[12]appliedaconvolutionalauto-encodertolearnfeaturesfromcroppeddocumentimagepatches,thenusethesefeaturestotrainaSVM[15]classiﬁer.Voetal.[52]proposedusingFCNtodetectlinesinhandwrittendocumentimages.However,thesemethodsarestrictlyre-strictedtovisualcues,andthusarenotabletodiscoverthesemanticmeaningoftheunderlyingtext.LogicalStructureAnalysis.Logicalstructureisde-ﬁnedasahierarchyoflogicalcomponentsindocuments,suchassectionheadings,paragraphsandlists[38].Earlyworkinlogicalstructurediscovery[18,29,24,14]focusedonusingasetofheuristicrulesbasedonthelocation,fontandtextofeachsentence.Shilmanetal.[45]modeleddoc-umentlayoutasagrammarandusedmachinelearningtominimizethecostofainvalidparsing.Luongetal.[35]proposedusingaconditionalrandomﬁeldsmodeltojointly5316labeleachsentencebasedonseveralhand-craftedfeatures.However,theperformanceofthesemethodsislimitedbytheirrelianceonhand-craftedfeatures,whichcannotcap-turethehighlysemanticcontext.SemanticSegmentation.Large-scaleannotations[32]andthedevelopmentofdeepneuralnetworkapproachessuchasthefullyconvolutionalnetwork(FCN)[33]haveledtorapidimprovementoftheaccuracyofsemanticsegmen-tation[13,42,41,54].However,theoriginallyproposedFCNmodelhasseverallimitations,suchasignoringsmallobjectsandmislabelinglargeobjectsduetotheﬁxedrecep-tiveﬁeldsize.Toaddressthisissue,Nohetal.[41]pro-posedusingunpooling,atechniquethatreusesthepooled“location”attheup-samplingstage.Pinheiroetal.[43]attemptedtouseskipconnectionstoreﬁnesegmentationboundaries.Ourmodeladdressesthisissuebyusingadi-latedblock,inspiredbydilatedconvolutions[54]andrecentwork[49,23]thatgroupsseverallayerstogether.Wefur-therinvestigatetheeffectivenessofdifferentapproachestooptimizeournetworkarchitecture.Collectingpixel-wiseannotationsforthousandsormil-lionsofimagesrequiresmassivelaborandcost.Tothisend,severalmethods[42,56,34]havebeenproposedtoharnessweakannotations(bounding-boxlevelorimagelevelanno-tations)inneuralnetworktraining.Ourconsistencylossre-liesonsimilarintuitionbutdoesnotrequirea“classlabel”foreachboundingbox.UnsupervisedLearning.Severalmethodshavebeenproposedtouseunsupervisedlearningtoimprovesuper-visedlearningtasks.Mairaletal.[36]proposedasparsecodingmethodthatlearnssparselocalfeaturesbysparsity-constrainedreconstructionlossfunctions.Zhaoetal.[58]proposedaStackedWhat-WhereAuto-Encoderthatusesunpoolingduringreconstruction.Byinjectingnoiseintotheinputandthemiddlefeatures,adenoisingauto-encoder[51]canlearnrobustﬁltersthatrecoveruncorruptedinput.Themainfocusinunsupervisedlearninghasbeenimage-levelclassiﬁcationandgenerativeapproaches,whereasinthispa-perweexplorethepotentialofsuchmethodsforpixel-wisesemanticsegmentation.Wenetal.[53]recentlyproposedacenterlossthaten-couragesdatasampleswiththesamelabeltohaveasimilarvisualrepresentation.Similarly,weintroduceanintra-classconsistencyconstraint.However,the“center”foreachclassintheirlossisdeterminedbydatasamplesacrossthewholedataset,whileinourcasethe“center”islocallydeterminedbypixelswithinthesameregionineachimage.LanguageandVision.Severaljointlearningtaskssuchasimagecaptioning[16,28],visualquestionanswer-ing[5,20,37],andone-shotlearning[19,48,11]havedemonstratedthesigniﬁcantimpactofusingtextualandvisualrepresentationsinajointframework.Ourworkisuniqueinthatweusetextualembeddingdirectlyforaseg-mentationtaskfortheﬁrsttime,andweshowthatourap-proachimprovestheresultsoftraditionalsegmentationap-proachesthatonlyusevisualcues.3.MethodOurmethoddoessupervisedtrainingforpixel-wiseseg-mentationwithaspecializedmultimodalfullyconvolu-tionalnetworkthatusesatextembeddingmapjointlywiththevisualcues.Moreover,ourMFCNarchitecturealsosupportstwounsupervisedlearningtaskstoimprovethelearneddocumentrepresentation:areconstructiontaskbasedonanauxiliarydecoderandaconsistencytaskeval-uatedinthemaindecoderbranchalongwiththeper-pixelsegmentationloss.3.1.MultimodalFullyConvolutionalNetworkAsshowninFig.2,ourMFCNmodelhasfourparts:anencoder,twodecodersandabridge.TheencoderanddecoderpartsroughlyfollowthearchitectureguidelinessetforthbyNohetal.[41].However,severalchangeshavebeenmadetobetteraddressdocumentsegmentation.First,weobservethatseveralsemantic-basedclassessuchassectionheadingandcaptionusuallyoccupyrela-tivelysmallareas.Moreover,correctlyidentifyingcertainregionsoftenreliesonsmallvisualcues,likelistsbeingidentiﬁedbysmallbulletsornumbersinfrontofeachitem.Thissuggeststhatlow-levelfeaturesneedtobeused.How-ever,becausemax-poolingnaturallylosesinformationdur-ingdownsampling,FCNoftenperformspoorlyforsmallobjects.Longetal.[33]attempttoavoidthisproblemus-ingskipconnections.However,simplyaveragingindepen-dentpredictionsbasedonfeaturesatdifferentscalesdoesnotprovideasatisfyingsolution.Low-levelrepresentations,limitedbythelocalreceptiveﬁeld,arenotawareofobject-levelsemanticinformation;ontheotherhand,high-levelfeaturesarenotnecessarilyalignedconsistentlywithobjectboundariesbecauseCNNmodelsareinvarianttotransla-tion.Weproposeanalternativeskipconnectionimplemen-tation,illustratedbythebluearrowsinFig.2,similartothatusedintheindependentworkSharpMask[43].However,theyusebilinearupsamplingafterskipconnectionwhileweuseunpoolingtopreservemorespatialinformation.Wealsonoticethatbroadercontextinformationisneededtoidentifycertainobjects.Foraninstance,itisoftendifﬁculttotellthedifferencebetweenalistandsev-eralparagraphsbyonlylookingatpartsofthem.InFig.3,tocorrectlysegmenttherightpartofthelist,thereceptiveﬁeldsmustbelargeenoughtocapturethebulletsontheleft.InspiredbytheInceptionarchitecture[49]anddilatedconvolution[54],weproposeadilatedconvolutionblock,whichisillustratedinFig.4(left).Eachdilatedconvolu-tionblockconsistsof5dilatedconvolutionswitha3×3kernelsizeandadilationd=1,2,4,8,16.5317Figure3:Acroppeddocumentimageanditssegmentationmaskgeneratedbyourmodel.Notethatthetop-rightcornerofthelistisyellowinsteadofcyan,indicatingthatithasbeenmislabeledasaparagraph.3.2.TextEmbeddingMapTraditionalimagesemanticsegmentationmodelslearnthesemanticmeaningsofobjectsfromavisualperspective.Ourtask,however,alsorequiresunderstandingthetextinimagesfromalinguisticperspective.Therefore,webuildatextembeddingmapandfeedittoourmultimodalmodeltomakeuseofbothvisualandtextualrepresentations.Wetreatasentenceastheminimumunitthatconveyscertainsemanticmeanings,andrepresentitusingalow-dimensionalvector.Oursentenceembeddingisbuiltbyaveragingembeddingsforindividualwords.Thisisasim-pleyeteffectivemethodthathasbeenshowntobeusefulinmanyapplications,includingsentimentanalysis[26]andtextclassiﬁcation[27].Usingsuchembeddings,wecre-ateatextembeddingmapasfollows:foreachpixelinsidetheareaofasentence,weusethecorrespondingsentenceembeddingastheinput.Pixelsthatbelongtothesamesen-tencethussharethesameembedding.Pixelsthatdonotbelongtoanysentenceswillbeﬁlledwithzerovectors.ForadocumentimageofsizeH×W,thisprocessresultsinanembeddingmapofsizeN×H×Wifthelearnedsen-tenceembeddingsareN-dimensionalvectors.Theembed-dingmapislaterconcatenatedwithafeatureresponsealongthenumber-of-channeldimensions(seeFig.2).Speciﬁcally,ourwordembeddingislearnedusingtheskip-grammodel[39,40].Fig.4(right)showsthebasicdiagram.LetVbethenumberofwordsinavocabularyandwbeaV-dimensionalone-hotvectorrepresentingaword.ThetrainingobjectiveistoﬁndaN-dimensional(N≪V)vectorrepresentationforeachwordthatisusefulforpredictingtheneighboringwords.Moreformally,givenasequenceofwords[w1,w2,···,wT],wemaximizetheaveragelogprobabilityT1TXt=1X−C≤j≤C,j6=0logP(wt+j|wt)(1)whereTisthelengthofthesequenceandCisthesizeofthecontextwindow.TheprobabilityofoutputtingawordFigure4:Left:Adilatedblockthatcontains5dilatedlayerswithdifferentdilationd.Batch-convolutionalNormalizationandnon-linearityarenotshownforbrevity.Right:Theskip-grammodelforwordembeddings.wogivenaninputwordwiisdeﬁnedusingsoftmax:P(wo|wi)=P′′⊤woexp(vVw=1exp(v′vwi)⊤vwi)w(2)wherevwandvdimensionalvectorrepresentationsofw.warethe“input”and“output”N-3.3.UnsupervisedTasksAlthoughoursyntheticdocuments(Sec.4)providealargeamountoflabeleddatafortraining,theyarelimitedinthevariationsoftheirlayouts.Tothisend,wedeﬁnetwounsupervisedlossfunctionstomakeuseofrealdocumentsandtoencouragebetterrepresentationlearning.ReconstructionTask.Ithasbeenshownthatrecon-structioncanhelplearningbetterrepresentationsandthere-foreimprovesperformanceforsupervisedtasks[58,57].Wethusintroduceaseconddecoderpathway(Fig.2-axil-larydecoder),denotedasDrec,anddeﬁneareconstructionlossatintermediatefeatures.Thisauxiliarydecoderonlyexistsduringthetrainingphase.Letal,l=1,2,···Lbetheactivationsofthelthlayeroftheencoder,anda0betheinputimage.Forafeed-forwardconvolutionalnetwork,alisafeaturemapofsizeCl×Hl×Wl.OurauxiliarydecoderDrecattemptstoreconstructahierarchyoffeaturemaps{˜al}.ReconstructionlossL(l)recforaspeciﬁclisthereforedeﬁnedasL(l)rec=1ClHlWlkal−˜alk22,l=0,1,2,···L(3)ConsistencyTask.Pixel-wiseannotationsarelabor-intensivetoobtain,howeveritisrelativelyeasytogetasetofboundingboxesfordetectedobjectsinadocument.FordocumentsinPDFformat,onecanﬁndboundingboxesbyanalyzingtherenderingcommandsinthePDFﬁles(Seeoursupplementarydocumentfortypicalexamples).Eveniftheirlabelsremainunknown,theseboundingboxesarestillbeneﬁcial:theyprovideknowledgeofwhichpartsofadocumentbelongstothesameobjectsandthusshouldnotbesegmentedintodifferentfragments.5318Bybuildingontheintuitionthatregionsbelongingtosameobjectsshouldhavesimilarfeaturerepresentations,wedeﬁnetheconsistencytasklossLconsasfollows.Letp(i,j)(i=1,2,···H,j=1,2,···W)beactivationsatlo-cation(i,j)inafeaturemapofsizeC×H×W,andbbetherectangularareainaboundingbox.Leteachrectangu-larareabisofsizeHb×Wb.Then,foreachb∈B,LconswillbegivenbyLcons=HbWbX(i,j)∈b(cid:13)(cid:13)(cid:13)p(i,j)−p(b)22(cid:13)(cid:13)(cid:13)11p(b)=HbWbX(i,j)∈bp(i,j)MinimizingconsistencylossLconsencouragesintra-regionconsistency.TheconsistencylossLconsisdifferentiableandcanbeoptimizedusingstochasticgradientdescent.ThegradientofLconswithrespecttop(i,j)is(p(i,j)−p(b))(HbWb−1)+∂Lcons∂p(i,j)=2bW2H2b2bW2H2(p(b)−p(u,v))bX(u,v)∈b(u,v)6=(i,j)sinceHbWb≫1,forefﬁciencyitcanbeapproximatedby:∂Lcons∂p(i,j)2≈HbWb(cid:16)p(i,j)−p(b)(cid:17).Weusetheunsupervisedconsistencyloss,Lcons,asalosslayer,thatisevaluatedatthemaindecoderbranch(bluebranchinFig.2)alongwithsupervisedsegmentationloss.(4)(5)(6)(7)triple-columnPDFs.Candidateﬁguresincludeacademic-styleﬁguresandgraphicdrawingsdownloadedusingwebimagesearch,andnaturalimagesfromMSCOCO[32],whichassociateseachimagewithseveralcaptions.Candi-datetablesaredownloadedusingwebimagesearch.Var-iousqueriesareusedtoincreasethediversityofdown-loadedtables.SinceourMFCNmodelreliesontheseman-ticmeaningoftexttomakeprediction,thecontentoftextregions(paragraph,sectionheading,list,caption)mustbecarefullyselected:•Forparagraphs,werandomlysamplesentencesfroma2016EnglishWikipediadump[3].•Forsectionheadings,weonlysamplesentencesandphrasesthataresectionorsubsectionheadingsinthe“Contents”blockinaWikipediapage.•Forlists,weensurethatallitemsinalistcomefromthesameWikipediapage.•Forcaptions,weeitherusetheassociatedcaption(forimagesfromMSCOCO)orthetitleoftheimageinwebimagesearch,whichcanbefoundinthespanwithclassname“ircpt”.Tofurtherincreasethecomplexityofthegenerateddocu-mentlayouts,wecollectedandlabeled271documentswithvaried,complicatedlayouts.Wethenrandomlyreplacedeachelementwithastandaloneparagraph,ﬁgure,table,caption,sectionheadingorlistgeneratedasstatedabove.Intotal,oursyntheticdatasetcontains135,000documentimages.ExamplesofoursyntheticdocumentsareshowninFig.5.Pleaserefertooursupplementarydocumentformoreexamplesofsyntheticdocumentsandindividualele-mentsusedinthegenerationprocess.4.SyntheticDocumentData5.ImplementationDetailsSinceourMFCNaimstogenerateasegmentationmaskofthewholedocumentimage,pixel-wiseannotationsarerequiredforthesupervisedtask.Whilethereareseveralpubliclyavailabledatasetsforpagesegmentation[44,50,6],thereareonlyafewhundredtoafewthousandpagesineach.Furthermore,thetypesoflabelsarelimited,forexampletotext,ﬁgureandtable,howeverourgoalistoperformamuchmoregranularsegmentation.Toaddresstheseissues,wecreatedasyntheticdataen-gine,capableofgeneratinglarge-scale,pixel-wiseanno-tateddocuments.Oursyntheticdocumentengineusestwomethodstogen-eratedocuments.Theﬁrstproducescompletelyautomatedandrandomlayoutofpartialdatascrapedfromtheweb.Morespeciﬁcally,wegenerateLaTeXsourceﬁlesinwhichparagraphs,ﬁgures,tables,captions,sectionheadingsandlistsarerandomlyarrangedtomakeupsingle,double,orFig.2summarizesthearchitectureofourmodel.Theauxiliarydecoderonlyexistsinthetrainingphase.Allcon-volutionallayershavea3×3kernelsizeandastrideof1.Thepooling(intheencoders)andunpooling(inthede-coders)haveakernelsizeof2×2.Weadoptbatchnormal-ization[25]immediatelyaftereachconvolutionandbeforeallnon-linearfunctions.Weperformper-channelmeansubtractionandresizeeachinputimagesothatitslongersideislessthan384pixels.Nootherpre-processingisapplied.WeuseAdadelta[55]withamini-batchsizeof2.Duringsemi-supervisedtraining,mini-batchesofsyntheticandrealdocumentsareusedalternatively.Forsyntheticdocu-ments,bothper-pixelclassiﬁcationlossandtheunsuper-visedlossesareactiveatback-propagation,whileforrealdocuments,onlytheunsupervisedlossesareactive.Sincetheareaofparagraphsisthelabelsareunbalanced(e.g.5319Figure5:Examplesyntheticdocuments,rawsegmentationsandresultsafteroptionalpost-processing(Sec.5).Segmentationlabelcolorsare:ﬁgure,table,sectionheading,caption,listandparagraph.muchlargerthanthatofcaption),classweightsfortheper-pixelclassiﬁcationlossaresetdifferentlyaccordingtothetotalnumberofpixelsineachclassinthetrainingset.Fortextembedding,werepresenteachwordasa128-dimensionalvectorandtrainaskip-grammodelonthe2016EnglishWikipediadump[3].Embeddingsforout-of-dictionarywordsareobtainedfollowingBojanowskietal.[9].WeuseTesseract[47]asourOCRengine.Post-processing.Weapplyanoptionalpost-processingstepasacleanupstrategyforsegmentmasks.Fordocu-mentsinPDFformat,weobtainasetofcandidateboundingboxesbyanalyzingthePDFformattoﬁndelementboxes.Wethenreﬁnethesegmentationmasksbyﬁrstcalculat-ingtheaverageclassprobabilityforpixelsbelongingtothesamebox,followedbyassigningthemostlikelylabeltothesepixels.6.ExperimentsWeusedthreedatasetsforevaluations:ICDAR2015[6],SectLabel[35]andournewdatasetnamedDSSE-200.ICDAR2015[6]isadatasetusedinthebiennialIC-DARpagesegmentationcompetitions[7]focusingmoreonappearance-basedregions.TheevaluationsetofIC-DAR2015consistsof70sampledpagesfromcontemporarymagazinesandtechnicalarticles.SectLabel[35]consistsof40academicpaperswith347pagesintheﬁeldofcom-puterscience.Eachtextlineinthesepapersismanuallyassignedasemantics-basedlabelsuchastext,sectionhead-ingorlistitem.Inadditiontothesetwodatasets,wein-troduceDSSE-2001,whichprovidesbothappearance-basedandsemantics-basedlabels.DSSE-200contains200pagesfrommagazinesandacademicpapers.Regionsinapageareassignedlabelsfromthefollowingdictionary:ﬁgure,table,section,caption,listandparagraph.NotethatDSSE-200hasamoregranularsegmentationthanpreviouslyreleasedbenchmarkdatasets.Theperformanceismeasuredintermsofpixel-wise1http://personal.psu.edu/xuy111/projects/cvpr2017_doc.html.intersection-over-union(IoU),whichisstandardinseman-ticsegmentationtasks.WeoptimizethearchitectureofourMFCNmodelbasedontheDSSE-200datasetsinceitcontainsbothappearance-basedandsemantics-basedla-bels.Sec.6.4comparesourresultstostate-of-the-artmeth-odsontheICDAR2015andSectLabeldatasets.6.1.AblationExperimentonModelArchitectureWeﬁrstsystematicallyevaluatetheeffectivenessofdif-ferentnetworkarchitectures.ResultsareshowninTable1.Notethattheseresultsdonotincorporatetextualinforma-tionorunsupervisedlearningtasks.Thepurposeofthisexperimentistoﬁndthebest“base”architecturetobeusedinthefollowingexperiments.AllmodelsaretrainedfromscratchandevaluatedontheDSSE-200dataset.Asasimplebaseline(Table1Model1),wetrainaplainencoder-decoderstylemodelfordocumentsegmentation.Itconsistsofafeed-forwardconvolutionalnetworkasanencoder,andadecoderimplementedbyafullyconvolu-tionalnetwork.Upsamplingisdonebybilinearinterpola-tion.ThismodelachievesameanIoUof61.4%.Next,weaddskipconnectionstothemodel,resultinginModel2.NotethatthismodelissimilartotheSharpMaskmodel.WeobserveameanIoUof65.4%,4%betterthanthebasemodel.Theimprovementsareevenmoresigniﬁ-cantforsmallobjectslikecaptions.Wefurtherevaluatetheeffectivenessofreplacingbilin-earupsamplingwithunpooling,givingModel3.Allup-samplinglayersinModel2arereplacedbyunpoolingwhileotherpartsarekeptunchanged.Doingsoresultsinasignif-icantimprovementformeanIoU(65.4%vs.71.2%).Thissuggeststhatthepooledindexshouldnotbediscardeddur-ingdecoding.Theseindexesarehelpfultodisambiguatethelocationinformationwhenconstructingthesegmenta-tionmaskinthedecoder.Finally,weinvestigatetheuseofdilatedconvolutions.Model3isequivalenttousingdilatedconvolutionwhend=1.Model4setsd=8whileModel5usesthedi-latedblockillustratedinFig.4(left).Thenumberofoutputchannelsareadjustedsuchthatthetotalnumberofparame-5320Figure6:Examplerealdocumentsandtheircorrespondingsegmentation.Top:DSSE-200.Middle:ICDAR2015.Bottom:SectLabel.SincethesedocumentsarenotinPDFformat,thesimplepost-processinginSec.5cannotbeapplied.OnemayconsiderexploitingaCRF[13]toreﬁnethesegmentation,butthatisbeyondthemainfocusofthispaper.Segmentationlabelcolorsare:ﬁgure,table,sectionheading,caption,listandparagraph.Model#12345dilation1118blockupsamplingbilinearbilinearunpoolingunpoolingunpoolingskipnoyesyesyesyesbkg80.382.184.183.984.6ﬁgure75.476.781.274.983.3table62.774.477.669.779.4section50.051.854.657.258.3caption33.842.460.360.261.0list57.358.765.964.666.7paragraphmean61.465.471.269.573.070.474.474.876.177.1Table1:AblationexperimentsonDSSE-200dataset.Thearchitectureofeachmodelischaracterizedbythedilationinconvolutionlayers,thewayofupsamplingandtheuseofskipconnection.IoUscores(%)arereported.tersaresimilar.Comparingtheresultsforthesethreemod-els,wecanseethattheIoUofModel4foreachclassisonparwithorworsethanModel3,whileModel5isbetterthanbothModel3andModel4forallclasses.6.2.AddingTextualInformationWenowinvestigatetheimportanceoftextualinforma-tioninourmultimodalmodel.Wetakethebestarchitec-ture,Model5,asourvision-onlymodel,andincorporateatextembeddingmapviaabridgemoduledepictedinFig.2.Thiscombinedmodelisﬁne-tunedonoursyntheticdocu-ments.AsshowninTable2,usingtextaswellimprovestheperformancefortextualclasses.Theaccuracyforsec-tionheading,caption,listandparagraphisboostedby1.1%,0.1%,1.7%and2.2%,respectively.WerelyonexistingOCRengines[47]toextracttext,buttheyarenotalwaysreliableforscanneddocumentsoflowquality.Toquantitativelyanalyzetheeffectsofusingex-tractedtext,wecomparetheperformanceofusingextractedtextversusrealtext.Thecomparisonisconductedonasub-setofoursyntheticdataset(200images),sinceground-truthtextisnaturallyavailable.AsshowninTable2,usingrealtextleadstoaremarkableimprovement(6.4%)formeanIoU,suggestingtheeffectivenessofincorporatingtextualinformation.UsingOCRextractedtextisnotaseffective,butstillresultsin2.6%improvement.Itisbetterthanthe0.3%improvementonDSSE-200dataset;weattributethistooursyntheticdatanotbeingascomplicatedasDSSE-200,soextractingtextbecomeseasier.5321baseModel5Model5Model5Model5Model5datasetDDSSStextnoneextractnoneextractrealbkg84.683.987.788.891.2ﬁgure83.383.783.185.490.3table79.479.784.386.689.0section58.359.470.873.178.4caption61.061.170.971.275.3list66.768.482.383.687.5para.mean73.077.173.379.379.683.182.287.286.089.6Table2:IoUscores(%)ontheDSSE-200(D)andsyntheticdataset(S)usingtextembeddingmap.Onsyntheticdataset,wefurtherinvestigatetheeffectsofusingextractedtextversusrealtextwhenbuildingthetextembeddingmap.LclsLrecLconsLrec+con73.373.975.475.9meanTable3:IoUscores(%)whenusingdifferenttrainingob-jectivesonDSSE-200dataset.cls:pixel-wiseclassiﬁcationtask,rec:reconstructiontaskandcons:consistencytask.non-text84.790.694.5ﬁgure70.177.1MethodsLeptonica[8]Bukharietal.[10]Ours(binary)MethodsFernandezetal.[17]Ours(binary)Table4:IoUscores(%)forpagesegmentationontheICDAR2015dataset.Forcomparisonpurpose,onlyIoUscoresfornon-text,textandﬁgureareshown.Howeverourmodelcanmakeﬁne-grainedpredictionsaswell.text86.890.391.0text85.891.0MethodsLuongetal.[35]Ourssection0.9160.919caption0.7810.893list0.7120.793para.0.9690.969Table5:F1scoresontheSectLabeldataset.Notethatourmodelcanalsoidentifynon-textclassessuchasﬁguresandtables.6.3.UnsupervisedLearningTasksHere,weexaminehowtheproposedtwounsupervisedlearningtasks—reconstructionandconsistencytasks—cancomplementthepixel-wiseclassiﬁcationduringtrain-ing.WetakethebestmodelinSec.6.2,andonlychangethetrainingobjectives.Ourmodelisthenﬁne-tunedinasemi-supervisedmannerasdescribedinSec.5.TheresultsareshowninTable3.AddingthereconstructiontaskslightlyimprovesthemeanIoUby0.6%,whileaddingtheconsis-tencytaskleadstoaboostof1.9%.Theseresultsjustifyourhypothesisthatharnessingregioninformationisbeneﬁcial.CombiningbothtasksresultsinameanIoUof75.9%.6.4.ComparisonswithPriorArtTable4and5presentcomparisonswithseveralmeth-odsthathavepreviouslyreportedperformanceontheIC-Itisworthemphasiz-DAR2015andSectLabeldatasets.ingthatourMFCNmodelsimultaneouslypredictsbothappearance-basedandsemantics-basedclasseswhileothermethodscannot.ComparisonsonICDAR2015dataset(Table4).Pre-viouspixel-wisepagesegmentationmodelsusuallysolveabinarysegmentationproblemanddonotmakepredictionsforﬁne-grainedclasses.Forfaircomparison,wechangethenumberofoutputchannelsofthelastlayerto3(back-ground,ﬁgureandtext)andﬁne-tunethislastlayer.Ourbi-naryMFCNmodelachieves94.5%,91.0%and77.1%IoUscoresfornon-text(backgroundandﬁgure),textandﬁgureregions,outperformingothermodels.ComparisonsonSectLabeldataset(Table5).Luongetat.[35]ﬁrstuseOmnipage[2]tolocalizeandrecognizetextlines,thenpredictthesemantics-basedlabelforeachline.TheF1scoreforeachclasswasreported.Forfaircompar-ison,weusethesamesetoftextlineboundingboxes,andusetheaveragedpixel-wisepredictionasthelabelforeachtextline.OurmodelachievesbetterF1scoresforsectionheading(0.919VS0.916),caption(0.893VS0.781)andlist(0.793VS0.712),whilebeingcapableofidentifyingﬁguresandtables.7.ConclusionWeproposedamultimodalfullyconvolutionalnetwork(MFCN)fordocumentsemanticstructureextraction.Theproposedmodelusesbothvisualandtextualinformation.Moreover,weproposeanefﬁcientsyntheticdatagenerationmethodthatyieldsper-pixelground-truth.Ourunsuper-visedauxiliarytaskshelpboostperformancetappingintounlabeledrealdocuments,facilitatingbetterrepresentationlearning.Weshowedthatboththemultimodalapproachandunsupervisedtaskscanhelpimproveperformance.Ourresultsindicatethatwehaveimprovedthestateoftheartonpreviouslyestablishedbenchmarks.Inaddition,wearepubliclyprovidingthelargesyntheticdataset(135,000pages)aswellasanewbenchmarkdataset:DSSE-200.AcknowledgmentThisworkstartedduringXiaoYang’sinternshipatAdobeResearch.ThisworkwassupportedbyNSFgrantCCF1317560andAdobeSystemsInc.5322References[1]Abbyy.https://www.abbyy.com/.2[2]Omnipage.https://goo.gl/nDQEpC.2,8[3]Wikipedia.https://dumps.wikimedia.org/.5,6[4]A.AminandR.Shiu.Pagesegmentationandclassiﬁcationutilizingbottom-upapproach.InternationalJournalofIm-ageandGraphics,1(02):345–361,2001.1,2[5]S.Antol,A.Agrawal,J.Lu,M.Mitchell,D.Batra,C.LawrenceZitnick,andD.Parikh.Vqa:Visualquestionanswering.InProceedingsoftheIEEEInternationalCon-ferenceonComputerVision,pages2425–2433,2015.3[6]A.Antonacopoulos,D.Bridson,C.Papadopoulos,andS.Pletschacher.Arealisticdatasetforperformanceevalua-tionofdocumentlayoutanalysis.In200910thInternationalConferenceonDocumentAnalysisandRecognition,pages296–300.IEEE,2009.2,5,6[7]A.Antonacopoulos,C.Clausner,C.Papadopoulos,andS.Pletschacher.Icdar2015competitiononrecognitionofInDocumentdocumentswithcomplexlayouts-rdcl2015.AnalysisandRecognition(ICDAR),201513thInternationalConferenceon,pages1151–1155.IEEE,2015.6[8]D.S.BloombergandL.Vincent.Documentimageapplica-tions.MorphologieMathmatique,2007.8[9]P.Bojanowski,E.Grave,A.Joulin,andT.Mikolov.Enrich-ingwordvectorswithsubwordinformation.arXivpreprintarXiv:1607.04606,2016.6[10]S.S.Bukhari,F.Shafait,andT.M.Breuel.Improveddocumentimagesegmentationalgorithmusingmultiresolu-tionmorphology.InIS&T/SPIEElectronicImaging,pages78740D–78740D.InternationalSocietyforOpticsandPho-tonics,2011.8[11]S.Changpinyo,W.-L.Chao,B.Gong,andF.Sha.Syn-thesizedclassiﬁersforzero-shotlearning.arXivpreprintarXiv:1603.00550,2016.3[12]K.Chen,M.Seuret,M.Liwicki,J.Hennebert,andR.In-gold.Pagesegmentationofhistoricaldocumentimageswithconvolutionalautoencoders.InDocumentAnalysisandRecognition(ICDAR),201513thInternationalConferenceon,pages1011–1015.IEEE,2015.2[13]L.-C.Chen,G.Papandreou,I.Kokkinos,K.Murphy,andA.L.Yuille.Semanticimagesegmentationwithdeepcon-arXivpreprintvolutionalnetsandfullyconnectedcrfs.arXiv:1412.7062,2014.3,7[14]A.Conway.Pagegrammarsandpageparsing.asyntacticap-proachtodocumentlayoutrecognition.InDocumentAnaly-sisandRecognition,1993.,ProceedingsoftheSecondInter-nationalConferenceon,pages761–764.IEEE,1993.2[15]C.CortesandV.Vapnik.Support-vectornetworks.Machinelearning,20(3):273–297,1995.2[16]J.Donahue,L.AnneHendricks,S.Guadarrama,M.Rohrbach,S.Venugopalan,K.Saenko,andT.Dar-rell.Long-termrecurrentconvolutionalnetworksforvisualInProceedingsoftheIEEErecognitionanddescription.ConferenceonComputerVisionandPatternRecognition,pages2625–2634,2015.3[17]F.C.Fern´andezandO.R.Terrades.Documentsegmenta-tionusingrelativelocationfeatures.InPatternRecognition(ICPR),201221stInternationalConferenceon,pages1562–1565.IEEE,2012.8[18]J.L.Fisher.Logicalstructuredescriptionsofsegmenteddoc-umentimages.ProceedingsofInternationalConferenceonDocumentAnalysisandRecognition,pages302–310,1991.2[19]A.Frome,G.S.Corrado,J.Shlens,S.Bengio,J.Dean,T.Mikolov,etal.Devise:Adeepvisual-semanticembed-dingmodel.InAdvancesinneuralinformationprocessingsystems,pages2121–2129,2013.3[20]H.Gao,J.Mao,J.Zhou,Z.Huang,L.Wang,andW.Xu.Areyoutalkingtoamachine?datasetandmethodsformul-tilingualimagequestion.InAdvancesinNeuralInformationProcessingSystems,pages2296–2304,2015.3[21]J.Ha,R.M.Haralick,andI.T.Phillips.DocumentpageInDocumentdecompositionbythebounding-boxproject.AnalysisandRecognition,1995.,ProceedingsoftheThirdInternationalConferenceon,volume2,pages1119–1122.IEEE,1995.1,2[22]J.Ha,R.M.Haralick,andI.T.Phillips.RecursivexycutInDoc-usingboundingboxesofconnectedcomponents.umentAnalysisandRecognition,1995.,ProceedingsoftheThirdInternationalConferenceon,volume2,pages952–955.IEEE,1995.1,2[23]K.He,X.Zhang,S.Ren,andJ.Sun.Deepresiduallearn-ingforimagerecognition.arXivpreprintarXiv:1512.03385,2015.3[24]R.IngoldandD.Armangil.Atop-downdocumentanalysismethodforlogicalstructurerecognition.InProceedingsofInternationalConferenceonDocumentAnalysisandRecog-nition,pages41–49,1991.2[25]S.IoffeandC.Szegedy.Batchnormalization:Acceleratingdeepnetworktrainingbyreducinginternalcovariateshift.arXivpreprintarXiv:1502.03167,2015.5[26]M.Iyyer,V.Manjunatha,J.Boyd-Graber,andH.Daum´eIII.DeepunorderedcompositionrivalssyntacticmethodsforInProceedingsoftheAssociationfortextclassiﬁcation.ComputationalLinguistics,2015.4[27]A.Joulin,E.Grave,P.Bojanowski,andT.Mikolov.BagarXivpreprintoftricksforefﬁcienttextclassiﬁcation.arXiv:1607.01759,2016.4[28]A.KarpathyandL.Fei-Fei.Deepvisual-semanticalign-InProceedingsmentsforgeneratingimagedescriptions.oftheIEEEConferenceonComputerVisionandPatternRecognition,pages3128–3137,2015.3[29]M.Krishnamoorthy,G.Nagy,S.Seth,andM.Viswanathan.SyntacticsegmentationandlabelingofdigitizedpagesfromIEEETransactionsonPatternAnalysistechnicaljournals.andMachineIntelligence,15(7):737–747,1993.2[30]F.Lebourgeois,Z.Bublinski,andH.Emptoz.Afastandefﬁcientmethodforextractingtextparagraphsandgraph-icsfromunconstraineddocuments.InPatternRecognition,1992.Vol.II.ConferenceB:PatternRecognitionMethod-ologyandSystems,Proceedings.,11thIAPRInternationalConferenceon,pages272–276.IEEE,1992.1,2[31]J.Liang,R.Rogers,R.M.Haralick,andI.T.Phillips.Uw-isldocumentimageanalysistoolbox:Anexperimentalen-vironment.InDocumentAnalysisandRecognition,1997.,5323ProceedingsoftheFourthInternationalConferenceon,vol-ume2,pages984–988.IEEE,1997.26,7[47]R.Smith.Anoverviewofthetesseractocrengine.2007.2,[48]R.Socher,M.Ganjoo,C.D.Manning,andA.Ng.Zero-shotlearningthroughcross-modaltransfer.InAdvancesinneuralinformationprocessingsystems,pages935–943,2013.3[49]C.Szegedy,W.Liu,Y.Jia,P.Sermanet,S.Reed,D.Anguelov,D.Erhan,V.Vanhoucke,andA.Rabinovich.Goingdeeperwithconvolutions.InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,pages1–9,2015.3[50]L.Todoran,M.Worring,andA.W.Smeulders.Theuvacolordocumentdataset.InternationalJournalofDocumentAnalysisandRecognition(IJDAR),7(4):228–240,2005.2,5[51]P.Vincent,H.Larochelle,Y.Bengio,andP.-A.Manzagol.Extractingandcomposingrobustfeatureswithdenoisingau-toencoders.InProceedingsofthe25thinternationalconfer-enceonMachinelearning,pages1096–1103.ACM,2008.3[52]Q.N.VoandG.Lee.Densepredictionfortextlinesegmen-tationinhandwrittendocumentimages.InImageProcess-ing(ICIP),2016IEEEInternationalConferenceon,pages3264–3268.IEEE,2016.2[53]Y.Wen,K.Zhang,Z.Li,andY.Qiao.Adiscrimina-tivefeaturelearningapproachfordeepfacerecognition.InEuropeanConferenceonComputerVision,pages499–515.Springer,2016.3[54]F.YuandV.Koltun.Multi-scalecontextaggregationbydi-latedconvolutions.arXivpreprintarXiv:1511.07122,2015.3[55]M.D.Zeiler.Adadelta:anadaptivelearningratemethod.arXivpreprintarXiv:1212.5701,2012.5[56]W.Zhang,S.Zeng,D.Wang,andX.Xue.Weaklysuper-visedsemanticsegmentationforsocialimages.InProceed-ingsoftheIEEEConferenceonComputerVisionandPatternRecognition,pages2718–2726,2015.3[57]Y.Zhang,E.K.Lee,E.H.Lee,andU.EDU.Augmentingsupervisedneuralnetworkswithunsupervisedobjectivesforlarge-scaleimageclassiﬁcation.4[58]J.Zhao,M.Mathieu,R.Goroshin,andY.Lecun.arXivpreprintStackedwhat-whereauto-encoders.arXiv:1506.02351,2015.3,4[32]T.-Y.Lin,M.Maire,S.Belongie,J.Hays,P.Perona,D.Ra-manan,P.Doll´ar,andC.L.Zitnick.Microsoftcoco:Com-monobjectsincontext.InEuropeanConferenceonCom-puterVision,pages740–755.Springer,2014.3,5[33]J.Long,E.Shelhamer,andT.Darrell.Fullyconvolutionalnetworksforsemanticsegmentation.InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecogni-tion,pages3431–3440,2015.3[34]Z.Lu,Z.Fu,T.Xiang,P.Han,L.Wang,andX.Gao.Learn-ingfromweakandnoisylabelsforsemanticsegmentation.2016.3[35]M.-T.Luong,T.D.Nguyen,andM.-Y.Kan.Logicalstruc-turerecoveryinscholarlyarticleswithrichdocumentfea-tures.MultimediaStorageandRetrievalInnovationsforDig-italLibrarySystems,270,2012.2,6,8[36]J.Mairal,J.Ponce,G.Sapiro,A.Zisserman,andF.R.Bach.Superviseddictionarylearning.InAdvancesinneuralinfor-mationprocessingsystems,pages1033–1040,2009.3[37]M.Malinowski,M.Rohrbach,andM.Fritz.Askyourneu-rons:Aneural-basedapproachtoansweringquestionsaboutimages.InProceedingsoftheIEEEInternationalConfer-enceonComputerVision,pages1–9,2015.3[38]S.Mao,A.Rosenfeld,andT.Kanungo.Documentstructureanalysisalgorithms:aliteraturesurvey.InElectronicImag-ing2003,pages197–207.InternationalSocietyforOpticsandPhotonics,2003.2[39]T.Mikolov,K.Chen,G.Corrado,andJ.Dean.Efﬁcientestimationofwordrepresentationsinvectorspace.arXivpreprintarXiv:1301.3781,2013.4[40]T.Mikolov,I.Sutskever,K.Chen,G.S.Corrado,andJ.Dean.Distributedrepresentationsofwordsandphrasesandtheircompositionality.InAdvancesinneuralinforma-tionprocessingsystems,pages3111–3119,2013.4[41]H.Noh,S.Hong,andB.Han.Learningdeconvolutionnet-workforsemanticsegmentation.InProceedingsoftheIEEEInternationalConferenceonComputerVision,pages1520–1528,2015.3[42]G.Papandreou,L.-C.Chen,K.Murphy,andA.L.Yuille.Weakly-andsemi-supervisedlearningofadcnnforseman-ticimagesegmentation.arXivpreprintarXiv:1502.02734,2015.3[43]P.O.Pinheiro,T.-Y.Lin,R.Collobert,andP.Doll´ar.Learn-ingtoreﬁneobjectsegments.InProceedingsoftheEuro-peanConferenceonComputerVision(ECCV),2016.3[44]J.SauvolaandH.Kauniskangas.Mediateamdocumentdatabaseii.ACD-ROMcollectionofdocumentimages,Uni-versityofOuluFinland,1999.2,5[45]M.Shilman,P.Liang,andP.Viola.Learningnongenerativegrammaticalmodelsfordocumentanalysis.InTenthIEEEInternationalConferenceonComputerVision(ICCV’05)Volume1,volume2,pages962–969.IEEE,2005.2[46]A.Simon,J.-C.Pret,andA.P.Johnson.Afastalgorithmforbottom-updocumentlayoutanalysis.IEEETransactionsonPatternAnalysisandMachineIntelligence,19(3):273–277,1997.1,25324\n"
    }
   ],
   "source": [
    "from lxml import etree \n",
    "tree = etree.parse('output.xml')\n",
    "root = tree.getroot()\n",
    "text = root.xpath(\"string()\")\n",
    "modified_text = []\n",
    "newline_count = 0\n",
    "for line in text:\n",
    "    if(line == '\\n'):\n",
    "        newline_count += 1\n",
    "        if(newline_count < 3):\n",
    "            continue\n",
    "    if(line == '\\t'):\n",
    "        continue\n",
    "    modified_text.append(line.lstrip())\n",
    "    newline_count = 0\n",
    "text = \"\".join(modified_text)\n",
    "print(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}