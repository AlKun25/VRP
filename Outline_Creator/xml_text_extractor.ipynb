{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lxml.etree as ET\n",
    "from xml.dom import minidom\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "tree = ET.parse(\"CVPR2017.pdf.tei.xml\")   # import xml from\n",
    "root = tree.getroot()\n",
    "\n",
    "headings_seq = []\n",
    "headings_name = []\n",
    "headings_para = []\n",
    "\n",
    "headings_seq.append({'n' : '0 '})\n",
    "headings_name.append(\"Abstract\")\n",
    "\n",
    "for item in root.findall(\"./{http://www.tei-c.org/ns/1.0}text/{http://www.tei-c.org/ns/1.0}body/{http://www.tei-c.org/ns/1.0}div/{http://www.tei-c.org/ns/1.0}head\"):\n",
    "    headings_seq.append(item.attrib)\n",
    "    headings_name.append(item.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[{'n': '0 '}, {'n': '1.'}, {'n': '2.'},\n {'n': '3.'}, {'n': '3.1.'},\n {'n': '3.2.'}, {'n': '3.3.'},\n {'n': '4.'}, {'n': '5.'}, {'n': '6.'},\n {'n': '6.1.'}, {'n': '6.2.'}, {},\n {'n': '6.3.'}, {'n': '6.4.'},\n {'n': '7.'}]\n['Abstract', 'Introduction',\n 'Background', 'Method',\n 'Multimodal Fully Convolutional '\n 'Network',\n 'Text Embedding Map',\n 'Unsupervised Tasks',\n 'Synthetic Document Data',\n 'Implementation Details', 'Experiments',\n 'Ablation Experiment on Model '\n 'Architecture',\n 'Adding Textual Information', 'Methods',\n 'Unsupervised Learning Tasks',\n 'Comparisons with Prior Art',\n 'Conclusion']\n[{'Introduction': 'Document semantic '\n                  'structure extraction '\n                  '(DSSE) is an '\n                  'actively-researched '\n                  'area dedicated to '\n                  'understanding images '\n                  'of documents. The '\n                  'goal is to split a '\n                  'document image into '\n                  'regions of interest '\n                  'and to recognize the '\n                  'role of each region. '\n                  'It is usually done '\n                  'in two steps: the '\n                  'first step, often '\n                  'referred to as page '\n                  'segmentation, is '\n                  'appearance-based and '\n                  'attempts to '\n                  'distinguish text '\n                  'regions from regions '\n                  'like figures, tables '\n                  'and line segments. '\n                  'The second step, '\n                  'often referred to as '\n                  'logical structure '\n                  'analysis, is '\n                  'semantics-based and '\n                  'categorizes each '\n                  'region into '\n                  'semantically-relevant '\n                  'classes like '\n                  'paragraph and '\n                  'caption.'},\n {'Introduction': 'In this work, we '\n                  'propose a unified '\n                  'multimodal fully '\n                  'convolutional '\n                  'network (MFCN) that '\n                  'simultaneously '\n                  'identifies both '\n                  'appearance-based and '\n                  'semantics-based '\n                  'classes. It is a '\n                  'generalized page '\n                  'segmentation model '\n                  'that additionally '\n                  'performs '\n                  'fine-grained '\n                  'recognition on text '\n                  'regions: text '\n                  'regions are assigned '\n                  'specific labels '\n                  'based on their '\n                  'semantic '\n                  'functionality in the '\n                  'document. Our '\n                  'approach simplifies '\n                  'DSSE and better '\n                  'supports document '\n                  'image '\n                  'understanding.'},\n {'Introduction': 'We consider DSSE as '\n                  'a pixel-wise '\n                  'segmentation '\n                  'problem: each pixel '\n                  'is labeled as '\n                  'background, figure, '\n                  'table, paragraph, '\n                  'section heading, '\n                  'list, caption, etc. '\n                  'We show that our '\n                  'MFCN model trained '\n                  'in an end-to-end, '\n                  'pixels-topixels '\n                  'manner on document '\n                  'images exceeds the '\n                  'state-ofthe-art '\n                  'significantly. It '\n                  'eliminates the need '\n                  'to design complex '\n                  'heuristic rules and '\n                  'extract hand-crafted '\n                  'features '},\n {'Introduction': 'In many cases, '\n                  'regions like section '\n                  'headings or captions '\n                  'can be visually '\n                  'identified. In '},\n {'Introduction': 'To this end, our '\n                  'multimodal fully '\n                  'convolutional '\n                  'network is designed '\n                  'to leverage the '\n                  'textual information '\n                  'in the document as '\n                  'well. To incorporate '\n                  'textual information '\n                  'in a CNNbased '\n                  'architecture, we '\n                  'build a text '\n                  'embedding map and '\n                  'feed it to our MFCN. '\n                  'More specifically, '\n                  'we embed each '\n                  'sentence and map the '\n                  'embedding to the '\n                  'corresponding pixels '\n                  'where the sentence '\n                  'is represented in '\n                  'the document. '},\n {'Introduction': 'One of the '\n                  'bottlenecks in '\n                  'training fully '\n                  'convolutional '\n                  'networks is the need '\n                  'for pixel-wise '\n                  'ground truth data. '\n                  'Previous document '\n                  'understanding '\n                  'datasets '},\n {'Introduction': 'Our main '\n                  'contributions are '\n                  'summarized as '\n                  'follows:'},\n {'Introduction': '• We propose an '\n                  'end-to-end, unified '\n                  'network to address '\n                  'document semantic '\n                  'structure '\n                  'extraction. Unlike '\n                  'previous two-step '\n                  'processes, we '\n                  'simultaneously '\n                  'identify both '\n                  'appearance-based and '\n                  'semantics-based '\n                  'classes.'},\n {'Introduction': '• Our network '\n                  'supports both '\n                  'supervised training '\n                  'on image and text of '\n                  'documents, as well '\n                  'as unsupervised '\n                  'auxiliary training '\n                  'for better '\n                  'representation '\n                  'learning.'},\n {'Introduction': '• We propose a '\n                  'synthetic data '\n                  'generation process '\n                  'and use it to '\n                  'synthesize a '\n                  'large-scale dataset '\n                  'for training the '\n                  'supervised part of '\n                  'our deep MFCN '\n                  'model.'},\n {'Background': 'Page Segmentation. '\n                'Most earlier works on '\n                'page segmentation '},\n {'Background': 'With recent advances '\n                'in deep convolutional '\n                'neural networks, '\n                'several neural-based '\n                'models have been '\n                'proposed. Chen et '\n                'al. '},\n {'Background': 'Logical Structure '\n                'Analysis. Logical '\n                'structure is defined '\n                'as a hierarchy of '\n                'logical components in '\n                'documents, such as '\n                'section headings, '\n                'paragraphs and lists '},\n {'Background': 'Collecting pixel-wise '\n                'annotations for '\n                'thousands or millions '\n                'of images requires '\n                'massive labor and '\n                'cost. To this end, '\n                'several methods '},\n {'Background': 'Unsupervised Learning. '\n                'Several methods have '\n                'been proposed to use '\n                'unsupervised learning '\n                'to improve supervised '\n                'learning tasks. Mairal '\n                'et al. '},\n {'Background': 'Wen et al. '},\n {'Background': 'Language and Vision. '\n                'Several joint learning '\n                'tasks such as image '\n                'captioning '},\n {'Method': 'Our method does supervised '\n            'training for pixel-wise '\n            'segmentation with a '\n            'specialized multimodal '\n            'fully convolutional '\n            'network that uses a text '\n            'embedding map jointly with '\n            'the visual cues. Moreover, '\n            'our MFCN architecture also '\n            'supports two unsupervised '\n            'learning tasks to improve '\n            'the learned document '\n            'representation: a '\n            'reconstruction task based '\n            'on an auxiliary decoder '\n            'and a consistency task '\n            'evaluated in the main '\n            'decoder branch along with '\n            'the per-pixel segmentation '\n            'loss.'},\n {'Multimodal Fully Convolutional Network': 'As '\n                                            'shown '\n                                            'in '},\n {'Multimodal Fully Convolutional Network': 'First, '\n                                            'we '\n                                            'observe '\n                                            'that '\n                                            'several '\n                                            'semantic-based '\n                                            'classes '\n                                            'such '\n                                            'as '\n                                            'section '\n                                            'heading '\n                                            'and '\n                                            'caption '\n                                            'usually '\n                                            'occupy '\n                                            'relatively '\n                                            'small '\n                                            'areas. '\n                                            'Moreover, '\n                                            'correctly '\n                                            'identifying '\n                                            'certain '\n                                            'regions '\n                                            'often '\n                                            'relies '\n                                            'on '\n                                            'small '\n                                            'visual '\n                                            'cues, '\n                                            'like '\n                                            'lists '\n                                            'being '\n                                            'identified '\n                                            'by '\n                                            'small '\n                                            'bullets '\n                                            'or '\n                                            'numbers '\n                                            'in '\n                                            'front '\n                                            'of '\n                                            'each '\n                                            'item. '\n                                            'This '\n                                            'suggests '\n                                            'that '\n                                            'low-level '\n                                            'features '\n                                            'need '\n                                            'to '\n                                            'be '\n                                            'used. '\n                                            'However, '\n                                            'because '\n                                            'max-pooling '\n                                            'naturally '\n                                            'loses '\n                                            'information '\n                                            'during '\n                                            'downsampling, '\n                                            'FCN '\n                                            'often '\n                                            'performs '\n                                            'poorly '\n                                            'for '\n                                            'small '\n                                            'objects. '\n                                            'Long '\n                                            'et '\n                                            'al. '},\n {'Multimodal Fully Convolutional Network': 'We '\n                                            'also '\n                                            'notice '\n                                            'that '\n                                            'broader '\n                                            'context '\n                                            'information '\n                                            'is '\n                                            'needed '\n                                            'to '\n                                            'identify '\n                                            'certain '\n                                            'objects. '\n                                            'For '\n                                            'an '\n                                            'instance, '\n                                            'it '\n                                            'is '\n                                            'often '\n                                            'difficult '\n                                            'to '\n                                            'tell '\n                                            'the '\n                                            'difference '\n                                            'between '\n                                            'a '\n                                            'list '\n                                            'and '\n                                            'several '\n                                            'paragraphs '\n                                            'by '\n                                            'only '\n                                            'looking '\n                                            'at '\n                                            'parts '\n                                            'of '\n                                            'them. '\n                                            'In '},\n {'Text Embedding Map': 'Traditional '\n                        'image semantic '\n                        'segmentation '\n                        'models learn '\n                        'the semantic '\n                        'meanings of '\n                        'objects from a '\n                        'visual '\n                        'perspective. '\n                        'Our task, '\n                        'however, also '\n                        'requires '\n                        'understanding '\n                        'the text in '\n                        'images from a '\n                        'linguistic '\n                        'perspective. '\n                        'Therefore, we '\n                        'build a text '\n                        'embedding map '\n                        'and feed it to '\n                        'our multimodal '\n                        'model to make '\n                        'use of both '\n                        'visual and '\n                        'textual '\n                        'representations.'},\n {'Text Embedding Map': 'We treat a '\n                        'sentence as '\n                        'the minimum '\n                        'unit that '\n                        'conveys '\n                        'certain '\n                        'semantic '\n                        'meanings, and '\n                        'represent it '\n                        'using a '\n                        'lowdimensional '\n                        'vector. Our '\n                        'sentence '\n                        'embedding is '\n                        'built by '\n                        'averaging '\n                        'embeddings for '\n                        'individual '\n                        'words. This is '\n                        'a simple yet '\n                        'effective '\n                        'method that '\n                        'has been shown '\n                        'to be useful '\n                        'in many '\n                        'applications, '\n                        'including '\n                        'sentiment '\n                        'analysis '},\n {'Text Embedding Map': 'Specifically, '\n                        'our word '\n                        'embedding is '\n                        'learned using '\n                        'the skip-gram '\n                        'model '},\n {'Text Embedding Map': ', we maximize '\n                        'the average '\n                        'log '\n                        'probability'},\n {'Text Embedding Map': 'where T is the '\n                        'length of the '\n                        'sequence and C '\n                        'is the size of '\n                        'the context '\n                        'window. The '\n                        'probability of '\n                        'outputting a '\n                        'word w o given '\n                        'an input word '\n                        'w i is defined '\n                        'using '\n                        'softmax:'},\n {'Text Embedding Map': 'where v w and '\n                        'v ′ w are the '\n                        '\"input\" and '\n                        '\"output\" '\n                        'Ndimensional '\n                        'vector '\n                        'representations '\n                        'of w.'},\n {'Unsupervised Tasks': 'Although our '\n                        'synthetic '\n                        'documents '\n                        '(Sec. 4) '\n                        'provide a '\n                        'large amount '\n                        'of labeled '\n                        'data for '\n                        'training, they '\n                        'are limited in '\n                        'the variations '\n                        'of their '\n                        'layouts. To '\n                        'this end, we '\n                        'define two '\n                        'unsupervised '\n                        'loss functions '\n                        'to make use of '\n                        'real documents '\n                        'and to '\n                        'encourage '\n                        'better '\n                        'representation '\n                        'learning.'},\n {'Unsupervised Tasks': 'Reconstruction '\n                        'Task. It has '\n                        'been shown '\n                        'that '\n                        'reconstruction '\n                        'can help '\n                        'learning '\n                        'better '\n                        'representations '\n                        'and therefore '\n                        'improves '\n                        'performance '\n                        'for supervised '\n                        'tasks '},\n {'Unsupervised Tasks': 'Consistency '\n                        'Task. '\n                        'Pixel-wise '\n                        'annotations '\n                        'are '\n                        'laborintensive '\n                        'to obtain, '\n                        'however it is '\n                        'relatively '\n                        'easy to get a '\n                        'set of '\n                        'bounding boxes '\n                        'for detected '\n                        'objects in a '\n                        'document. For '\n                        'documents in '\n                        'PDF format, '\n                        'one can find '\n                        'bounding boxes '\n                        'by analyzing '\n                        'the rendering '\n                        'commands in '\n                        'the PDF files '\n                        '(See our '\n                        'supplementary '\n                        'document for '\n                        'typical '\n                        'examples). '\n                        'Even if their '\n                        'labels remain '\n                        'unknown, these '\n                        'bounding boxes '\n                        'are still '\n                        'beneficial: '\n                        'they provide '\n                        'knowledge of '\n                        'which parts of '\n                        'a document '\n                        'belongs to the '\n                        'same objects '\n                        'and thus '\n                        'should not be '\n                        'segmented into '\n                        'different '\n                        'fragments.'},\n {'Unsupervised Tasks': 'By building on '\n                        'the intuition '\n                        'that regions '\n                        'belonging to '\n                        'same objects '\n                        'should have '\n                        'similar '\n                        'feature '\n                        'representations, '\n                        'we define the '\n                        'consistency '\n                        'task loss L '\n                        'cons as '\n                        'follows. Let'},\n {'Unsupervised Tasks': 'be activations '\n                        'at location '\n                        '(i, j) in a '\n                        'feature map of '\n                        'size C × H × W '\n                        ', and b be the '\n                        'rectangular '\n                        'area in a '\n                        'bounding box. '\n                        'Let each '\n                        'rectangular '\n                        'area b is of '\n                        'size H b × W b '\n                        '. Then, for '\n                        'each b ∈ B, L '\n                        'cons will be '\n                        'given by'},\n {'Unsupervised Tasks': 'Minimizing '\n                        'consistency '\n                        'loss L cons '\n                        'encourages '\n                        'intra-region '\n                        'consistency.'},\n {'Unsupervised Tasks': 'The '\n                        'consistency '\n                        'loss L cons is '\n                        'differentiable '\n                        'and can be '\n                        'optimized '\n                        'using '\n                        'stochastic '\n                        'gradient '\n                        'descent. The '\n                        'gradient of L '\n                        'cons with '\n                        'respect to'},\n {'Unsupervised Tasks': 'since H b W b '\n                        '≫ 1, for '\n                        'efficiency it '\n                        'can be '\n                        'approximated '\n                        'by:'},\n {'Unsupervised Tasks': 'We use the '\n                        'unsupervised '\n                        'consistency '\n                        'loss, L cons , '\n                        'as a loss '\n                        'layer, that is '\n                        'evaluated at '\n                        'the main '\n                        'decoder branch '\n                        '(blue branch '\n                        'in '},\n {'Synthetic Document Data': 'Since our '\n                             'MFCN aims '\n                             'to '\n                             'generate '\n                             'a '\n                             'segmentation '\n                             'mask of '\n                             'the whole '\n                             'document '\n                             'image, '\n                             'pixel-wise '\n                             'annotations '\n                             'are '\n                             'required '\n                             'for the '\n                             'supervised '\n                             'task. '\n                             'While '\n                             'there are '\n                             'several '\n                             'publicly '\n                             'available '\n                             'datasets '\n                             'for page '\n                             'segmentation '},\n {'Synthetic Document Data': 'To '\n                             'address '\n                             'these '\n                             'issues, '\n                             'we '\n                             'created a '\n                             'synthetic '\n                             'data '\n                             'engine, '\n                             'capable '\n                             'of '\n                             'generating '\n                             'large-scale, '\n                             'pixel-wise '\n                             'annotated '\n                             'documents.'},\n {'Synthetic Document Data': 'Our '\n                             'synthetic '\n                             'document '\n                             'engine '\n                             'uses two '\n                             'methods '\n                             'to '\n                             'generate '\n                             'documents. '\n                             'The first '\n                             'produces '\n                             'completely '\n                             'automated '\n                             'and '\n                             'random '\n                             'layout of '\n                             'partial '\n                             'data '\n                             'scraped '\n                             'from the '\n                             'web. More '\n                             'specifically, '\n                             'we '\n                             'generate '\n                             'LaTeX '\n                             'source '\n                             'files in '\n                             'which '\n                             'paragraphs, '\n                             'figures, '\n                             'tables, '\n                             'captions, '\n                             'section '\n                             'headings '\n                             'and lists '\n                             'are '\n                             'randomly '\n                             'arranged '\n                             'to make '\n                             'up '\n                             'single, '\n                             'double, '\n                             'or '\n                             'triple-column '\n                             'PDFs. '\n                             'Candidate '\n                             'figures '\n                             'include '\n                             'academicstyle '\n                             'figures '\n                             'and '\n                             'graphic '\n                             'drawings '\n                             'downloaded '\n                             'using web '\n                             'image '\n                             'search, '\n                             'and '\n                             'natural '\n                             'images '\n                             'from MS '\n                             'COCO '},\n {'Synthetic Document Data': '• For '\n                             'paragraphs, '\n                             'we '\n                             'randomly '\n                             'sample '\n                             'sentences '\n                             'from a '\n                             '2016 '\n                             'English '\n                             'Wikipedia '\n                             'dump '},\n {'Synthetic Document Data': '• For '\n                             'section '\n                             'headings, '\n                             'we only '\n                             'sample '\n                             'sentences '\n                             'and '\n                             'phrases '\n                             'that are '\n                             'section '\n                             'or '\n                             'subsection '\n                             'headings '\n                             'in the '\n                             '\"Contents\" '\n                             'block in '\n                             'a '\n                             'Wikipedia '\n                             'page.'},\n {'Synthetic Document Data': '• For '\n                             'lists, we '\n                             'ensure '\n                             'that all '\n                             'items in '\n                             'a list '\n                             'come from '\n                             'the same '\n                             'Wikipedia '\n                             'page.'},\n {'Synthetic Document Data': '• For '\n                             'captions, '\n                             'we either '\n                             'use the '\n                             'associated '\n                             'caption '\n                             '(for '\n                             'images '\n                             'from MS '\n                             'COCO) or '\n                             'the title '\n                             'of the '\n                             'image in '\n                             'web image '\n                             'search, '\n                             'which can '\n                             'be found '\n                             'in the '\n                             'span with '\n                             'class '\n                             'name \"irc '\n                             'pt\".'},\n {'Synthetic Document Data': 'To '\n                             'further '\n                             'increase '\n                             'the '\n                             'complexity '\n                             'of the '\n                             'generated '\n                             'document '\n                             'layouts, '\n                             'we '\n                             'collected '\n                             'and '\n                             'labeled '\n                             '271 '\n                             'documents '\n                             'with '\n                             'varied, '\n                             'complicated '\n                             'layouts. '\n                             'We then '\n                             'randomly '\n                             'replaced '\n                             'each '\n                             'element '\n                             'with a '\n                             'standalone '\n                             'paragraph, '\n                             'figure, '\n                             'table, '\n                             'caption, '\n                             'section '\n                             'heading '\n                             'or list '\n                             'generated '\n                             'as stated '\n                             'above.'},\n {'Synthetic Document Data': 'In total, '\n                             'our '\n                             'synthetic '\n                             'dataset '\n                             'contains '\n                             '135,000 '\n                             'document '\n                             'images. '\n                             'Examples '\n                             'of our '\n                             'synthetic '\n                             'documents '\n                             'are shown '\n                             'in '},\n {'Implementation Details': 'We perform '\n                            'per-channel '\n                            'mean '\n                            'subtraction '\n                            'and resize '\n                            'each input '\n                            'image so '\n                            'that its '\n                            'longer '\n                            'side is '\n                            'less than '\n                            '384 '\n                            'pixels. No '\n                            'other '\n                            'pre-processing '\n                            'is '\n                            'applied. '\n                            'We use '\n                            'Adadelta '},\n {'Implementation Details': 'For text '\n                            'embedding, '\n                            'we '\n                            'represent '\n                            'each word '\n                            'as a '\n                            '128dimensional '\n                            'vector and '\n                            'train a '\n                            'skip-gram '\n                            'model on '\n                            'the 2016 '\n                            'English '\n                            'Wikipedia '\n                            'dump '},\n {'Implementation Details': 'Post-processing. '\n                            'We apply '\n                            'an '\n                            'optional '\n                            'post-processing '\n                            'step as a '\n                            'cleanup '\n                            'strategy '\n                            'for '\n                            'segment '\n                            'masks. For '\n                            'documents '\n                            'in PDF '\n                            'format, we '\n                            'obtain a '\n                            'set of '\n                            'candidate '\n                            'bounding '\n                            'boxes by '\n                            'analyzing '\n                            'the PDF '\n                            'format to '\n                            'find '\n                            'element '\n                            'boxes. We '\n                            'then '\n                            'refine the '\n                            'segmentation '\n                            'masks by '\n                            'first '\n                            'calculating '\n                            'the '\n                            'average '\n                            'class '\n                            'probability '\n                            'for pixels '\n                            'belonging '\n                            'to the '\n                            'same box, '\n                            'followed '\n                            'by '\n                            'assigning '\n                            'the most '\n                            'likely '\n                            'label to '\n                            'these '\n                            'pixels.'},\n {'Experiments': 'We used three '\n                 'datasets for '\n                 'evaluations: '\n                 'ICDAR2015 '},\n {'Experiments': 'The performance is '\n                 'measured in terms of '\n                 'pixel-wise '\n                 'intersection-over-union '\n                 '(IoU), which is '\n                 'standard in semantic '\n                 'segmentation tasks. '\n                 'We optimize the '\n                 'architecture of our '\n                 'MFCN model based on '\n                 'the DSSE-200 dataset '\n                 'since it contains '\n                 'both appearance-based '\n                 'and semantics-based '\n                 'labels. Sec. 6.4 '\n                 'compares our results '\n                 'to state-of-the-art '\n                 'methods on the '\n                 'ICDAR2015 and '\n                 'SectLabel datasets.'},\n {'Ablation Experiment on Model Architecture': 'We '\n                                               'first '\n                                               'systematically '\n                                               'evaluate '\n                                               'the '\n                                               'effectiveness '\n                                               'of '\n                                               'different '\n                                               'network '\n                                               'architectures. '\n                                               'Results '\n                                               'are '\n                                               'shown '\n                                               'in '},\n {'Ablation Experiment on Model Architecture': 'As '\n                                               'a '\n                                               'simple '\n                                               'baseline '},\n {'Ablation Experiment on Model Architecture': 'Next, '\n                                               'we '\n                                               'add '\n                                               'skip '\n                                               'connections '\n                                               'to '\n                                               'the '\n                                               'model, '\n                                               'resulting '\n                                               'in '\n                                               'Model2. '\n                                               'Note '\n                                               'that '\n                                               'this '\n                                               'model '\n                                               'is '\n                                               'similar '\n                                               'to '\n                                               'the '\n                                               'SharpMask '\n                                               'model. '\n                                               'We '\n                                               'observe '\n                                               'a '\n                                               'mean '\n                                               'IoU '\n                                               'of '\n                                               '65.4%, '\n                                               '4% '\n                                               'better '\n                                               'than '\n                                               'the '\n                                               'base '\n                                               'model. '\n                                               'The '\n                                               'improvements '\n                                               'are '\n                                               'even '\n                                               'more '\n                                               'significant '\n                                               'for '\n                                               'small '\n                                               'objects '\n                                               'like '\n                                               'captions.'},\n {'Ablation Experiment on Model Architecture': 'We '\n                                               'further '\n                                               'evaluate '\n                                               'the '\n                                               'effectiveness '\n                                               'of '\n                                               'replacing '\n                                               'bilinear '\n                                               'upsampling '\n                                               'with '\n                                               'unpooling, '\n                                               'giving '\n                                               'Model3. '\n                                               'All '\n                                               'upsampling '\n                                               'layers '\n                                               'in '\n                                               'Model2 '\n                                               'are '\n                                               'replaced '\n                                               'by '\n                                               'unpooling '\n                                               'while '\n                                               'other '\n                                               'parts '\n                                               'are '\n                                               'kept '\n                                               'unchanged. '\n                                               'Doing '\n                                               'so '\n                                               'results '\n                                               'in '\n                                               'a '\n                                               'significant '\n                                               'improvement '\n                                               'for '\n                                               'mean '\n                                               'IoU '\n                                               '(65.4% '\n                                               'vs. '\n                                               '71.2%). '\n                                               'This '\n                                               'suggests '\n                                               'that '\n                                               'the '\n                                               'pooled '\n                                               'index '\n                                               'should '\n                                               'not '\n                                               'be '\n                                               'discarded '\n                                               'during '\n                                               'decoding. '\n                                               'These '\n                                               'indexes '\n                                               'are '\n                                               'helpful '\n                                               'to '\n                                               'disambiguate '\n                                               'the '\n                                               'location '\n                                               'information '\n                                               'when '\n                                               'constructing '\n                                               'the '\n                                               'segmentation '\n                                               'mask '\n                                               'in '\n                                               'the '\n                                               'decoder.'},\n {'Ablation Experiment on Model Architecture': 'Finally, '\n                                               'we '\n                                               'investigate '\n                                               'the '\n                                               'use '\n                                               'of '\n                                               'dilated '\n                                               'convolutions. '\n                                               'Model3 '\n                                               'is '\n                                               'equivalent '\n                                               'to '\n                                               'using '\n                                               'dilated '\n                                               'convolution '\n                                               'when '\n                                               'd '\n                                               '= '\n                                               '1. '\n                                               'Model4 '\n                                               'sets '\n                                               'd '\n                                               '= '\n                                               '8 '\n                                               'while '\n                                               'Model5 '\n                                               'uses '\n                                               'the '\n                                               'dilated '\n                                               'block '\n                                               'illustrated '\n                                               'in '},\n {'Adding Textual Information': 'We now '\n                                'investigate '\n                                'the '\n                                'importance '\n                                'of '\n                                'textual '\n                                'information '\n                                'in our '\n                                'multimodal '\n                                'model. '\n                                'We '\n                                'take '\n                                'the '\n                                'best '\n                                'architecture, '\n                                'Model5, '\n                                'as our '\n                                'vision-only '\n                                'model, '\n                                'and '\n                                'incorporate '\n                                'a text '\n                                'embedding '\n                                'map '\n                                'via a '\n                                'bridge '\n                                'module '\n                                'depicted '\n                                'in '},\n {'Adding Textual Information': '75.4 '\n                                '75.9 '},\n {'Methods': 'non-text text Leptonica '},\n {'Unsupervised Learning Tasks': 'Here, '\n                                 'we '\n                                 'examine '\n                                 'how '\n                                 'the '\n                                 'proposed '\n                                 'two '\n                                 'unsupervised '\n                                 'learning '\n                                 'tasks '\n                                 '-reconstruction '\n                                 'and '\n                                 'consistency '\n                                 'taskscan '\n                                 'complement '\n                                 'the '\n                                 'pixel-wise '\n                                 'classification '\n                                 'during '\n                                 'training. '\n                                 'We '\n                                 'take '\n                                 'the '\n                                 'best '\n                                 'model '\n                                 'in '\n                                 'Sec. '\n                                 '6.2, '\n                                 'and '\n                                 'only '\n                                 'change '\n                                 'the '\n                                 'training '\n                                 'objectives. '\n                                 'Our '\n                                 'model '\n                                 'is '\n                                 'then '\n                                 'fine-tuned '\n                                 'in a '\n                                 'semisupervised '\n                                 'manner '\n                                 'as '\n                                 'described '\n                                 'in '\n                                 'Sec. '\n                                 '5. '\n                                 'The '\n                                 'results '\n                                 'are '\n                                 'shown '\n                                 'in '},\n {'Comparisons with Prior Art': 'Comparisons '\n                                'on '\n                                'ICDAR2015 '\n                                'dataset '},\n {'Comparisons with Prior Art': 'Comparisons '\n                                'on '\n                                'SectLabel '\n                                'dataset '},\n {'Conclusion': 'We proposed a '\n                'multimodal fully '\n                'convolutional network '\n                '(MFCN) for document '\n                'semantic structure '\n                'extraction. The '\n                'proposed model uses '\n                'both visual and '\n                'textual information. '\n                'Moreover, we propose '\n                'an efficient synthetic '\n                'data generation method '\n                'that yields per-pixel '\n                'ground-truth. Our '\n                'unsupervised auxiliary '\n                'tasks help boost '\n                'performance tapping '\n                'into unlabeled real '\n                'documents, '\n                'facilitating better '\n                'representation '\n                'learning. We showed '\n                'that both the '\n                'multimodal approach '\n                'and unsupervised tasks '\n                'can help improve '\n                'performance. Our '\n                'results indicate that '\n                'we have improved the '\n                'state of the art on '\n                'previously established '\n                'benchmarks. In '\n                'addition, we are '\n                'publicly providing the '\n                'large synthetic '\n                'dataset (135,000 '\n                'pages) as well as a '\n                'new benchmark dataset: '\n                'DSSE-200.'},\n {'Conclusion': 'We present an '\n                'end-to-end, '\n                'multimodal, fully '\n                'convolutional network '\n                'for extracting '\n                'semantic structures '\n                'from document images. '\n                'We consider document '\n                'semantic structure '\n                'extraction as a '\n                'pixel-wise '\n                'segmentation task, and '\n                'propose a unified '\n                'model that classifies '\n                'pixels based not only '\n                'on their visual '\n                'appearance, as in the '\n                'traditional page '\n                'segmentation task, but '\n                'also on the content of '\n                'underlying text. '\n                'Moreover, we propose '\n                'an efficient synthetic '\n                'document generation '\n                'process that we use to '\n                'generate pretraining '\n                'data for our network. '\n                'Once the network is '\n                'trained on a large set '\n                'of synthetic '\n                'documents, we '\n                'fine-tune the network '\n                'on unlabeled real '\n                'documents using a '\n                'semi-supervised '\n                'approach. We '\n                'systematically study '\n                'the optimum network '\n                'architecture and show '\n                'that both our '\n                'multimodal approach '\n                'and the synthetic data '\n                'pretraining '\n                'significantly boost '\n                'the performance.'},\n {'Abstract': 'Introduction'},\n {'Abstract': 'Document semantic '\n              'structure extraction '\n              '(DSSE) is an '\n              'actively-researched area '\n              'dedicated to '\n              'understanding images of '\n              'documents. The goal is '\n              'to split a document '\n              'image into regions of '\n              'interest and to '\n              'recognize the role of '\n              'each region. It is '\n              'usually done in two '\n              'steps: the first step, '\n              'often referred to as '\n              'page segmentation, is '\n              'appearance-based and '\n              'attempts to distinguish '\n              'text regions from '\n              'regions like figures, '\n              'tables and line '\n              'segments. The second '\n              'step, often referred to '\n              'as logical structure '\n              'analysis, is '\n              'semantics-based and '\n              'categorizes each region '\n              'into '\n              'semantically-relevant '\n              'classes like paragraph '\n              'and caption.'},\n {'Abstract': 'In this work, we propose '\n              'a unified multimodal '\n              'fully convolutional '\n              'network (MFCN) that '\n              'simultaneously '\n              'identifies both '\n              'appearance-based and '\n              'semantics-based classes. '\n              'It is a generalized page '\n              'segmentation model that '\n              'additionally performs '\n              'fine-grained recognition '\n              'on text regions: text '\n              'regions are assigned '\n              'specific labels based on '\n              'their semantic '\n              'functionality in the '\n              'document. Our approach '\n              'simplifies DSSE and '\n              'better supports document '\n              'image understanding.'},\n {'Abstract': 'We consider DSSE as a '\n              'pixel-wise segmentation '\n              'problem: each pixel is '\n              'labeled as background, '\n              'figure, table, '\n              'paragraph, section '\n              'heading, list, caption, '\n              'etc. We show that our '\n              'MFCN model trained in an '\n              'end-to-end, '\n              'pixels-topixels manner '\n              'on document images '\n              'exceeds the '\n              'state-ofthe-art '\n              'significantly. It '\n              'eliminates the need to '\n              'design complex heuristic '\n              'rules and extract '\n              'hand-crafted features '},\n {'Abstract': 'In many cases, regions '\n              'like section headings or '\n              'captions can be visually '\n              'identified. In '},\n {'Abstract': 'To this end, our '\n              'multimodal fully '\n              'convolutional network is '\n              'designed to leverage the '\n              'textual information in '\n              'the document as well. To '\n              'incorporate textual '\n              'information in a '\n              'CNNbased architecture, '\n              'we build a text '\n              'embedding map and feed '\n              'it to our MFCN. More '\n              'specifically, we embed '\n              'each sentence and map '\n              'the embedding to the '\n              'corresponding pixels '\n              'where the sentence is '\n              'represented in the '\n              'document. '},\n {'Abstract': 'One of the bottlenecks '\n              'in training fully '\n              'convolutional networks '\n              'is the need for '\n              'pixel-wise ground truth '\n              'data. Previous document '\n              'understanding datasets '},\n {'Abstract': 'Our main contributions '\n              'are summarized as '\n              'follows:'},\n {'Abstract': '• We propose an '\n              'end-to-end, unified '\n              'network to address '\n              'document semantic '\n              'structure extraction. '\n              'Unlike previous two-step '\n              'processes, we '\n              'simultaneously identify '\n              'both appearance-based '\n              'and semantics-based '\n              'classes.'},\n {'Abstract': '• Our network supports '\n              'both supervised training '\n              'on image and text of '\n              'documents, as well as '\n              'unsupervised auxiliary '\n              'training for better '\n              'representation '\n              'learning.'},\n {'Abstract': '• We propose a synthetic '\n              'data generation process '\n              'and use it to synthesize '\n              'a large-scale dataset '\n              'for training the '\n              'supervised part of our '\n              'deep MFCN model.'},\n {'Abstract': 'Background'},\n {'Abstract': 'Page Segmentation. Most '\n              'earlier works on page '\n              'segmentation '},\n {'Abstract': 'With recent advances in '\n              'deep convolutional '\n              'neural networks, several '\n              'neural-based models have '\n              'been proposed. Chen et '\n              'al. '},\n {'Abstract': 'Logical Structure '\n              'Analysis. Logical '\n              'structure is defined as '\n              'a hierarchy of logical '\n              'components in documents, '\n              'such as section '\n              'headings, paragraphs and '\n              'lists '},\n {'Abstract': 'Collecting pixel-wise '\n              'annotations for '\n              'thousands or millions of '\n              'images requires massive '\n              'labor and cost. To this '\n              'end, several methods '},\n {'Abstract': 'Unsupervised Learning. '\n              'Several methods have '\n              'been proposed to use '\n              'unsupervised learning to '\n              'improve supervised '\n              'learning tasks. Mairal '\n              'et al. '},\n {'Abstract': 'Wen et al. '},\n {'Abstract': 'Language and Vision. '\n              'Several joint learning '\n              'tasks such as image '\n              'captioning '},\n {'Abstract': 'Method'},\n {'Abstract': 'Our method does '\n              'supervised training for '\n              'pixel-wise segmentation '\n              'with a specialized '\n              'multimodal fully '\n              'convolutional network '\n              'that uses a text '\n              'embedding map jointly '\n              'with the visual cues. '\n              'Moreover, our MFCN '\n              'architecture also '\n              'supports two '\n              'unsupervised learning '\n              'tasks to improve the '\n              'learned document '\n              'representation: a '\n              'reconstruction task '\n              'based on an auxiliary '\n              'decoder and a '\n              'consistency task '\n              'evaluated in the main '\n              'decoder branch along '\n              'with the per-pixel '\n              'segmentation loss.'},\n {'Abstract': 'Multimodal Fully '\n              'Convolutional Network'},\n {'Abstract': 'As shown in '},\n {'Abstract': 'First, we observe that '\n              'several semantic-based '\n              'classes such as section '\n              'heading and caption '\n              'usually occupy '\n              'relatively small areas. '\n              'Moreover, correctly '\n              'identifying certain '\n              'regions often relies on '\n              'small visual cues, like '\n              'lists being identified '\n              'by small bullets or '\n              'numbers in front of each '\n              'item. This suggests that '\n              'low-level features need '\n              'to be used. However, '\n              'because max-pooling '\n              'naturally loses '\n              'information during '\n              'downsampling, FCN often '\n              'performs poorly for '\n              'small objects. Long et '\n              'al. '},\n {'Abstract': 'We also notice that '\n              'broader context '\n              'information is needed to '\n              'identify certain '\n              'objects. For an '\n              'instance, it is often '\n              'difficult to tell the '\n              'difference between a '\n              'list and several '\n              'paragraphs by only '\n              'looking at parts of '\n              'them. In '},\n {'Abstract': 'Text Embedding Map'},\n {'Abstract': 'Traditional image '\n              'semantic segmentation '\n              'models learn the '\n              'semantic meanings of '\n              'objects from a visual '\n              'perspective. Our task, '\n              'however, also requires '\n              'understanding the text '\n              'in images from a '\n              'linguistic perspective. '\n              'Therefore, we build a '\n              'text embedding map and '\n              'feed it to our '\n              'multimodal model to make '\n              'use of both visual and '\n              'textual '\n              'representations.'},\n {'Abstract': 'We treat a sentence as '\n              'the minimum unit that '\n              'conveys certain semantic '\n              'meanings, and represent '\n              'it using a '\n              'lowdimensional vector. '\n              'Our sentence embedding '\n              'is built by averaging '\n              'embeddings for '\n              'individual words. This '\n              'is a simple yet '\n              'effective method that '\n              'has been shown to be '\n              'useful in many '\n              'applications, including '\n              'sentiment analysis '},\n {'Abstract': 'Specifically, our word '\n              'embedding is learned '\n              'using the skip-gram '\n              'model '},\n {'Abstract': '[w 1 , w 2 , • • • , w T '\n              ']'},\n {'Abstract': ', we maximize the '\n              'average log probability'},\n {'Abstract': '1 T T t=1 −C≤j≤C,j =0 '\n              'logP (w t+j |w t )'},\n {'Abstract': 'where T is the length of '\n              'the sequence and C is '\n              'the size of the context '\n              'window. The probability '\n              'of outputting a word w o '\n              'given an input word w i '\n              'is defined using '\n              'softmax:'},\n {'Abstract': 'P (w o |w i ) = exp(v ′ '\n              'wo ⊤ v wi ) V w=1 exp(v '\n              '′ w ⊤ v wi )'},\n {'Abstract': 'where v w and v ′ w are '\n              'the \"input\" and \"output\" '\n              'Ndimensional vector '\n              'representations of w.'},\n {'Abstract': 'Unsupervised Tasks'},\n {'Abstract': 'Although our synthetic '\n              'documents (Sec. 4) '\n              'provide a large amount '\n              'of labeled data for '\n              'training, they are '\n              'limited in the '\n              'variations of their '\n              'layouts. To this end, we '\n              'define two unsupervised '\n              'loss functions to make '\n              'use of real documents '\n              'and to encourage better '\n              'representation '\n              'learning.'},\n {'Abstract': 'Reconstruction Task. It '\n              'has been shown that '\n              'reconstruction can help '\n              'learning better '\n              'representations and '\n              'therefore improves '\n              'performance for '\n              'supervised tasks '},\n {'Abstract': 'L (l) rec = 1 C l H l W '\n              'l a l −ã l 2 2 , l = 0, '\n              '1, 2, • • • L'},\n {'Abstract': 'Consistency Task. '\n              'Pixel-wise annotations '\n              'are laborintensive to '\n              'obtain, however it is '\n              'relatively easy to get a '\n              'set of bounding boxes '\n              'for detected objects in '\n              'a document. For '\n              'documents in PDF format, '\n              'one can find bounding '\n              'boxes by analyzing the '\n              'rendering commands in '\n              'the PDF files (See our '\n              'supplementary document '\n              'for typical examples). '\n              'Even if their labels '\n              'remain unknown, these '\n              'bounding boxes are still '\n              'beneficial: they provide '\n              'knowledge of which parts '\n              'of a document belongs to '\n              'the same objects and '\n              'thus should not be '\n              'segmented into different '\n              'fragments.'},\n {'Abstract': 'By building on the '\n              'intuition that regions '\n              'belonging to same '\n              'objects should have '\n              'similar feature '\n              'representations, we '\n              'define the consistency '\n              'task loss L cons as '\n              'follows. Let'},\n {'Abstract': 'p (i,j) (i = 1, 2, • • • '\n              'H, j = 1, 2, • • • W )'},\n {'Abstract': 'be activations at '\n              'location (i, j) in a '\n              'feature map of size C × '\n              'H × W , and b be the '\n              'rectangular area in a '\n              'bounding box. Let each '\n              'rectangular area b is of '\n              'size H b × W b . Then, '\n              'for each b ∈ B, L cons '\n              'will be given by'},\n {'Abstract': 'L cons = 1 H b W b '\n              '(i,j)∈b p (i,j) − p (b) '\n              '2 2'},\n {'Abstract': 'p (b) = 1 H b W b '\n              '(i,j)∈b p (i,j)'},\n {'Abstract': 'Minimizing consistency '\n              'loss L cons encourages '\n              'intra-region '\n              'consistency.'},\n {'Abstract': 'The consistency loss L '\n              'cons is differentiable '\n              'and can be optimized '\n              'using stochastic '\n              'gradient descent. The '\n              'gradient of L cons with '\n              'respect to'},\n {'Abstract': 'p (i,j) is ∂L cons ∂p '\n              '(i,j) = 2 H 2 b W 2 b (p '\n              '(i,j) − p (b) )(H b W b '\n              '− 1)+ 2 H 2 b W 2 b '\n              '(u,v)∈b (u,v) =(i,j) (p '\n              '(b) − p (u,v) )'},\n {'Abstract': 'since H b W b ≫ 1, for '\n              'efficiency it can be '\n              'approximated by:'},\n {'Abstract': '∂L cons ∂p (i,j) ≈ 2 H b '\n              'W b p (i,j) − p (b) .'},\n {'Abstract': 'We use the unsupervised '\n              'consistency loss, L cons '\n              ', as a loss layer, that '\n              'is evaluated at the main '\n              'decoder branch (blue '\n              'branch in '},\n {'Abstract': 'Synthetic Document Data'},\n {'Abstract': 'Since our MFCN aims to '\n              'generate a segmentation '\n              'mask of the whole '\n              'document image, '\n              'pixel-wise annotations '\n              'are required for the '\n              'supervised task. While '\n              'there are several '\n              'publicly available '\n              'datasets for page '\n              'segmentation '},\n {'Abstract': 'To address these issues, '\n              'we created a synthetic '\n              'data engine, capable of '\n              'generating large-scale, '\n              'pixel-wise annotated '\n              'documents.'},\n {'Abstract': 'Our synthetic document '\n              'engine uses two methods '\n              'to generate documents. '\n              'The first produces '\n              'completely automated and '\n              'random layout of partial '\n              'data scraped from the '\n              'web. More specifically, '\n              'we generate LaTeX source '\n              'files in which '\n              'paragraphs, figures, '\n              'tables, captions, '\n              'section headings and '\n              'lists are randomly '\n              'arranged to make up '\n              'single, double, or '\n              'triple-column PDFs. '\n              'Candidate figures '\n              'include academicstyle '\n              'figures and graphic '\n              'drawings downloaded '\n              'using web image search, '\n              'and natural images from '\n              'MS COCO '},\n {'Abstract': '• For paragraphs, we '\n              'randomly sample '\n              'sentences from a 2016 '\n              'English Wikipedia dump '},\n {'Abstract': '• For section headings, '\n              'we only sample sentences '\n              'and phrases that are '\n              'section or subsection '\n              'headings in the '\n              '\"Contents\" block in a '\n              'Wikipedia page.'},\n {'Abstract': '• For lists, we ensure '\n              'that all items in a list '\n              'come from the same '\n              'Wikipedia page.'},\n {'Abstract': '• For captions, we '\n              'either use the '\n              'associated caption (for '\n              'images from MS COCO) or '\n              'the title of the image '\n              'in web image search, '\n              'which can be found in '\n              'the span with class name '\n              '\"irc pt\".'},\n {'Abstract': 'To further increase the '\n              'complexity of the '\n              'generated document '\n              'layouts, we collected '\n              'and labeled 271 '\n              'documents with varied, '\n              'complicated layouts. We '\n              'then randomly replaced '\n              'each element with a '\n              'standalone paragraph, '\n              'figure, table, caption, '\n              'section heading or list '\n              'generated as stated '\n              'above.'},\n {'Abstract': 'In total, our synthetic '\n              'dataset contains 135,000 '\n              'document images. '\n              'Examples of our '\n              'synthetic documents are '\n              'shown in '},\n {'Abstract': 'Implementation Details'},\n {'Abstract': 'We perform per-channel '\n              'mean subtraction and '\n              'resize each input image '\n              'so that its longer side '\n              'is less than 384 pixels. '\n              'No other pre-processing '\n              'is applied. We use '\n              'Adadelta '},\n {'Abstract': 'For text embedding, we '\n              'represent each word as a '\n              '128dimensional vector '\n              'and train a skip-gram '\n              'model on the 2016 '\n              'English Wikipedia dump '},\n {'Abstract': 'Post-processing. We '\n              'apply an optional '\n              'post-processing step as '\n              'a cleanup strategy for '\n              'segment masks. For '\n              'documents in PDF format, '\n              'we obtain a set of '\n              'candidate bounding boxes '\n              'by analyzing the PDF '\n              'format to find element '\n              'boxes. We then refine '\n              'the segmentation masks '\n              'by first calculating the '\n              'average class '\n              'probability for pixels '\n              'belonging to the same '\n              'box, followed by '\n              'assigning the most '\n              'likely label to these '\n              'pixels.'},\n {'Abstract': 'Experiments'},\n {'Abstract': 'We used three datasets '\n              'for evaluations: '\n              'ICDAR2015 '},\n {'Abstract': 'The performance is '\n              'measured in terms of '\n              'pixel-wise '\n              'intersection-over-union '\n              '(IoU), which is standard '\n              'in semantic segmentation '\n              'tasks. We optimize the '\n              'architecture of our MFCN '\n              'model based on the '\n              'DSSE-200 dataset since '\n              'it contains both '\n              'appearance-based and '\n              'semantics-based labels. '\n              'Sec. 6.4 compares our '\n              'results to '\n              'state-of-the-art methods '\n              'on the ICDAR2015 and '\n              'SectLabel datasets.'},\n {'Abstract': 'Ablation Experiment on '\n              'Model Architecture'},\n {'Abstract': 'We first systematically '\n              'evaluate the '\n              'effectiveness of '\n              'different network '\n              'architectures. Results '\n              'are shown in '},\n {'Abstract': 'As a simple baseline '},\n {'Abstract': 'Next, we add skip '\n              'connections to the '\n              'model, resulting in '\n              'Model2. Note that this '\n              'model is similar to the '\n              'SharpMask model. We '\n              'observe a mean IoU of '\n              '65.4%, 4% better than '\n              'the base model. The '\n              'improvements are even '\n              'more significant for '\n              'small objects like '\n              'captions.'},\n {'Abstract': 'We further evaluate the '\n              'effectiveness of '\n              'replacing bilinear '\n              'upsampling with '\n              'unpooling, giving '\n              'Model3. All upsampling '\n              'layers in Model2 are '\n              'replaced by unpooling '\n              'while other parts are '\n              'kept unchanged. Doing so '\n              'results in a significant '\n              'improvement for mean IoU '\n              '(65.4% vs. 71.2%). This '\n              'suggests that the pooled '\n              'index should not be '\n              'discarded during '\n              'decoding. These indexes '\n              'are helpful to '\n              'disambiguate the '\n              'location information '\n              'when constructing the '\n              'segmentation mask in the '\n              'decoder.'},\n {'Abstract': 'Finally, we investigate '\n              'the use of dilated '\n              'convolutions. Model3 is '\n              'equivalent to using '\n              'dilated convolution when '\n              'd = 1. Model4 sets d = 8 '\n              'while Model5 uses the '\n              'dilated block '\n              'illustrated in '},\n {'Abstract': 'Adding Textual '\n              'Information'},\n {'Abstract': 'We now investigate the '\n              'importance of textual '\n              'information in our '\n              'multimodal model. We '\n              'take the best '\n              'architecture, Model5, as '\n              'our vision-only model, '\n              'and incorporate a text '\n              'embedding map via a '\n              'bridge module depicted '\n              'in '},\n {'Abstract': 'L cls L rec L cons L '\n              'rec+con mean 73.3 73.9'},\n {'Abstract': '75.4 75.9 '},\n {'Abstract': 'Methods'},\n {'Abstract': 'non-text text '\n              'Leptonica '},\n {'Abstract': 'Unsupervised Learning '\n              'Tasks'},\n {'Abstract': 'Here, we examine how the '\n              'proposed two '\n              'unsupervised learning '\n              'tasks -reconstruction '\n              'and consistency taskscan '\n              'complement the '\n              'pixel-wise '\n              'classification during '\n              'training. We take the '\n              'best model in Sec. 6.2, '\n              'and only change the '\n              'training objectives. Our '\n              'model is then fine-tuned '\n              'in a semisupervised '\n              'manner as described in '\n              'Sec. 5. The results are '\n              'shown in '},\n {'Abstract': 'Comparisons with Prior '\n              'Art'},\n {'Abstract': 'Comparisons on ICDAR2015 '\n              'dataset '},\n {'Abstract': 'Comparisons on SectLabel '\n              'dataset '},\n {'Abstract': 'Conclusion'},\n {'Abstract': 'We proposed a multimodal '\n              'fully convolutional '\n              'network (MFCN) for '\n              'document semantic '\n              'structure extraction. '\n              'The proposed model uses '\n              'both visual and textual '\n              'information. Moreover, '\n              'we propose an efficient '\n              'synthetic data '\n              'generation method that '\n              'yields per-pixel '\n              'ground-truth. Our '\n              'unsupervised auxiliary '\n              'tasks help boost '\n              'performance tapping into '\n              'unlabeled real '\n              'documents, facilitating '\n              'better representation '\n              'learning. We showed that '\n              'both the multimodal '\n              'approach and '\n              'unsupervised tasks can '\n              'help improve '\n              'performance. Our results '\n              'indicate that we have '\n              'improved the state of '\n              'the art on previously '\n              'established benchmarks. '\n              'In addition, we are '\n              'publicly providing the '\n              'large synthetic dataset '\n              '(135,000 pages) as well '\n              'as a new benchmark '\n              'dataset: DSSE-200.'},\n {'Conclusion': 'We present an '\n                'end-to-end, '\n                'multimodal, fully '\n                'convolutional network '\n                'for extracting '\n                'semantic structures '\n                'from document images. '\n                'We consider document '\n                'semantic structure '\n                'extraction as a '\n                'pixel-wise '\n                'segmentation task, and '\n                'propose a unified '\n                'model that classifies '\n                'pixels based not only '\n                'on their visual '\n                'appearance, as in the '\n                'traditional page '\n                'segmentation task, but '\n                'also on the content of '\n                'underlying text. '\n                'Moreover, we propose '\n                'an efficient synthetic '\n                'document generation '\n                'process that we use to '\n                'generate pretraining '\n                'data for our network. '\n                'Once the network is '\n                'trained on a large set '\n                'of synthetic '\n                'documents, we '\n                'fine-tune the network '\n                'on unlabeled real '\n                'documents using a '\n                'semi-supervised '\n                'approach. We '\n                'systematically study '\n                'the optimum network '\n                'architecture and show '\n                'that both our '\n                'multimodal approach '\n                'and the synthetic data '\n                'pretraining '\n                'significantly boost '\n                'the performance.'},\n {'Abstract': 'Document semantic '\n              'structure extraction '\n              '(DSSE) is an '\n              'actively-researched area '\n              'dedicated to '\n              'understanding images of '\n              'documents. The goal is '\n              'to split a document '\n              'image into regions of '\n              'interest and to '\n              'recognize the role of '\n              'each region. It is '\n              'usually done in two '\n              'steps: the first step, '\n              'often referred to as '\n              'page segmentation, is '\n              'appearance-based and '\n              'attempts to distinguish '\n              'text regions from '\n              'regions like figures, '\n              'tables and line '\n              'segments. The second '\n              'step, often referred to '\n              'as logical structure '\n              'analysis, is '\n              'semantics-based and '\n              'categorizes each region '\n              'into '\n              'semantically-relevant '\n              'classes like paragraph '\n              'and caption.'},\n {'Abstract': 'In this work, we propose '\n              'a unified multimodal '\n              'fully convolutional '\n              'network (MFCN) that '\n              'simultaneously '\n              'identifies both '\n              'appearance-based and '\n              'semantics-based classes. '\n              'It is a generalized page '\n              'segmentation model that '\n              'additionally performs '\n              'fine-grained recognition '\n              'on text regions: text '\n              'regions are assigned '\n              'specific labels based on '\n              'their semantic '\n              'functionality in the '\n              'document. Our approach '\n              'simplifies DSSE and '\n              'better supports document '\n              'image understanding.'},\n {'Abstract': 'We consider DSSE as a '\n              'pixel-wise segmentation '\n              'problem: each pixel is '\n              'labeled as background, '\n              'figure, table, '\n              'paragraph, section '\n              'heading, list, caption, '\n              'etc. We show that our '\n              'MFCN model trained in an '\n              'end-to-end, '\n              'pixels-topixels manner '\n              'on document images '\n              'exceeds the '\n              'state-ofthe-art '\n              'significantly. It '\n              'eliminates the need to '\n              'design complex heuristic '\n              'rules and extract '\n              'hand-crafted features '},\n {'Abstract': 'In many cases, regions '\n              'like section headings or '\n              'captions can be visually '\n              'identified. In '},\n {'Abstract': 'To this end, our '\n              'multimodal fully '\n              'convolutional network is '\n              'designed to leverage the '\n              'textual information in '\n              'the document as well. To '\n              'incorporate textual '\n              'information in a '\n              'CNNbased architecture, '\n              'we build a text '\n              'embedding map and feed '\n              'it to our MFCN. More '\n              'specifically, we embed '\n              'each sentence and map '\n              'the embedding to the '\n              'corresponding pixels '\n              'where the sentence is '\n              'represented in the '\n              'document. '},\n {'Abstract': 'One of the bottlenecks '\n              'in training fully '\n              'convolutional networks '\n              'is the need for '\n              'pixel-wise ground truth '\n              'data. Previous document '\n              'understanding datasets '},\n {'Abstract': 'Our main contributions '\n              'are summarized as '\n              'follows:'},\n {'Abstract': '• We propose an '\n              'end-to-end, unified '\n              'network to address '\n              'document semantic '\n              'structure extraction. '\n              'Unlike previous two-step '\n              'processes, we '\n              'simultaneously identify '\n              'both appearance-based '\n              'and semantics-based '\n              'classes.'},\n {'Abstract': '• Our network supports '\n              'both supervised training '\n              'on image and text of '\n              'documents, as well as '\n              'unsupervised auxiliary '\n              'training for better '\n              'representation '\n              'learning.'},\n {'Abstract': '• We propose a synthetic '\n              'data generation process '\n              'and use it to synthesize '\n              'a large-scale dataset '\n              'for training the '\n              'supervised part of our '\n              'deep MFCN model.'},\n {'Abstract': 'Page Segmentation. Most '\n              'earlier works on page '\n              'segmentation '},\n {'Abstract': 'With recent advances in '\n              'deep convolutional '\n              'neural networks, several '\n              'neural-based models have '\n              'been proposed. Chen et '\n              'al. '},\n {'Abstract': 'Logical Structure '\n              'Analysis. Logical '\n              'structure is defined as '\n              'a hierarchy of logical '\n              'components in documents, '\n              'such as section '\n              'headings, paragraphs and '\n              'lists '},\n {'Abstract': 'Collecting pixel-wise '\n              'annotations for '\n              'thousands or millions of '\n              'images requires massive '\n              'labor and cost. To this '\n              'end, several methods '},\n {'Abstract': 'Unsupervised Learning. '\n              'Several methods have '\n              'been proposed to use '\n              'unsupervised learning to '\n              'improve supervised '\n              'learning tasks. Mairal '\n              'et al. '},\n {'Abstract': 'Wen et al. '},\n {'Abstract': 'Language and Vision. '\n              'Several joint learning '\n              'tasks such as image '\n              'captioning '},\n {'Abstract': 'Our method does '\n              'supervised training for '\n              'pixel-wise segmentation '\n              'with a specialized '\n              'multimodal fully '\n              'convolutional network '\n              'that uses a text '\n              'embedding map jointly '\n              'with the visual cues. '\n              'Moreover, our MFCN '\n              'architecture also '\n              'supports two '\n              'unsupervised learning '\n              'tasks to improve the '\n              'learned document '\n              'representation: a '\n              'reconstruction task '\n              'based on an auxiliary '\n              'decoder and a '\n              'consistency task '\n              'evaluated in the main '\n              'decoder branch along '\n              'with the per-pixel '\n              'segmentation loss.'},\n {'Abstract': 'As shown in '},\n {'Abstract': 'First, we observe that '\n              'several semantic-based '\n              'classes such as section '\n              'heading and caption '\n              'usually occupy '\n              'relatively small areas. '\n              'Moreover, correctly '\n              'identifying certain '\n              'regions often relies on '\n              'small visual cues, like '\n              'lists being identified '\n              'by small bullets or '\n              'numbers in front of each '\n              'item. This suggests that '\n              'low-level features need '\n              'to be used. However, '\n              'because max-pooling '\n              'naturally loses '\n              'information during '\n              'downsampling, FCN often '\n              'performs poorly for '\n              'small objects. Long et '\n              'al. '},\n {'Abstract': 'We also notice that '\n              'broader context '\n              'information is needed to '\n              'identify certain '\n              'objects. For an '\n              'instance, it is often '\n              'difficult to tell the '\n              'difference between a '\n              'list and several '\n              'paragraphs by only '\n              'looking at parts of '\n              'them. In '},\n {'Abstract': 'Traditional image '\n              'semantic segmentation '\n              'models learn the '\n              'semantic meanings of '\n              'objects from a visual '\n              'perspective. Our task, '\n              'however, also requires '\n              'understanding the text '\n              'in images from a '\n              'linguistic perspective. '\n              'Therefore, we build a '\n              'text embedding map and '\n              'feed it to our '\n              'multimodal model to make '\n              'use of both visual and '\n              'textual '\n              'representations.'},\n {'Abstract': 'We treat a sentence as '\n              'the minimum unit that '\n              'conveys certain semantic '\n              'meanings, and represent '\n              'it using a '\n              'lowdimensional vector. '\n              'Our sentence embedding '\n              'is built by averaging '\n              'embeddings for '\n              'individual words. This '\n              'is a simple yet '\n              'effective method that '\n              'has been shown to be '\n              'useful in many '\n              'applications, including '\n              'sentiment analysis '},\n {'Abstract': 'Specifically, our word '\n              'embedding is learned '\n              'using the skip-gram '\n              'model '},\n {'Abstract': ', we maximize the '\n              'average log probability'},\n {'Abstract': 'where T is the length of '\n              'the sequence and C is '\n              'the size of the context '\n              'window. The probability '\n              'of outputting a word w o '\n              'given an input word w i '\n              'is defined using '\n              'softmax:'},\n {'Abstract': 'where v w and v ′ w are '\n              'the \"input\" and \"output\" '\n              'Ndimensional vector '\n              'representations of w.'},\n {'Abstract': 'Although our synthetic '\n              'documents (Sec. 4) '\n              'provide a large amount '\n              'of labeled data for '\n              'training, they are '\n              'limited in the '\n              'variations of their '\n              'layouts. To this end, we '\n              'define two unsupervised '\n              'loss functions to make '\n              'use of real documents '\n              'and to encourage better '\n              'representation '\n              'learning.'},\n {'Abstract': 'Reconstruction Task. It '\n              'has been shown that '\n              'reconstruction can help '\n              'learning better '\n              'representations and '\n              'therefore improves '\n              'performance for '\n              'supervised tasks '},\n {'Abstract': 'Consistency Task. '\n              'Pixel-wise annotations '\n              'are laborintensive to '\n              'obtain, however it is '\n              'relatively easy to get a '\n              'set of bounding boxes '\n              'for detected objects in '\n              'a document. For '\n              'documents in PDF format, '\n              'one can find bounding '\n              'boxes by analyzing the '\n              'rendering commands in '\n              'the PDF files (See our '\n              'supplementary document '\n              'for typical examples). '\n              'Even if their labels '\n              'remain unknown, these '\n              'bounding boxes are still '\n              'beneficial: they provide '\n              'knowledge of which parts '\n              'of a document belongs to '\n              'the same objects and '\n              'thus should not be '\n              'segmented into different '\n              'fragments.'},\n {'Abstract': 'By building on the '\n              'intuition that regions '\n              'belonging to same '\n              'objects should have '\n              'similar feature '\n              'representations, we '\n              'define the consistency '\n              'task loss L cons as '\n              'follows. Let'},\n {'Abstract': 'be activations at '\n              'location (i, j) in a '\n              'feature map of size C × '\n              'H × W , and b be the '\n              'rectangular area in a '\n              'bounding box. Let each '\n              'rectangular area b is of '\n              'size H b × W b . Then, '\n              'for each b ∈ B, L cons '\n              'will be given by'},\n {'Abstract': 'Minimizing consistency '\n              'loss L cons encourages '\n              'intra-region '\n              'consistency.'},\n {'Abstract': 'The consistency loss L '\n              'cons is differentiable '\n              'and can be optimized '\n              'using stochastic '\n              'gradient descent. The '\n              'gradient of L cons with '\n              'respect to'},\n {'Abstract': 'since H b W b ≫ 1, for '\n              'efficiency it can be '\n              'approximated by:'},\n {'Abstract': 'We use the unsupervised '\n              'consistency loss, L cons '\n              ', as a loss layer, that '\n              'is evaluated at the main '\n              'decoder branch (blue '\n              'branch in '},\n {'Abstract': 'Since our MFCN aims to '\n              'generate a segmentation '\n              'mask of the whole '\n              'document image, '\n              'pixel-wise annotations '\n              'are required for the '\n              'supervised task. While '\n              'there are several '\n              'publicly available '\n              'datasets for page '\n              'segmentation '},\n {'Abstract': 'To address these issues, '\n              'we created a synthetic '\n              'data engine, capable of '\n              'generating large-scale, '\n              'pixel-wise annotated '\n              'documents.'},\n {'Abstract': 'Our synthetic document '\n              'engine uses two methods '\n              'to generate documents. '\n              'The first produces '\n              'completely automated and '\n              'random layout of partial '\n              'data scraped from the '\n              'web. More specifically, '\n              'we generate LaTeX source '\n              'files in which '\n              'paragraphs, figures, '\n              'tables, captions, '\n              'section headings and '\n              'lists are randomly '\n              'arranged to make up '\n              'single, double, or '\n              'triple-column PDFs. '\n              'Candidate figures '\n              'include academicstyle '\n              'figures and graphic '\n              'drawings downloaded '\n              'using web image search, '\n              'and natural images from '\n              'MS COCO '},\n {'Abstract': '• For paragraphs, we '\n              'randomly sample '\n              'sentences from a 2016 '\n              'English Wikipedia dump '},\n {'Abstract': '• For section headings, '\n              'we only sample sentences '\n              'and phrases that are '\n              'section or subsection '\n              'headings in the '\n              '\"Contents\" block in a '\n              'Wikipedia page.'},\n {'Abstract': '• For lists, we ensure '\n              'that all items in a list '\n              'come from the same '\n              'Wikipedia page.'},\n {'Abstract': '• For captions, we '\n              'either use the '\n              'associated caption (for '\n              'images from MS COCO) or '\n              'the title of the image '\n              'in web image search, '\n              'which can be found in '\n              'the span with class name '\n              '\"irc pt\".'},\n {'Abstract': 'To further increase the '\n              'complexity of the '\n              'generated document '\n              'layouts, we collected '\n              'and labeled 271 '\n              'documents with varied, '\n              'complicated layouts. We '\n              'then randomly replaced '\n              'each element with a '\n              'standalone paragraph, '\n              'figure, table, caption, '\n              'section heading or list '\n              'generated as stated '\n              'above.'},\n {'Abstract': 'In total, our synthetic '\n              'dataset contains 135,000 '\n              'document images. '\n              'Examples of our '\n              'synthetic documents are '\n              'shown in '},\n {'Abstract': 'We perform per-channel '\n              'mean subtraction and '\n              'resize each input image '\n              'so that its longer side '\n              'is less than 384 pixels. '\n              'No other pre-processing '\n              'is applied. We use '\n              'Adadelta '},\n {'Abstract': 'For text embedding, we '\n              'represent each word as a '\n              '128dimensional vector '\n              'and train a skip-gram '\n              'model on the 2016 '\n              'English Wikipedia dump '},\n {'Abstract': 'Post-processing. We '\n              'apply an optional '\n              'post-processing step as '\n              'a cleanup strategy for '\n              'segment masks. For '\n              'documents in PDF format, '\n              'we obtain a set of '\n              'candidate bounding boxes '\n              'by analyzing the PDF '\n              'format to find element '\n              'boxes. We then refine '\n              'the segmentation masks '\n              'by first calculating the '\n              'average class '\n              'probability for pixels '\n              'belonging to the same '\n              'box, followed by '\n              'assigning the most '\n              'likely label to these '\n              'pixels.'},\n {'Abstract': 'We used three datasets '\n              'for evaluations: '\n              'ICDAR2015 '},\n {'Abstract': 'The performance is '\n              'measured in terms of '\n              'pixel-wise '\n              'intersection-over-union '\n              '(IoU), which is standard '\n              'in semantic segmentation '\n              'tasks. We optimize the '\n              'architecture of our MFCN '\n              'model based on the '\n              'DSSE-200 dataset since '\n              'it contains both '\n              'appearance-based and '\n              'semantics-based labels. '\n              'Sec. 6.4 compares our '\n              'results to '\n              'state-of-the-art methods '\n              'on the ICDAR2015 and '\n              'SectLabel datasets.'},\n {'Abstract': 'We first systematically '\n              'evaluate the '\n              'effectiveness of '\n              'different network '\n              'architectures. Results '\n              'are shown in '},\n {'Abstract': 'As a simple baseline '},\n {'Abstract': 'Next, we add skip '\n              'connections to the '\n              'model, resulting in '\n              'Model2. Note that this '\n              'model is similar to the '\n              'SharpMask model. We '\n              'observe a mean IoU of '\n              '65.4%, 4% better than '\n              'the base model. The '\n              'improvements are even '\n              'more significant for '\n              'small objects like '\n              'captions.'},\n {'Abstract': 'We further evaluate the '\n              'effectiveness of '\n              'replacing bilinear '\n              'upsampling with '\n              'unpooling, giving '\n              'Model3. All upsampling '\n              'layers in Model2 are '\n              'replaced by unpooling '\n              'while other parts are '\n              'kept unchanged. Doing so '\n              'results in a significant '\n              'improvement for mean IoU '\n              '(65.4% vs. 71.2%). This '\n              'suggests that the pooled '\n              'index should not be '\n              'discarded during '\n              'decoding. These indexes '\n              'are helpful to '\n              'disambiguate the '\n              'location information '\n              'when constructing the '\n              'segmentation mask in the '\n              'decoder.'},\n {'Abstract': 'Finally, we investigate '\n              'the use of dilated '\n              'convolutions. Model3 is '\n              'equivalent to using '\n              'dilated convolution when '\n              'd = 1. Model4 sets d = 8 '\n              'while Model5 uses the '\n              'dilated block '\n              'illustrated in '},\n {'Abstract': 'We now investigate the '\n              'importance of textual '\n              'information in our '\n              'multimodal model. We '\n              'take the best '\n              'architecture, Model5, as '\n              'our vision-only model, '\n              'and incorporate a text '\n              'embedding map via a '\n              'bridge module depicted '\n              'in '},\n {'Abstract': '75.4 75.9 '},\n {'Abstract': 'non-text text '\n              'Leptonica '},\n {'Abstract': 'Here, we examine how the '\n              'proposed two '\n              'unsupervised learning '\n              'tasks -reconstruction '\n              'and consistency taskscan '\n              'complement the '\n              'pixel-wise '\n              'classification during '\n              'training. We take the '\n              'best model in Sec. 6.2, '\n              'and only change the '\n              'training objectives. Our '\n              'model is then fine-tuned '\n              'in a semisupervised '\n              'manner as described in '\n              'Sec. 5. The results are '\n              'shown in '},\n {'Abstract': 'Comparisons on ICDAR2015 '\n              'dataset '},\n {'Abstract': 'Comparisons on SectLabel '\n              'dataset '},\n {'Abstract': 'We proposed a multimodal '\n              'fully convolutional '\n              'network (MFCN) for '\n              'document semantic '\n              'structure extraction. '\n              'The proposed model uses '\n              'both visual and textual '\n              'information. Moreover, '\n              'we propose an efficient '\n              'synthetic data '\n              'generation method that '\n              'yields per-pixel '\n              'ground-truth. Our '\n              'unsupervised auxiliary '\n              'tasks help boost '\n              'performance tapping into '\n              'unlabeled real '\n              'documents, facilitating '\n              'better representation '\n              'learning. We showed that '\n              'both the multimodal '\n              'approach and '\n              'unsupervised tasks can '\n              'help improve '\n              'performance. Our results '\n              'indicate that we have '\n              'improved the state of '\n              'the art on previously '\n              'established benchmarks. '\n              'In addition, we are '\n              'publicly providing the '\n              'large synthetic dataset '\n              '(135,000 pages) as well '\n              'as a new benchmark '\n              'dataset: DSSE-200.'},\n {'Abstract': 'This work started during '\n              \"Xiao Yang's internship \"\n              'at Adobe Research. This '\n              'work was supported by '\n              'NSF grant CCF 1317560 '\n              'and Adobe Systems Inc.'},\n {'Acknowledgment': 'This work started '\n                    \"during Xiao Yang's \"\n                    'internship at '\n                    'Adobe Research. '\n                    'This work was '\n                    'supported by NSF '\n                    'grant CCF 1317560 '\n                    'and Adobe Systems '\n                    'Inc.'},\n {'Abstract': 'We present an '\n              'end-to-end, multimodal, '\n              'fully convolutional '\n              'network for extracting '\n              'semantic structures from '\n              'document images. We '\n              'consider document '\n              'semantic structure '\n              'extraction as a '\n              'pixel-wise segmentation '\n              'task, and propose a '\n              'unified model that '\n              'classifies pixels based '\n              'not only on their visual '\n              'appearance, as in the '\n              'traditional page '\n              'segmentation task, but '\n              'also on the content of '\n              'underlying text. '\n              'Moreover, we propose an '\n              'efficient synthetic '\n              'document generation '\n              'process that we use to '\n              'generate pretraining '\n              'data for our network. '\n              'Once the network is '\n              'trained on a large set '\n              'of synthetic documents, '\n              'we fine-tune the network '\n              'on unlabeled real '\n              'documents using a '\n              'semi-supervised '\n              'approach. We '\n              'systematically study the '\n              'optimum network '\n              'architecture and show '\n              'that both our multimodal '\n              'approach and the '\n              'synthetic data '\n              'pretraining '\n              'significantly boost the '\n              'performance.'},\n {'Acknowledgment': 'We present an '\n                    'end-to-end, '\n                    'multimodal, fully '\n                    'convolutional '\n                    'network for '\n                    'extracting '\n                    'semantic '\n                    'structures from '\n                    'document images. '\n                    'We consider '\n                    'document semantic '\n                    'structure '\n                    'extraction as a '\n                    'pixel-wise '\n                    'segmentation task, '\n                    'and propose a '\n                    'unified model that '\n                    'classifies pixels '\n                    'based not only on '\n                    'their visual '\n                    'appearance, as in '\n                    'the traditional '\n                    'page segmentation '\n                    'task, but also on '\n                    'the content of '\n                    'underlying text. '\n                    'Moreover, we '\n                    'propose an '\n                    'efficient '\n                    'synthetic document '\n                    'generation process '\n                    'that we use to '\n                    'generate '\n                    'pretraining data '\n                    'for our network. '\n                    'Once the network '\n                    'is trained on a '\n                    'large set of '\n                    'synthetic '\n                    'documents, we '\n                    'fine-tune the '\n                    'network on '\n                    'unlabeled real '\n                    'documents using a '\n                    'semi-supervised '\n                    'approach. We '\n                    'systematically '\n                    'study the optimum '\n                    'network '\n                    'architecture and '\n                    'show that both our '\n                    'multimodal '\n                    'approach and the '\n                    'synthetic data '\n                    'pretraining '\n                    'significantly '\n                    'boost the '\n                    'performance.'},\n {'Acknowledgment': 'This work started '\n                    \"during Xiao Yang's \"\n                    'internship at '\n                    'Adobe Research. '\n                    'This work was '\n                    'supported by NSF '\n                    'grant CCF 1317560 '\n                    'and Adobe Systems '\n                    'Inc.'},\n {'Acknowledgment': 'We present an '\n                    'end-to-end, '\n                    'multimodal, fully '\n                    'convolutional '\n                    'network for '\n                    'extracting '\n                    'semantic '\n                    'structures from '\n                    'document images. '\n                    'We consider '\n                    'document semantic '\n                    'structure '\n                    'extraction as a '\n                    'pixel-wise '\n                    'segmentation task, '\n                    'and propose a '\n                    'unified model that '\n                    'classifies pixels '\n                    'based not only on '\n                    'their visual '\n                    'appearance, as in '\n                    'the traditional '\n                    'page segmentation '\n                    'task, but also on '\n                    'the content of '\n                    'underlying text. '\n                    'Moreover, we '\n                    'propose an '\n                    'efficient '\n                    'synthetic document '\n                    'generation process '\n                    'that we use to '\n                    'generate '\n                    'pretraining data '\n                    'for our network. '\n                    'Once the network '\n                    'is trained on a '\n                    'large set of '\n                    'synthetic '\n                    'documents, we '\n                    'fine-tune the '\n                    'network on '\n                    'unlabeled real '\n                    'documents using a '\n                    'semi-supervised '\n                    'approach. We '\n                    'systematically '\n                    'study the optimum '\n                    'network '\n                    'architecture and '\n                    'show that both our '\n                    'multimodal '\n                    'approach and the '\n                    'synthetic data '\n                    'pretraining '\n                    'significantly '\n                    'boost the '\n                    'performance.'},\n {'Acknowledgment': 'This work started '\n                    \"during Xiao Yang's \"\n                    'internship at '\n                    'Adobe Research. '\n                    'This work was '\n                    'supported by NSF '\n                    'grant CCF 1317560 '\n                    'and Adobe Systems '\n                    'Inc.'},\n {'Abstract': 'We present an '\n              'end-to-end, multimodal, '\n              'fully convolutional '\n              'network for extracting '\n              'semantic structures from '\n              'document images. We '\n              'consider document '\n              'semantic structure '\n              'extraction as a '\n              'pixel-wise segmentation '\n              'task, and propose a '\n              'unified model that '\n              'classifies pixels based '\n              'not only on their visual '\n              'appearance, as in the '\n              'traditional page '\n              'segmentation task, but '\n              'also on the content of '\n              'underlying text. '\n              'Moreover, we propose an '\n              'efficient synthetic '\n              'document generation '\n              'process that we use to '\n              'generate pretraining '\n              'data for our network. '\n              'Once the network is '\n              'trained on a large set '\n              'of synthetic documents, '\n              'we fine-tune the network '\n              'on unlabeled real '\n              'documents using a '\n              'semi-supervised '\n              'approach. We '\n              'systematically study the '\n              'optimum network '\n              'architecture and show '\n              'that both our multimodal '\n              'approach and the '\n              'synthetic data '\n              'pretraining '\n              'significantly boost the '\n              'performance.'},\n {'Acknowledgment': 'We present an '\n                    'end-to-end, '\n                    'multimodal, fully '\n                    'convolutional '\n                    'network for '\n                    'extracting '\n                    'semantic '\n                    'structures from '\n                    'document images. '\n                    'We consider '\n                    'document semantic '\n                    'structure '\n                    'extraction as a '\n                    'pixel-wise '\n                    'segmentation task, '\n                    'and propose a '\n                    'unified model that '\n                    'classifies pixels '\n                    'based not only on '\n                    'their visual '\n                    'appearance, as in '\n                    'the traditional '\n                    'page segmentation '\n                    'task, but also on '\n                    'the content of '\n                    'underlying text. '\n                    'Moreover, we '\n                    'propose an '\n                    'efficient '\n                    'synthetic document '\n                    'generation process '\n                    'that we use to '\n                    'generate '\n                    'pretraining data '\n                    'for our network. '\n                    'Once the network '\n                    'is trained on a '\n                    'large set of '\n                    'synthetic '\n                    'documents, we '\n                    'fine-tune the '\n                    'network on '\n                    'unlabeled real '\n                    'documents using a '\n                    'semi-supervised '\n                    'approach. We '\n                    'systematically '\n                    'study the optimum '\n                    'network '\n                    'architecture and '\n                    'show that both our '\n                    'multimodal '\n                    'approach and the '\n                    'synthetic data '\n                    'pretraining '\n                    'significantly '\n                    'boost the '\n                    'performance.'},\n {'Acknowledgment': 'This work started '\n                    \"during Xiao Yang's \"\n                    'internship at '\n                    'Adobe Research. '\n                    'This work was '\n                    'supported by NSF '\n                    'grant CCF 1317560 '\n                    'and Adobe Systems '\n                    'Inc.'}]\n"
    }
   ],
   "source": [
    "mydoc = minidom.parse(\"CVPR2017.pdf.tei.xml\")\n",
    "divs = mydoc.getElementsByTagName(\"div\")\n",
    "for div in divs: #<div> \n",
    "    if(div.parentNode.nodeName == 'abstract'):\n",
    "        try:\n",
    "            x = 'works'\n",
    "        except AttributeError:\n",
    "            continue\n",
    "        else:\n",
    "            for elem in div.childNodes:\n",
    "                if(elem.nodeName == 'p'):\n",
    "                    headings_para.append({\"Abstract\" : elem.firstChild.data})\n",
    "\n",
    "    if(div.parentNode.nodeName == \"body\"):\n",
    "        try:\n",
    "            x = 'works'\n",
    "        except AttributeError:\n",
    "            continue\n",
    "    else:\n",
    "        for elem in div.childNodes:\n",
    "            if(not elem.nodeName =='formula'):\n",
    "                if(elem.nodeName == 'head'):\n",
    "                    section = elem.firstChild.data\n",
    "                    continue\n",
    "                if(elem.nodeName == 'p'):\n",
    "                    headings_para.append({section : elem.firstChild.data})\n",
    "\n",
    "pp = pprint.PrettyPrinter(width=41, depth = 5, compact=True)\n",
    "pp.pprint(headings_seq)\n",
    "pp.pprint(headings_name)\n",
    "pp.pprint(headings_para)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}