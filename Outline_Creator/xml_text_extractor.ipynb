{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lxml.etree as ET\n",
    "from xml.dom import minidom\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "\n",
    "tree = ET.parse(\"source.xml\")   # import xml from\n",
    "root = tree.getroot()\n",
    "\n",
    "headings_seq = []\n",
    "headings_name = []\n",
    "headings_para = []\n",
    "\n",
    "headings_seq.append({'n' : '0 '})\n",
    "headings_name.append(\"Abstract\")\n",
    "\n",
    "for item in root.findall(\"./{http://www.tei-c.org/ns/1.0}text/{http://www.tei-c.org/ns/1.0}body/{http://www.tei-c.org/ns/1.0}div/{http://www.tei-c.org/ns/1.0}head\"):\n",
    "    headings_seq.append(item.attrib)\n",
    "    headings_name.append(item.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[{'n': '0 '}, {'n': '1'}, {'n': '2'},\n {'n': '3.1'}, {'n': '3.2'}, {'n': '4'},\n {'n': '4.1'}, {'n': '4.2'},\n {'n': '4.3'}, {'n': '4.4'},\n {'n': '4.5'}, {'n': '4.6'}, {'n': '5'},\n {'n': '5.1'}, {'n': '5.2'},\n {'n': '5.3'}, {'n': '5.4'}, {'n': '6'}]\n['Abstract', 'Introduction',\n 'Related work', 'Proposed task',\n 'Proposed models', 'Experimental setup',\n 'Visual Genome data set',\n 'Evaluation sets',\n 'Data pre-processing',\n 'Evaluation metrics', 'Word embeddings',\n 'Model hyperparameters and '\n 'implementation',\n 'Results and discussion',\n 'Evaluation with raw data',\n 'Generalized evaluations',\n 'Qualitative evaluation (spatial '\n 'templates)',\n 'Interpretation of model weights',\n 'Conclusions']\n[{'Abstract': 'Spatial understanding is '\n              'a fundamental problem '\n              'with widereaching '\n              'real-world applications. '\n              'The representation of '\n              'spatial knowledge is '\n              'often modeled with '\n              'spatial templates, i.e., '\n              'regions of acceptability '\n              'of two objects under an '\n              'explicit spatial '\n              'relationship (e.g., '\n              '\"on\", \"below\", etc.). In '\n              'contrast with prior work '\n              'that restricts spatial '\n              'templates to explicit '\n              'spatial prepositions '\n              '(e.g., \"glass on '\n              'table\"), here we extend '\n              'this concept to implicit '\n              'spatial language, i.e., '\n              'those relationships '\n              '(generally actions) for '\n              'which the spatial '\n              'arrangement of the '\n              'objects is only '\n              'implicitly implied '\n              '(e.g., \"man riding '\n              'horse\"). In contrast '\n              'with explicit '\n              'relationships, '\n              'predicting spatial '\n              'arrangements from '\n              'implicit spatial '\n              'language requires '\n              'significant common sense '\n              'spatial understanding. '\n              'Here, we introduce the '\n              'task of predicting '\n              'spatial templates for '\n              'two objects under a '\n              'relationship, which can '\n              'be seen as a spatial '\n              'question-answering task '\n              'with a (2D) continuous '\n              'output (\"where is the '\n              'man w.r.t. a horse when '\n              'the man is walking the '\n              'horse?\"). We present two '\n              'simple neural-based '\n              'models that leverage '\n              'annotated images and '\n              'structured text to learn '\n              'this task. The good '\n              'performance of these '\n              'models reveals that '\n              'spatial locations are to '\n              'a large extent '\n              'predictable from '\n              'implicit spatial '\n              'language. Crucially, the '\n              'models attain similar '\n              'performance in a '\n              'challenging generalized '\n              'setting, where the '\n              'object-relation-object '\n              'combinations (e.g.,\"man '\n              'walking dog\") have never '\n              'been seen before. Next, '\n              'we go one step further '\n              'by presenting the models '\n              'with unseen objects '\n              '(e.g., \"dog\"). In this '\n              'scenario, we show that '\n              'leveraging word '\n              'embeddings enables the '\n              'models to output '\n              'accurate spatial '\n              'predictions, proving '\n              'that the models acquire '\n              'solid common sense '\n              'spatial knowledge '\n              'allowing for such '\n              'generalization.'},\n {'Introduction': 'To provide machines '\n                  'with common sense is '\n                  'one of the major '\n                  'long term goals of '\n                  'artificial '\n                  'intelligence. Common '\n                  'sense knowledge '\n                  'regards knowledge '\n                  'that humans have '\n                  'acquired through a '\n                  'lifetime of '\n                  'experiences. It is '\n                  'crucial in language '\n                  'understanding '\n                  'because a lot of '\n                  'content needed for '\n                  'correct '\n                  'understanding is not '\n                  'expressed explicitly '\n                  'but resides in the '\n                  'mind of communicator '\n                  'and audience. In '\n                  'addition, humans '\n                  'rely on their common '\n                  'sense knowledge when '\n                  'performing a variety '\n                  'of tasks including '\n                  'interpreting images, '\n                  'navigation and '\n                  'reasoning, to name a '\n                  'few. Representing '\n                  'and understanding '\n                  'spatial knowledge '\n                  'are in fact '\n                  'imperative for any '\n                  'agent (human, animal '\n                  'or robot) that '\n                  'navigates in a '\n                  'physical world. In '\n                  'this paper, we are '\n                  'interested in '\n                  'acquiring spatial '\n                  'commonsense '\n                  'knowledge from '\n                  'language paired with '\n                  'visual data.'},\n {'Introduction': 'Computational and '\n                  'cognitive models '\n                  'often handle spatial '\n                  'representations as '\n                  'spatial templates or '\n                  'regions of '\n                  'acceptability for '\n                  'two objects under an '\n                  'explicit (a.k.a. '\n                  'deictic) spatial '\n                  'preposition such as '\n                  '\"on\", \"below\" or '\n                  '\"left\" '},\n {'Introduction': 'Predicting spatial '\n                  'templates for '\n                  'implicit '\n                  'relationships is '\n                  'notably more '\n                  'challenging than for '\n                  'explicit '\n                  'relationships. '\n                  'Firstly, whereas '\n                  'there are only a few '\n                  'tens of explicit '\n                  'spatial '\n                  'prepositions, there '\n                  'exist thousands of '\n                  'actions, entailing '\n                  'thus a drastic '\n                  'increase in the '\n                  'sparsity of (object '\n                  '1 , relationship, '\n                  'object 2 ) '\n                  'combinations. '\n                  'Secondly, the '\n                  'complexity of the '\n                  'task radically '\n                  'increases in '\n                  'implicit language. '\n                  'More precisely, '\n                  'while explicit '\n                  'spatial prepositions '\n                  '1 are highly '\n                  'deterministic about '\n                  'the spatial '\n                  'arrangements (e.g., '\n                  '(object 1 , below, '\n                  'object 2 ) '\n                  'unequivocally '\n                  'implies that object '\n                  '1 is relatively '\n                  'lower than object 2 '\n                  '), actions generally '\n                  'are not. E.g., the '\n                  'relative spatial '\n                  'configuration of '\n                  '\"man\" and the object '\n                  'is clearly distinct '\n                  'in (man, pulling, '\n                  'kite) than in (man, '\n                  'pulling, luggage) '\n                  'yet the action is '\n                  'the same. '\n                  'Contrarily, other '\n                  'relationships such '\n                  'as \"jumping\" are '\n                  'highly informative '\n                  'about the spatial '\n                  'template, i.e., in '\n                  '(object 1 , jumping, '\n                  'object 2 ), object 2 '\n                  'is in a lower '\n                  'position than object '\n                  '1 . Hence, unlike '\n                  'explicit '\n                  'relationships, '\n                  'predicting spatial '\n                  'layouts from '\n                  'implicit spatial '\n                  'language requires '\n                  'spatial common sense '\n                  'knowledge about the '\n                  'objects, actions and '\n                  'their interaction, '\n                  'which suggests the '\n                  'need of learning to '\n                  'compose the triplet '\n                  '(Subject, '\n                  'Relationship, '\n                  'Object) as a whole '\n                  'instead of learning '\n                  'a template for each '\n                  'Relationship.'},\n {'Introduction': 'To systematically '\n                  'study these '\n                  'questions, we '\n                  'propose the task of '\n                  'predicting the '\n                  'relative spatial '\n                  'locations of two '\n                  'objects given a '\n                  'structured text '\n                  'input (Subject, '\n                  'Relationship, '\n                  'Object). We '\n                  'introduce two simple '\n                  'neural-based models '\n                  'trained from '\n                  'annotated images '\n                  'that successfully '\n                  'address the two '\n                  'challenges of '\n                  'implicit spatial '\n                  'language discussed '\n                  'above. Our '\n                  'quantitative '\n                  'evaluation reveals '\n                  'that spatial '\n                  'templates can be '\n                  'reliably predicted '\n                  'from implicit '\n                  'spatial language-as '\n                  'accurately as from '\n                  'explicit spatial '\n                  'language. We also '\n                  'show that our models '\n                  'generalize well to '\n                  'templates of unseen '\n                  'combinations, e.g., '\n                  'predicting (man, '\n                  'riding, elephant) '\n                  'without having been '\n                  'exposed to such '\n                  'scene before, '\n                  'tackling thus the '\n                  'challenge of '\n                  'sparsity. '\n                  'Furthermore, by '\n                  'leveraging word '\n                  'embeddings, the '\n                  'models can correctly '\n                  'generalize to '\n                  'spatial templates '\n                  'with unseen words, '\n                  'e.g., predicting '\n                  '(man, riding, '\n                  'elephant) without '\n                  'having ever seen an '\n                  '\"elephant\" before. '\n                  'Since word '\n                  'embeddings capture '\n                  'attributes of '\n                  'objects '},\n {'Introduction': 'The rest of the '\n                  'paper is organized '\n                  'as follows. In Sect. '\n                  '2 we review related '\n                  'research. In Sect. 3 '\n                  'we first introduce '\n                  'the task of '\n                  'predicting spatial '\n                  'templates and then '\n                  'present two simple '\n                  'neural models. Then, '\n                  'in Sect. 4, we '\n                  'describe our '\n                  'experimental setup. '\n                  'In Sect. 5 we '\n                  'present and discuss '\n                  'our results. '\n                  'Finally, in Sect. 6 '\n                  'we summarize the '\n                  'contributions of '\n                  'this article.'},\n {'Related work': 'Spatial processing '\n                  'has drawn '\n                  'significant '\n                  'attention from the '\n                  'cognitive '},\n {'Related work': 'Spatial templates. '\n                  'Earlier approaches '\n                  'have predominantly '\n                  'considered '\n                  'rule-based spatial '\n                  'representations '},\n {'Related work': 'Additionally, while '\n                  'they build a spatial '\n                  'template for each '\n                  '(explicit) '\n                  'Relationship, we '\n                  'build a template for '\n                  'each (Subject, '\n                  'Relationship, '\n                  'Object) combination, '\n                  'allowing the '\n                  'template to be '\n                  'determined by the '\n                  'interaction/composition '\n                  'of the Subject, '\n                  'Relationship and '\n                  'Object instead of '\n                  'the Relationship '\n                  'alone. Additionally, '\n                  'the model from '},\n {'Related work': 'Leveraging spatial '\n                  'knowledge in tasks. '\n                  'It has been shown '\n                  'that knowledge of '\n                  'the spatial '\n                  'structure in images '\n                  'improves the task of '\n                  'image captioning '},\n {'Related work': 'Common sense spatial '\n                  'knowledge. '},\n {'Related work': 'Image generation. '\n                  'Although models that '\n                  'generate images from '\n                  'text exist (e.g., '\n                  'DRAW model '},\n {'Related work': 'As discussed, '\n                  'spatial knowledge '\n                  'can improve a wide '\n                  'range of tasks '},\n {'Related work': 'Obj.'},\n {'Related work': 'Subj.'},\n {'Related work': 'x y Elliott and '\n                  'Keller 2013). This '\n                  'suggests that the '\n                  'predictions of our '\n                  'models can be used '\n                  'as spatial common '\n                  'sense input for '\n                  'methods that rely on '\n                  'good spatial priors. '\n                  'Additionally, '\n                  'existing methods '\n                  'require the spatial '\n                  'information to be '\n                  'present in the data '\n                  'and lack the '\n                  'capacity to '\n                  'extrapolate/generalize. '\n                  'Thus, in this paper '\n                  'we focus, first, on '\n                  'showing that '\n                  'generalizing spatial '\n                  'arrangements to '\n                  'unseen objects and '\n                  'object-relation-object '\n                  'combinations is '\n                  'possible and, '\n                  'second, on ensuring '\n                  'that the predicted '\n                  'templates are '\n                  'accurate by '\n                  'performing several '\n                  'quantitative and '\n                  'qualitative '\n                  'evaluations.'},\n {'Related work': '3 Proposed task and '\n                  'model'},\n {'Proposed task': 'To learn spatial '\n                   'templates, we '\n                   'propose the task of '\n                   'predicting the 2D '\n                   'relative spatial '\n                   'arrangement of two '\n                   'objects under a '\n                   'relationship given '\n                   'a structured text '\n                   'input of the form '\n                   '(Subject, '\n                   'Relationship, '\n                   'Object)-henceforth '\n                   'abbreviated as (S, '\n                   'R, O). Let us '\n                   'denote the 2D '\n                   'coordinates of the '\n                   'center (\"c\") of the '\n                   \"Object's box as\"},\n {'Proposed task': 'y ∈ R are the '\n                   'horizontal and '\n                   'vertical components '\n                   'respectively. Let'},\n {'Proposed task': 'x , O b y ] ∈ R 2 '\n                   'be half of the '\n                   'width (O b x ) and '\n                   'half of the height '\n                   '(O b y ) of the '\n                   \"Object's box \"\n                   '(\"b\"). We employ a '\n                   'similar notation '\n                   'for the Subject (S '\n                   'c , S b ), and '\n                   'model predictions '\n                   'are denoted with a '\n                   'hat O c , O b . The '\n                   'task consists in '\n                   'predicting the '\n                   \"Object's location \"\n                   'and size'},\n {'Proposed task': 'given the '\n                   'structured text '\n                   'input (S, R, O) and '\n                   'the location S c '\n                   'and size S b of the '\n                   'Subject '},\n {'Proposed task': '2 Crucially, we '\n                   'notice that knowing '\n                   \"the Subject's \"\n                   'coordinates is not '\n                   'a requirement for '\n                   'generating '\n                   'templates (but only '\n                   'for evaluating '\n                   'against the ground '\n                   'truth) since '\n                   'inputting arbitrary '\n                   'coordinates (e.g., '\n                   'S c =[0.5, 0.5]) '\n                   'still enables '\n                   'visualizing '\n                   'relative object '\n                   'locations. '\n                   'Additionally, we '\n                   \"input the Subject's \"\n                   'size in order to '\n                   'provide a reference '\n                   'size to the model. '\n                   'However, we find '\n                   'that without this '\n                   'input, the model '\n                   'still learns to '\n                   'predict an \"average '\n                   'size\" for each '\n                   'Object.'},\n {'Proposed task': 'Hence, the goal is '\n                   'to answer common '\n                   'sense spatial '\n                   'questions such as '\n                   '\"if a man is '\n                   'feeding a horse, '\n                   'where is the man '\n                   'relative to the '\n                   'horse?\" or \"where '\n                   'would a child wear '\n                   'her shoes?\"'},\n {'Proposed models': 'To build a '\n                     'mapping from the '\n                     'input to the '\n                     'output in our '\n                     'task (Sect. 3.1) '\n                     'we propose two '\n                     'simple neural '\n                     'models ( '},\n {'Proposed models': 'are one-hot '\n                     'encodings of S, '\n                     'R, O (a.k.a. '\n                     'one-of-k '\n                     'encoding, i.e., a '\n                     'sparse vector '\n                     'with 0 everywhere '\n                     'except for a 1 at '\n                     'the position of '\n                     'the k-th word) '\n                     'and'},\n {'Proposed models': 'This layer '\n                     'represents '\n                     'objects and '\n                     'relationships as '\n                     'continuous '\n                     'features, '\n                     'enabling thus to '\n                     'introduce '\n                     'external '\n                     'knowledge of '\n                     'unseen objects as '\n                     'features. The '\n                     'embeddings are '\n                     'then concatenated '\n                     'with the Subject '\n                     'center S c and '\n                     'size'},\n {'Proposed models': 'which is inputted '\n                     'to a stack of '\n                     'hidden layers '\n                     'that compose S, R '\n                     'and O into a '\n                     'joint hidden '\n                     'representation z '\n                     'h :'},\n {'Proposed models': 'where f (•) is an '\n                     'element-wise '\n                     'non-linear '\n                     'function and W h '\n                     'and b h are the '\n                     'weight matrix and '\n                     'bias '\n                     'respectively. 3 '\n                     '(ii) Output and '\n                     'loss. We consider '\n                     'two different '\n                     'possibilities for '\n                     'the outputŷ that '\n                     'follows '\n                     'immediately after '\n                     'the last layer: '},\n {'Proposed models': 'These models are '\n                     'conceptually '\n                     'different and '\n                     'have different '\n                     'capabilities. '\n                     'While the REG '\n                     'model outputs '\n                     '\"crisp\" pointwise '\n                     'predictions, PIX '\n                     'can model more '\n                     'diffuse spatial '\n                     'templates where '\n                     'the location of '\n                     'the object has '\n                     'more variability, '\n                     'e.g., in (man, '\n                     'flying, kite) the '\n                     '\"kite\" can easily '\n                     'move around. '\n                     'Notice that in '\n                     'contrast with '\n                     'convolutional '\n                     'neural networks '\n                     '(CNNs) our '\n                     'approach does not '\n                     'make use of the '\n                     'image pixels ( '},\n {'Experimental setup': 'We employ a '\n                        '10-fold '\n                        'cross-validation '\n                        '(CV) setting. '\n                        'Data are '\n                        'randomly split '\n                        'into 10 '\n                        'disjoint parts '\n                        'and 10% is '\n                        'employed for '\n                        'testing and '\n                        '90% for '\n                        'training, '\n                        'repeating this '\n                        'for each of '\n                        'the 10 folds. '\n                        'Reported '\n                        'results are '\n                        'averages over '\n                        'the 10 folds.'},\n {'Visual Genome data set': 'We use the '\n                            'Visual '\n                            'Genome '\n                            'dataset '},\n {'Visual Genome data set': 'dog, '\n                            'catches, '\n                            'frisbee '\n                            'boy, '\n                            'feeds, '\n                            'giraffe '\n                            'man, '\n                            'throws, '\n                            'frisbee '\n                            'cat, wears '\n                            'glasses '},\n {'Evaluation sets': 'We consider the '\n                     'following subsets '\n                     'of the Visual '\n                     'Genome data to '\n                     'evaluate '\n                     'performance. (i) '\n                     'Raw data: Simply '\n                     'the unfiltered '\n                     'instances from '\n                     'the Visual Genome '\n                     'data (Sect. 4.1). '\n                     'This set contains '\n                     'a substantial '\n                     'proportion of '\n                     'meaningless '\n                     '(e.g., (nose, '\n                     'almost, '\n                     'touching)) and '\n                     'irrelevant (e.g., '\n                     '(sign, says, gate '\n                     '2)) instances. '\n                     '(ii) Generalized '\n                     'Triplets: We pick '\n                     'at random 100 '\n                     'combinations (S, '\n                     'R, O) among the '\n                     '1,000 most '\n                     'frequent implicit '\n                     'combinations in '\n                     'Visual Genome. '\n                     'This yields ∼25K '\n                     'instances such as '\n                     '(person, holding, '\n                     'racket), (man, '\n                     'flying, kite), '\n                     'etc. 4 (iii) '\n                     'Generalized '\n                     'Words: We '\n                     'randomly choose '\n                     '25 objects (e.g., '\n                     '\"woman\", \"apple\", '\n                     'etc.) 5 among the '\n                     '100 most frequent '\n                     'objects in Visual '\n                     'Genome and take '\n                     'all the instances '\n                     '(∼130K) that '\n                     'contain any of '\n                     'these words. For '\n                     'example, since '\n                     '\"apple\" is in our '\n                     'list, e.g., (cat, '\n                     'sniffing, apple) '\n                     'is kept. '},\n {'Evaluation sets': 'When enforcing '\n                     'generalization '\n                     'conditions in our '\n                     'experiments, all '\n                     'combinations from '\n                     'sets (ii) and '\n                     '(iii) are removed '\n                     'from the training '\n                     'data to prevent '\n                     'the model from '\n                     'seeing them. 7 '\n                     'Even without '\n                     'imposing '\n                     'generalization '\n                     'conditions (or '\n                     'when testing with '\n                     'Raw data), '\n                     'reported results '\n                     'are always on '\n                     'unseen '\n                     'instances-yet the '\n                     'combinations (S, '\n                     'R, O) may have '\n                     'been seen during '\n                     'training (e.g., '\n                     'in different '\n                     'images). All sets '\n                     'above contain '\n                     'exclusively '\n                     'implicit spatial '\n                     'language, '\n                     'although an '\n                     'analogous version '\n                     'of the Raw set '\n                     'where R are '\n                     'explicit spatial '\n                     'prepositions is '\n                     'also considered '\n                     'in our '\n                     'experiments.'},\n {'Data pre-processing': 'The '\n                         'coordinates '\n                         'of bounding '\n                         'boxes in the '\n                         'images are '\n                         'normalized by '\n                         'the width and '\n                         'height of the '\n                         'image. Thus, '\n                         'S c , O c ∈ '\n                         '[0, 1] 2 . '\n                         'Additionally, '\n                         'we notice '\n                         'that the '\n                         'distinction '\n                         'between left '\n                         'and right is '\n                         'arbitrary '\n                         'regarding the '\n                         'semantics of '\n                         'the image '},\n {'Evaluation metrics': 'The REG model '\n                        'directly '\n                        'outputs Object '\n                        'coordinates, '\n                        'while PIX '\n                        'outputs 2D '\n                        'heatmaps. We '\n                        'however enable '\n                        'evaluating the '\n                        'PIX model with '\n                        'regression/classification '\n                        'metrics by '\n                        'taking the '\n                        'point of '\n                        'maximum '\n                        'activation (or '\n                        'their average, '\n                        'if there are '\n                        'many) as the '\n                        'Object center '\n                        'O c . The '\n                        'predicted '\n                        'Object size O '\n                        'b is not '\n                        'estimated. We '\n                        'use the '\n                        'following '\n                        'performance '\n                        'metrics.'},\n {'Evaluation metrics': '(A) '\n                        'Intersection '\n                        'over Union '},\n {'Evaluation metrics': 'where B O and '\n                        'B O are the '\n                        'predicted and '\n                        'ground truth '\n                        'Object '\n                        'bounding '\n                        'boxes, '\n                        'respectively. '\n                        'If the IoU is '\n                        'larger than '\n                        '50%, the '\n                        'prediction is '\n                        'counted as '\n                        'correct. It '\n                        'must be noted '\n                        'that our '\n                        'setting is not '\n                        'comparable to '\n                        'object '\n                        'detection (nor '\n                        'our results) '\n                        'since we do '\n                        'not employ the '\n                        'image as input '\n                        '(but text) and '\n                        'thus we cannot '\n                        'leverage the '\n                        'pixels to '\n                        'predict the '\n                        \"Object's \"\n                        'location.'},\n {'Evaluation metrics': '(B) '\n                        'Regression. We '\n                        'consider '\n                        'standard '\n                        'regression '\n                        'metrics. '},\n {'Evaluation metrics': '(D) Pixel '\n                        '(macro) '\n                        'accuracy. The '\n                        'IoU on pixels '\n                        'is equivalent '\n                        'to binary '\n                        'pixel '\n                        'accuracy. '\n                        'However, this '\n                        'is not a good '\n                        'measure here, '\n                        'where class 1 '\n                        '(Object box) '\n                        'comprises, on '\n                        'average, only '\n                        '5% of the '\n                        'pixels. Thus, '\n                        'a constant '\n                        'prediction of '\n                        'zeros '\n                        'everywhere '\n                        'would obtain '\n                        '95% accuracy. '\n                        'Hence, we '\n                        'consider '\n                        'macro-averaged '\n                        'pixel '\n                        'accuracy, '\n                        'a.k.a. mean '\n                        'IoU (mIoU) '},\n {'Word embeddings': 'We use '\n                     '300-dimensional '\n                     'GloVe word '\n                     'embeddings '\n                     '(Pennington, '\n                     'Socher, and '\n                     'Manning 2014) '\n                     'pre-trained on '\n                     'the Common Crawl '\n                     'corpus '\n                     '(consisting of '\n                     '840B-tokens), '\n                     'which we obtain '\n                     \"from the authors' \"\n                     'website. 8'},\n {'Model hyperparameters and implementation': 'Our '\n                                              'experiments '\n                                              'are '\n                                              'implemented '\n                                              'in '\n                                              'Python '\n                                              '2.7 '\n                                              'and '\n                                              'we '\n                                              'use '\n                                              'Keras '\n                                              'deep '\n                                              'learning '\n                                              'framework '\n                                              'for '\n                                              'our '\n                                              'models '},\n {'Results and discussion': 'We '\n                            'consider '\n                            'the '\n                            'evaluation '\n                            'sets from '\n                            'Sect. 4.2 '\n                            'and the '\n                            'following '\n                            'variations '\n                            'of PIX and '\n                            'REG models '\n                            '(Sect. '\n                            '3.2). The '\n                            'subindex '\n                            'EMB '\n                            'denotes a '\n                            'model that '\n                            'employs '\n                            'GloVe '\n                            'embeddings '\n                            'and RND a '\n                            'model with '\n                            'embeddings '\n                            'randomly '\n                            'drawn from '\n                            'a '\n                            'dimension-wise '\n                            'normal '\n                            'distribution '\n                            'of mean '\n                            '(μ) and '\n                            'standard '\n                            'deviation '\n                            '(σ) equal '\n                            'to those '\n                            'of the '\n                            'GloVe '\n                            'embeddings, '\n                            'preserving '\n                            'the '\n                            'original '\n                            'dimensionality '\n                            '(d=300). A '\n                            'third type '\n                            'employs '\n                            'one-hot '\n                            'vectors '\n                            '(1H). We '\n                            'additionally  '\n                            'consider a '\n                            'control '\n                            'method '\n                            '(ctrl) '\n                            'that '\n                            'outputs '\n                            'random '\n                            'normal '\n                            'predictions '\n                            'of μ and σ '\n                            'equal to '\n                            'the '\n                            'dimension-wise '\n                            'mean and '\n                            'standard '\n                            'deviation '\n                            'of the '\n                            'training '\n                            'targets. '\n                            'We test '\n                            'statistical '\n                            'significance '\n                            'with a '\n                            'Friedman '\n                            'rank test '\n                            'and post '\n                            'hoc '\n                            'Nemenyi '\n                            'tests on '\n                            'the '\n                            'results of '\n                            'the 10 '\n                            'folds. We '\n                            'indicate '\n                            'with an '\n                            'asterisk * '\n                            'in the '\n                            'tables '\n                            'when a '\n                            'method is '\n                            'significantly '\n                            'better '\n                            'than the '\n                            'rest (p < '\n                            '0.01) '\n                            'within the '\n                            'same model '\n                            '(PIX or '\n                            'REG). '},\n {'Qualitative evaluation (spatial templates)': 'To '\n                                                'ensure '\n                                                'that '\n                                                'model '\n                                                'predictions '\n                                                'are '\n                                                'meaningful '\n                                                'and '\n                                                'interpretable, '\n                                                'we '\n                                                'further '\n                                                'validate '\n                                                'the '\n                                                'quantitative '\n                                                'results '\n                                                'above '\n                                                'with '\n                                                'a '\n                                                'qualitative '\n                                                'evaluation. '\n                                                'Notice '\n                                                'that '\n                                                'all '\n                                                'plots '\n                                                'in '},\n {'Interpretation of model weights': 'We '\n                                     'study '\n                                     'how '\n                                     'the '\n                                     'weights '\n                                     'of '\n                                     'the '\n                                     'model '\n                                     'provide '\n                                     'insight '\n                                     'into '\n                                     'the '\n                                     'spatial '\n                                     'properties '\n                                     'of '\n                                     'words. '\n                                     'To '\n                                     'obtain '\n                                     'more '\n                                     'interpretable '\n                                     'weights, '\n                                     'we '\n                                     'learn '\n                                     'a '\n                                     'REG '\n                                     '9 '\n                                     'model '\n                                     'without '\n                                     'hidden '\n                                     'layers, '\n                                     'resulting '\n                                     'in '\n                                     'only '\n                                     'an '\n                                     'embedding '\n                                     'layer '\n                                     'followed '\n                                     'by '\n                                     'a '\n                                     'linear '\n                                     'output '\n                                     'layerŷ '\n                                     '= '\n                                     'W '\n                                     'out '\n                                     'u '\n                                     '+ '\n                                     'b '\n                                     'out '\n                                     ', '\n                                     'where'},\n {'Interpretation of model weights': 'By '\n                                     'using '\n                                     'one-hot '\n                                     'encodings '\n                                     'w '\n                                     'S '\n                                     ', '\n                                     'w '\n                                     'R '\n                                     ', '\n                                     'w '\n                                     'O '\n                                     ', '\n                                     'the '\n                                     'concatenation '\n                                     'layer '\n                                     'u '\n                                     'becomes '\n                                     'of '\n                                     'size '\n                                     '|V '\n                                     'S '\n                                     '| '\n                                     '+ '\n                                     '|V '\n                                     'R '\n                                     '| '\n                                     '+ '\n                                     '|V '\n                                     'O '\n                                     '| '\n                                     '+ '\n                                     '2 '\n                                     '+ '\n                                     '2. '\n                                     'E.g., '\n                                     'if '\n                                     '\"wearing\" '\n                                     'has '\n                                     'one-hot '\n                                     'index '\n                                     'j '\n                                     'in '\n                                     'the '\n                                     \"Relationships' \"\n                                     'vocabulary '\n                                     '(V '\n                                     'R '\n                                     '), '\n                                     'its '\n                                     'index '\n                                     'in '\n                                     'the '\n                                     'concatenation '\n                                     'layer '\n                                     'is '\n                                     '|V '\n                                     'S '\n                                     '| '\n                                     '+ '\n                                     'j. '\n                                     'The '\n                                     'product '\n                                     'W '\n                                     'out '\n                                     'u '\n                                     'is '\n                                     'a '\n                                     '4-dimensional '\n                                     'vector, '\n                                     'where '\n                                     'its '\n                                     'i-th '\n                                     'component '\n                                     'is '\n                                     'the '\n                                     'product '\n                                     'of '\n                                     'the '\n                                     'i-th '\n                                     'row '\n                                     'of '\n                                     'W '\n                                     'out '\n                                     'with '\n                                     'the '\n                                     'vector '\n                                     'u. '\n                                     'Thus, '\n                                     'the '\n                                     'component '\n                                     '|V '\n                                     'S '\n                                     '|+j '\n                                     'of '\n                                     'the '\n                                     'i-th '\n                                     'row '\n                                     'of '\n                                     'W '\n                                     'out '\n                                     'gives '\n                                     'us '\n                                     'the '\n                                     'influence '\n                                     'of '\n                                     'the '\n'j-th '\n                                     'relationship '\n                                     '(i.e., '\n                                     '\"wearing\") '\n                                     'on '\n                                     'the '\n                                     'i-th '\n                                     'dimension '\n                                     'of '\n                                     'the '\n                                     'outputŷ '\n                                     '=   '},\n {'Conclusions': 'Overall, this paper '\n                 'provides insight into '\n                 'the fundamental '\n                 'differences between '\n                 'implicit and explicit '\n                 'spatial language and '\n                 'extends the concept '\n                 'of spatial templates '\n                 'to implicit spatial '\n                 'language, the '\n                 'understanding of '\n                 'which requires common '\n                 'sense spatial '\n                 'knowledge about '\n                 'objects and actions. '\n                 'We define the task of '\n                 'predicting relative '\n                 'spatial arrangements '\n                 'of two objects under '\n                 'a relationship and '\n                 'present two '\n                 'embedding-based '\n                 'neural models that '\n                 'attain promising '\n                 'performance, proving '\n                 'that spatial '\n                 'templates can be '\n                 'accurately predicted '\n                 'from implicit spatial '\n                 'language. Remarkably, '\n                 'our models generalize '\n                 'well, predicting '\n                 'correctly unseen '\n                 '(generalized) '\n                 'object-relationshipobject '\n                 'combinations. '\n                 'Furthermore, the '\n                 'acquired common sense '\n                 'spatial '\n                 'knowledge-aided with '\n                 'word embeddingsallows '\n                 'the model to '\n                 'correctly predict '\n                 'templates for unseen '\n                 'words. Finally, we '\n                 'show that the weights '\n                 'of the model provide '\n                 'great insight into '\n                 'the spatial '\n                 'connotations of '\n                 'words.'},\n {'Conclusions': 'A first limitation of '\n                 'our approach is the '\n                 'fully supervised '\n                 'setting where the '\n                 'models are trained '\n                 'using images with '\n                 'detected ground truth '\n                 'objects and parsed '\n                 'text-which aims at '\n                 'keeping the design '\n                 'clean in this first '\n                 'study on implicit '\n                 'spatial templates. '\n                 'Notice however that '\n                 'methods to '\n                 'automatically parse '\n                 'images and text '\n                 'exist. In future '\n                 'work, we aim at '\n                 'implementing our '\n                 'approach in a weakly '\n                 'supervised setting. A '\n                 'second limitation is '\n                 'the 2D spatial '\n                 'treatment of the '\n                 'actual 3D world. It '\n                 'is worth noting '\n                 'however, that our '\n                 'models (PIX and REG) '\n                 'and setting trivially '\n                 'generalize to 3D if '\n                 'appropriate data are '\n                 'available.'}]\n"
    }
   ],
   "source": [
    "mydoc = minidom.parse(\"source.xml\")\n",
    "divs = mydoc.getElementsByTagName(\"div\")\n",
    "for div in divs: #<div>\n",
    "    if(div.parentNode.nodeName == 'abstract'):\n",
    "        for elem in div.childNodes:\n",
    "            if(elem.nodeName == 'p'):\n",
    "                headings_para.append({\"Abstract\": elem.firstChild.data})\n",
    "\n",
    "    if(div.parentNode.nodeName == \"body\"):\n",
    "        try:\n",
    "            x = 'works'\n",
    "        except AttributeError:\n",
    "            continue\n",
    "        else:\n",
    "            for elem in div.childNodes:\n",
    "                if(not elem.nodeName =='formula'):\n",
    "                    if(elem.nodeName == 'head'):\n",
    "                        section = elem.firstChild.data\n",
    "                        continue\n",
    "                    if(elem.nodeName == 'p'):\n",
    "                        headings_para.append({section : elem.firstChild.data})\n",
    "\n",
    "pp = pprint.PrettyPrinter(width=41, depth = 5, compact=True)\n",
    "pp.pprint(headings_seq)\n",
    "pp.pprint(headings_name)\n",
    "pp.pprint(headings_para)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}