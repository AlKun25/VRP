{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[{'n': '1.'}, {'n': '2.'}, {'n': '3.'},\n {'n': '3.1.'}, {'n': '3.2.'},\n {'n': '3.3.'}, {'n': '4.'}, {'n': '5.'},\n {'n': '6.'}, {'n': '6.1.'},\n {'n': '6.2.'}, {}, {'n': '6.3.'},\n {'n': '6.4.'}, {'n': '7.'}]\n['Introduction', 'Background', 'Method',\n 'Multimodal Fully Convolutional '\n 'Network',\n 'Text Embedding Map',\n 'Unsupervised Tasks',\n 'Synthetic Document Data',\n 'Implementation Details', 'Experiments',\n 'Ablation Experiment on Model '\n 'Architecture',\n 'Adding Textual Information', 'Methods',\n 'Unsupervised Learning Tasks',\n 'Comparisons with Prior Art',\n 'Conclusion']\n[{'Introduction': 'We proposed a '\n                  'multimodal fully '\n                  'convolutional '\n                  'network (MFCN) for '\n                  'document semantic '\n                  'structure '\n                  'extraction. The '\n                  'proposed model uses '\n                  'both visual and '\n                  'textual information. '\n                  'Moreover, we propose '\n                  'an efficient '\n                  'synthetic data '\n                  'generation method '\n                  'that yields '\n                  'per-pixel '\n                  'ground-truth. Our '\n                  'unsupervised '\n                  'auxiliary tasks help '\n                  'boost performance '\n                  'tapping into '\n                  'unlabeled real '\n                  'documents, '\n                  'facilitating better '\n                  'representation '\n                  'learning. We showed '\n                  'that both the '\n                  'multimodal approach '\n                  'and unsupervised '\n                  'tasks can help '\n                  'improve performance. '\n                  'Our results indicate '\n                  'that we have '\n                  'improved the state '\n                  'of the art on '\n                  'previously '\n                  'established '\n                  'benchmarks. In '\n                  'addition, we are '\n                  'publicly providing '\n                  'the large synthetic '\n                  'dataset (135,000 '\n                  'pages) as well as a '\n                  'new benchmark '\n                  'dataset: DSSE-200.'},\n {'Background': 'We proposed a '\n                'multimodal fully '\n                'convolutional network '\n                '(MFCN) for document '\n                'semantic structure '\n                'extraction. The '\n                'proposed model uses '\n                'both visual and '\n                'textual information. '\n                'Moreover, we propose '\n                'an efficient synthetic '\n                'data generation method '\n                'that yields per-pixel '\n                'ground-truth. Our '\n                'unsupervised auxiliary '\n                'tasks help boost '\n                'performance tapping '\n                'into unlabeled real '\n                'documents, '\n                'facilitating better '\n                'representation '\n                'learning. We showed '\n                'that both the '\n                'multimodal approach '\n                'and unsupervised tasks '\n                'can help improve '\n                'performance. Our '\n                'results indicate that '\n                'we have improved the '\n                'state of the art on '\n                'previously established '\n                'benchmarks. In '\n                'addition, we are '\n                'publicly providing the '\n                'large synthetic '\n                'dataset (135,000 '\n                'pages) as well as a '\n                'new benchmark dataset: '\n                'DSSE-200.'},\n {'Method': 'We proposed a multimodal '\n            'fully convolutional '\n            'network (MFCN) for '\n            'document semantic '\n            'structure extraction. The '\n            'proposed model uses both '\n            'visual and textual '\n            'information. Moreover, we '\n            'propose an efficient '\n            'synthetic data generation '\n            'method that yields '\n            'per-pixel ground-truth. '\n            'Our unsupervised auxiliary '\n            'tasks help boost '\n            'performance tapping into '\n            'unlabeled real documents, '\n            'facilitating better '\n            'representation learning. '\n            'We showed that both the '\n            'multimodal approach and '\n            'unsupervised tasks can '\n            'help improve performance. '\n            'Our results indicate that '\n            'we have improved the state '\n            'of the art on previously '\n            'established benchmarks. In '\n            'addition, we are publicly '\n            'providing the large '\n            'synthetic dataset (135,000 '\n            'pages) as well as a new '\n            'benchmark dataset: '\n            'DSSE-200.'},\n {'Multimodal Fully Convolutional Network': 'We '\n                                            'proposed '\n                                            'a '\n                                            'multimodal '\n                                            'fully '\n                                            'convolutional '\n                                            'network '\n                                            '(MFCN) '\n                                            'for '\n                                            'document '\n                                            'semantic '\n                                            'structure '\n                                            'extraction. '\n                                            'The '\n                                            'proposed '\n                                            'model '\n                                            'uses '\n                                            'both '\n                                            'visual '\n                                            'and '\n                                            'textual '\n                                            'information. '\n                                            'Moreover, '\n                                            'we '\n                                            'propose '\n                                            'an '\n                                            'efficient '\n                                            'synthetic '\n                                            'data '\n                                            'generation '\n                                            'method '\n                                            'that '\n                                            'yields '\n                                            'per-pixel '\n                                            'ground-truth. '\n                                            'Our '\n                                            'unsupervised '\n                                            'auxiliary '\n                                            'tasks '\n                                            'help '\n                                            'boost '\n                                            'performance '\n                                            'tapping '\n                                            'into '\n                                            'unlabeled '\n                                            'real '\n                                            'documents, '\n                                            'facilitating '\n                                            'better '\n                                            'representation '\n                                            'learning. '\n                                            'We '\n                                            'showed '\n                                            'that '\n                                            'both '\n                                            'the '\n                                            'multimodal '\n                                            'approach '\n                                            'and '\n                                            'unsupervised '\n                                            'tasks '\n                                            'can '\n                                            'help '\n                                            'improve '\n                                            'performance. '\n                                            'Our '\n                                            'results '\n                                            'indicate '\n                                            'that '\n                                            'we '\n                                            'have '\n                                            'improved '\n                                            'the '\n                                            'state '\n                                            'of '\n                                            'the '\n                                            'art '\n                                            'on '\n                                            'previously '\n                                            'established '\n                                            'benchmarks. '\n                                            'In '\n                                            'addition, '\n                                            'we '\n                                            'are '\n                                            'publicly '\n                                            'providing '\n                                            'the '\n                                            'large '\n                                            'synthetic '\n                                            'dataset '\n                                            '(135,000 '\n                                            'pages) '\n                                            'as '\n                                            'well '\n                                            'as '\n                                            'a '\n                                            'new '\n                                            'benchmark '\n                                            'dataset: '\n                                            'DSSE-200.'},\n {'Text Embedding Map': 'We proposed a '\n                        'multimodal '\n                        'fully '\n                        'convolutional '\n                        'network (MFCN) '\n                        'for document '\n                        'semantic '\n                        'structure '\n                        'extraction. '\n                        'The proposed '\n                        'model uses '\n                        'both visual '\n                        'and textual '\n                        'information. '\n                        'Moreover, we '\n                        'propose an '\n                        'efficient '\n                        'synthetic data '\n                        'generation '\n                        'method that '\n                        'yields '\n                        'per-pixel '\n                        'ground-truth. '\n                        'Our '\n                        'unsupervised '\n                        'auxiliary '\n                        'tasks help '\n                        'boost '\n                        'performance '\n                        'tapping into '\n                        'unlabeled real '\n                        'documents, '\n                        'facilitating '\n                        'better '\n                        'representation '\n                        'learning. We '\n                        'showed that '\n                        'both the '\n                        'multimodal '\n                        'approach and '\n                        'unsupervised '\n                        'tasks can help '\n                        'improve '\n                        'performance. '\n                        'Our results '\n                        'indicate that '\n                        'we have '\n                        'improved the '\n                        'state of the '\n                        'art on '\n                        'previously '\n                        'established '\n                        'benchmarks. In '\n                        'addition, we '\n                        'are publicly '\n                        'providing the '\n                        'large '\n                        'synthetic '\n                        'dataset '\n                        '(135,000 '\n                        'pages) as well '\n                        'as a new '\n                        'benchmark '\n                        'dataset: '\n                        'DSSE-200.'},\n {'Unsupervised Tasks': 'We proposed a '\n                        'multimodal '\n                        'fully '\n                        'convolutional '\n                        'network (MFCN) '\n                        'for document '\n                        'semantic '\n                        'structure '\n                        'extraction. '\n                        'The proposed '\n                        'model uses '\n                        'both visual '\n                        'and textual '\n                        'information. '\n                        'Moreover, we '\n                        'propose an '\n                        'efficient '\n                        'synthetic data '\n                        'generation '\n                        'method that '\n                        'yields '\n                        'per-pixel '\n                        'ground-truth. '\n                        'Our '\n                        'unsupervised '\n                        'auxiliary '\n                        'tasks help '\n                        'boost '\n                        'performance '\n                        'tapping into '\n                        'unlabeled real '\n                        'documents, '\n                        'facilitating '\n                        'better '\n                        'representation '\n                        'learning. We '\n                        'showed that '\n                        'both the '\n                        'multimodal '\n                        'approach and '\n                        'unsupervised '\n                        'tasks can help '\n                        'improve '\n                        'performance. '\n                        'Our results '\n                        'indicate that '\n                        'we have '\n                        'improved the '\n                        'state of the '\n                        'art on '\n                        'previously '\n                        'established '\n                        'benchmarks. In '\n                        'addition, we '\n                        'are publicly '\n                        'providing the '\n                        'large '\n                        'synthetic '\n                        'dataset '\n                        '(135,000 '\n                        'pages) as well '\n                        'as a new '\n                        'benchmark '\n                        'dataset: '\n                        'DSSE-200.'},\n {'Synthetic Document Data': 'We '\n                             'proposed '\n                             'a '\n                             'multimodal '\n                             'fully '\n                             'convolutional '\n                             'network '\n                             '(MFCN) '\n                             'for '\n                             'document '\n                             'semantic '\n                             'structure '\n                             'extraction. '\n                             'The '\n                             'proposed '\n                             'model '\n                             'uses both '\n                             'visual '\n                             'and '\n                             'textual '\n                             'information. '\n                             'Moreover, '\n                             'we '\n                             'propose '\n                             'an '\n                             'efficient '\n                             'synthetic '\n                             'data '\n                             'generation '\n                             'method '\n                             'that '\n                             'yields '\n                             'per-pixel '\n                             'ground-truth. '\n                             'Our '\n                             'unsupervised '\n                             'auxiliary '\n                             'tasks '\n                             'help '\n                             'boost '\n                             'performance '\n                             'tapping '\n                             'into '\n                             'unlabeled '\n                             'real '\n                             'documents, '\n                             'facilitating '\n                             'better '\n                             'representation '\n                             'learning. '\n                             'We showed '\n                             'that both '\n                             'the '\n                             'multimodal '\n                             'approach '\n                             'and '\n                             'unsupervised '\n                             'tasks can '\n                             'help '\n                             'improve '\n                             'performance. '\n                             'Our '\n                             'results '\n                             'indicate '\n                             'that we '\n                             'have '\n                             'improved '\n                             'the state '\n                             'of the '\n                             'art on '\n                             'previously '\n                             'established '\n                             'benchmarks. '\n                             'In '\n                             'addition, '\n                             'we are '\n                             'publicly '\n                             'providing '\n                             'the large '\n                             'synthetic '\n                             'dataset '\n                             '(135,000 '\n                             'pages) as '\n                             'well as a '\n                             'new '\n                             'benchmark '\n                             'dataset: '\n                             'DSSE-200.'},\n {'Implementation Details': 'We '\n                            'proposed a '\n                            'multimodal '\n                            'fully '\n                            'convolutional '\n                            'network '\n                            '(MFCN) for '\n                            'document '\n                            'semantic '\n                            'structure '\n                            'extraction. '\n                            'The '\n                            'proposed '\n                            'model uses '\n                            'both '\n                            'visual and '\n                            'textual '\n                            'information. '\n                            'Moreover, '\n                            'we propose '\n                            'an '\n                            'efficient '\n                            'synthetic '\n                            'data '\n                            'generation '\n                            'method '\n                            'that '\n                            'yields '\n                            'per-pixel '\n                            'ground-truth. '\n                            'Our '\n                            'unsupervised '\n                            'auxiliary '\n                            'tasks help '\n                            'boost '\n                            'performance '\n                            'tapping '\n                            'into '\n                            'unlabeled '\n                            'real '\n                            'documents, '\n                            'facilitating '\n                            'better '\n                            'representation '\n                            'learning. '\n                            'We showed '\n                            'that both '\n                            'the '\n                            'multimodal '\n                            'approach '\n                            'and '\n                            'unsupervised '\n                            'tasks can '\n                            'help '\n                            'improve '\n                            'performance. '\n                            'Our '\n                            'results '\n                            'indicate '\n                            'that we '\n                            'have '\n                            'improved '\n                            'the state '\n                            'of the art '\n                            'on '\n                            'previously '\n                            'established '\n                            'benchmarks. '\n                            'In '\n                            'addition, '\n                            'we are '\n                            'publicly '\n                            'providing '\n                            'the large '\n                            'synthetic '\n                            'dataset '\n                            '(135,000 '\n                            'pages) as '\n                            'well as a '\n                            'new '\n                            'benchmark '\n                            'dataset: '\n                            'DSSE-200.'},\n {'Experiments': 'We proposed a '\n                 'multimodal fully '\n                 'convolutional network '\n                 '(MFCN) for document '\n                 'semantic structure '\n                 'extraction. The '\n                 'proposed model uses '\n                 'both visual and '\n                 'textual information. '\n                 'Moreover, we propose '\n                 'an efficient '\n                 'synthetic data '\n                 'generation method '\n                 'that yields per-pixel '\n                 'ground-truth. Our '\n                 'unsupervised '\n                 'auxiliary tasks help '\n                 'boost performance '\n                 'tapping into '\n                 'unlabeled real '\n                 'documents, '\n                 'facilitating better '\n                 'representation '\n                 'learning. We showed '\n                 'that both the '\n                 'multimodal approach '\n                 'and unsupervised '\n                 'tasks can help '\n                 'improve performance. '\n                 'Our results indicate '\n                 'that we have improved '\n                 'the state of the art '\n                 'on previously '\n                 'established '\n                 'benchmarks. In '\n                 'addition, we are '\n                 'publicly providing '\n                 'the large synthetic '\n                 'dataset (135,000 '\n                 'pages) as well as a '\n                 'new benchmark '\n                 'dataset: DSSE-200.'},\n {'Ablation Experiment on Model Architecture': 'We '\n                                               'proposed '\n                                               'a '\n                                               'multimodal '\n                                               'fully '\n                                               'convolutional '\n                                               'network '\n                                               '(MFCN) '\n                                               'for '\n                                               'document '\n                                               'semantic '\n                                               'structure '\n                                               'extraction. '\n                                               'The '\n                                               'proposed '\n                                               'model '\n                                               'uses '\n                                               'both '\n                                               'visual '\n                                               'and '\n                                               'textual '\n                                               'information. '\n                                               'Moreover, '\n                                               'we '\n                                               'propose '\n                                               'an '\n                                               'efficient '\n                                               'synthetic '\n                                               'data '\n                                               'generation '\n                                               'method '\n                                               'that '\n                                               'yields '\n                                               'per-pixel '\n                                               'ground-truth. '\n                                               'Our '\n                                               'unsupervised '\n                                               'auxiliary '\n                                               'tasks '\n                                               'help '\n                                               'boost '\n                                               'performance '\n                                               'tapping '\n                                               'into '\n                                               'unlabeled '\n                                               'real '\n                                               'documents, '\n                                               'facilitating '\n                                               'better '\n                                               'representation '\n                                               'learning. '\n                                               'We '\n                                               'showed '\n                                               'that '\n                                               'both '\n                                               'the '\n                                               'multimodal '\n                                               'approach '\n                                               'and '\n                                               'unsupervised '\n                                               'tasks '\n                                               'can '\n                                               'help '\n                                               'improve '\n                                               'performance. '\n                                               'Our '\n                                               'results '\n                                               'indicate '\n                                               'that '\n                                               'we '\n                                               'have '\n                                               'improved '\n                                               'the '\n                                               'state '\n                                               'of '\n                                               'the '\n                                               'art '\n                                               'on '\n                                               'previously '\n                                               'established '\n                                               'benchmarks. '\n                                               'In '\n                                               'addition, '\n                                               'we '\n                                               'are '\n                                               'publicly '\n                                               'providing '\n                                               'the '\n                                               'large '\n                                               'synthetic '\n                                               'dataset '\n                                               '(135,000 '\n                                               'pages) '\n                                               'as '\n                                               'well '\n                                               'as '\n                                               'a '\n                                               'new '\n                                               'benchmark '\n                                               'dataset: '\n                                               'DSSE-200.'},\n {'Adding Textual Information': 'We '\n                                'proposed '\n                                'a '\n                                'multimodal '\n                                'fully '\n                                'convolutional '\n                                'network '\n                                '(MFCN) '\n                                'for '\n                                'document '\n                                'semantic '\n                                'structure '\n                                'extraction. '\n                                'The '\n                                'proposed '\n                                'model '\n                                'uses '\n                                'both '\n                                'visual '\n                                'and '\n                                'textual '\n                                'information. '\n                                'Moreover, '\n                                'we '\n                                'propose '\n                                'an '\n                                'efficient '\n                                'synthetic '\n                                'data '\n                                'generation '\n                                'method '\n                                'that '\n                                'yields '\n                                'per-pixel '\n                                'ground-truth. '\n                                'Our '\n                                'unsupervised '\n                                'auxiliary '\n                                'tasks '\n                                'help '\n                                'boost '\n                                'performance '\n                                'tapping '\n                                'into '\n                                'unlabeled '\n                                'real '\n                                'documents, '\n                                'facilitating '\n                                'better '\n                                'representation '\n                                'learning. '\n                                'We '\n                                'showed '\n                                'that '\n                                'both '\n                                'the '\n                                'multimodal '\n                                'approach '\n                                'and '\n                                'unsupervised '\n                                'tasks '\n                                'can '\n                                'help '\n                                'improve '\n                                'performance. '\n                                'Our '\n                                'results '\n                                'indicate '\n                                'that '\n                                'we '\n                                'have '\n                                'improved '\n                                'the '\n                                'state '\n                                'of the '\n                                'art on '\n                                'previously '\n                                'established '\n                                'benchmarks. '\n                                'In '\n                                'addition, '\n                                'we are '\n                                'publicly '\n                                'providing '\n                                'the '\n                                'large '\n                                'synthetic '\n                                'dataset '\n                                '(135,000 '\n                                'pages) '\n                                'as '\n                                'well '\n                                'as a '\n                                'new '\n                                'benchmark '\n                                'dataset: '\n                                'DSSE-200.'},\n {'Methods': 'We proposed a multimodal '\n             'fully convolutional '\n             'network (MFCN) for '\n             'document semantic '\n             'structure extraction. The '\n             'proposed model uses both '\n             'visual and textual '\n             'information. Moreover, we '\n             'propose an efficient '\n             'synthetic data generation '\n             'method that yields '\n             'per-pixel ground-truth. '\n             'Our unsupervised '\n             'auxiliary tasks help '\n             'boost performance tapping '\n             'into unlabeled real '\n             'documents, facilitating '\n             'better representation '\n             'learning. We showed that '\n             'both the multimodal '\n             'approach and unsupervised '\n             'tasks can help improve '\n             'performance. Our results '\n             'indicate that we have '\n             'improved the state of the '\n             'art on previously '\n             'established benchmarks. '\n             'In addition, we are '\n             'publicly providing the '\n             'large synthetic dataset '\n             '(135,000 pages) as well '\n             'as a new benchmark '\n             'dataset: DSSE-200.'},\n {'Unsupervised Learning Tasks': 'We '\n                                 'proposed '\n                                 'a '\n                                 'multimodal '\n                                 'fully '\n                                 'convolutional '\n                                 'network '\n                                 '(MFCN) '\n                                 'for '\n                                 'document '\n                                 'semantic '\n                                 'structure '\n                                 'extraction. '\n                                 'The '\n                                 'proposed '\n                                 'model '\n                                 'uses '\n                                 'both '\n                                 'visual '\n                                 'and '\n                                 'textual '\n                                 'information. '\n                                 'Moreover, '\n                                 'we '\n                                 'propose '\n                                 'an '\n                                 'efficient '\n                                 'synthetic '\n                                 'data '\n                                 'generation '\n                                 'method '\n                                 'that '\n                                 'yields '\n                                 'per-pixel '\n                                 'ground-truth. '\n                                 'Our '\n                                 'unsupervised '\n                                 'auxiliary '\n                                 'tasks '\n                                 'help '\n                                 'boost '\n                                 'performance '\n                                 'tapping '\n                                 'into '\n                                 'unlabeled '\n                                 'real '\n                                 'documents, '\n                                 'facilitating '\n                                 'better '\n                                 'representation '\n                                 'learning. '\n                                 'We '\n                                 'showed '\n                                 'that '\n                                 'both '\n                                 'the '\n                                 'multimodal '\n                                 'approach '\n                                 'and '\n                                 'unsupervised '\n                                 'tasks '\n                                 'can '\n                                 'help '\n                                 'improve '\n                                 'performance. '\n                                 'Our '\n                                 'results '\n                                 'indicate '\n                                 'that '\n                                 'we '\n                                 'have '\n                                 'improved '\n                                 'the '\n                                 'state '\n                                 'of '\n                                 'the '\n                                 'art '\n                                 'on '\n                                 'previously '\n                                 'established '\n                                 'benchmarks. '\n                                 'In '\n                                 'addition, '\n                                 'we '\n                                 'are '\n                                 'publicly '\n                                 'providing '\n                                 'the '\n                                 'large '\n                                 'synthetic '\n                                 'dataset '\n                                 '(135,000 '\n                                 'pages) '\n                                 'as '\n                                 'well '\n                                 'as a '\n                                 'new '\n                                 'benchmark '\n                                 'dataset: '\n                                 'DSSE-200.'},\n {'Comparisons with Prior Art': 'We '\n                                'proposed '\n                                'a '\n                                'multimodal '\n                                'fully '\n                                'convolutional '\n                                'network '\n                                '(MFCN) '\n                                'for '\n                                'document '\n                                'semantic '\n                                'structure '\n                                'extraction. '\n                                'The '\n                                'proposed '\n                                'model '\n                                'uses '\n                                'both '\n                                'visual '\n                                'and '\n                                'textual '\n                                'information. '\n                                'Moreover, '\n                                'we '\n                                'propose '\n                                'an '\n                                'efficient '\n                                'synthetic '\n                                'data '\n                                'generation '\n                                'method '\n                                'that '\n                                'yields '\n                                'per-pixel '\n                                'ground-truth. '\n                                'Our '\n                                'unsupervised '\n                                'auxiliary '\n                                'tasks '\n                                'help '\n                                'boost '\n                                'performance '\n                                'tapping '\n                                'into '\n                                'unlabeled '\n                                'real '\n                                'documents, '\n                                'facilitating '\n                                'better '\n                                'representation '\n                                'learning. '\n                                'We '\n                                'showed '\n                                'that '\n                                'both '\n                                'the '\n                                'multimodal '\n                                'approach '\n                                'and '\n                                'unsupervised '\n                                'tasks '\n                                'can '\n                                'help '\n                                'improve '\n                                'performance. '\n                                'Our '\n                                'results '\n                                'indicate '\n                                'that '\n                                'we '\n                                'have '\n                                'improved '\n                                'the '\n                                'state '\n                                'of the '\n                                'art on '\n                                'previously '\n                                'established '\n                                'benchmarks. '\n                                'In '\n                                'addition, '\n                                'we are '\n                                'publicly '\n                                'providing '\n                                'the '\n                                'large '\n                                'synthetic '\n                                'dataset '\n                                '(135,000 '\n                                'pages) '\n                                'as '\n                                'well '\n                                'as a '\n                                'new '\n                                'benchmark '\n                                'dataset: '\n                                'DSSE-200.'},\n {'Conclusion': 'We proposed a '\n                'multimodal fully '\n                'convolutional network '\n                '(MFCN) for document '\n                'semantic structure '\n                'extraction. The '\n                'proposed model uses '\n                'both visual and '\n                'textual information. '\n                'Moreover, we propose '\n                'an efficient synthetic '\n                'data generation method '\n                'that yields per-pixel '\n                'ground-truth. Our '\n                'unsupervised auxiliary '\n                'tasks help boost '\n                'performance tapping '\n                'into unlabeled real '\n                'documents, '\n                'facilitating better '\n                'representation '\n                'learning. We showed '\n                'that both the '\n                'multimodal approach '\n                'and unsupervised tasks '\n                'can help improve '\n                'performance. Our '\n                'results indicate that '\n                'we have improved the '\n                'state of the art on '\n                'previously established '\n                'benchmarks. In '\n                'addition, we are '\n                'publicly providing the '\n                'large synthetic '\n                'dataset (135,000 '\n                'pages) as well as a '\n                'new benchmark dataset: '\n                'DSSE-200.'}]\n"
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pprint\n",
    "tree = ET.parse(\"CVPR2017.pdf.tei.xml\")   # import xml from\n",
    "root = tree.getroot()\n",
    "\n",
    "headings_seq = [] #the sequence numbers of headings\n",
    "headings_name = [] #each heading name\n",
    "headings_para = [] #a list of dictionaries holding the the text under each paragraph\n",
    "\n",
    "for item in root.findall(\"./{http://www.tei-c.org/ns/1.0}text/{http://www.tei-c.org/ns/1.0}body/{http://www.tei-c.org/ns/1.0}div/{http://www.tei-c.org/ns/1.0}head\"):  #this stays the same for all XML files generated using GROBID\n",
    "    headings_seq.append(item.attrib)\n",
    "    headings_name.append(item.text)\n",
    "    headings_para.append({item.text : child.text})\n",
    "pp = pprint.PrettyPrinter(width=41, depth = 5, compact=True)\n",
    "pp.pprint(headings_seq)\n",
    "pp.pprint(headings_name)\n",
    "pp.pprint(headings_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}