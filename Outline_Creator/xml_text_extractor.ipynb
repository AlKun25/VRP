{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lxml.etree as ET\n",
    "from xml.dom import minidom\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "tree = ET.parse(\"CVPR2017.pdf.tei.xml\")   # import xml from\n",
    "root = tree.getroot()\n",
    "\n",
    "headings_seq = []\n",
    "headings_name = []\n",
    "headings_para = []\n",
    "\n",
    "headings_seq.append({'n' : '0 '})\n",
    "headings_name.append(\"Abstract\")\n",
    "\n",
    "for item in root.findall(\"./{http://www.tei-c.org/ns/1.0}text/{http://www.tei-c.org/ns/1.0}body/{http://www.tei-c.org/ns/1.0}div/{http://www.tei-c.org/ns/1.0}head\"):\n",
    "    headings_seq.append(item.attrib)\n",
    "    headings_name.append(item.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[{'n': '0 '}, {'n': '1.'}, {'n': '2.'},\n {'n': '3.'}, {'n': '3.1.'},\n {'n': '3.2.'}, {'n': '3.3.'},\n {'n': '4.'}, {'n': '5.'}, {'n': '6.'},\n {'n': '6.1.'}, {'n': '6.2.'}, {},\n {'n': '6.3.'}, {'n': '6.4.'},\n {'n': '7.'}]\n['Abstract', 'Introduction',\n 'Background', 'Method',\n 'Multimodal Fully Convolutional '\n 'Network',\n 'Text Embedding Map',\n 'Unsupervised Tasks',\n 'Synthetic Document Data',\n 'Implementation Details', 'Experiments',\n 'Ablation Experiment on Model '\n 'Architecture',\n 'Adding Textual Information', 'Methods',\n 'Unsupervised Learning Tasks',\n 'Comparisons with Prior Art',\n 'Conclusion']\n[{'Introduction': 'Document semantic '\n                  'structure extraction '\n                  '(DSSE) is an '\n                  'actively-researched '\n                  'area dedicated to '\n                  'understanding images '\n                  'of documents. The '\n                  'goal is to split a '\n                  'document image into '\n                  'regions of interest '\n                  'and to recognize the '\n                  'role of each region. '\n                  'It is usually done '\n                  'in two steps: the '\n                  'first step, often '\n                  'referred to as page '\n                  'segmentation, is '\n                  'appearance-based and '\n                  'attempts to '\n                  'distinguish text '\n                  'regions from regions '\n                  'like figures, tables '\n                  'and line segments. '\n                  'The second step, '\n                  'often referred to as '\n                  'logical structure '\n                  'analysis, is '\n                  'semantics-based and '\n                  'categorizes each '\n                  'region into '\n                  'semantically-relevant '\n                  'classes like '\n                  'paragraph and '\n                  'caption.'},\n {'Introduction': 'In this work, we '\n                  'propose a unified '\n                  'multimodal fully '\n                  'convolutional '\n                  'network (MFCN) that '\n                  'simultaneously '\n                  'identifies both '\n                  'appearance-based and '\n                  'semantics-based '\n                  'classes. It is a '\n                  'generalized page '\n                  'segmentation model '\n                  'that additionally '\n                  'performs '\n                  'fine-grained '\n                  'recognition on text '\n                  'regions: text '\n                  'regions are assigned '\n                  'specific labels '\n                  'based on their '\n                  'semantic '\n                  'functionality in the '\n                  'document. Our '\n                  'approach simplifies '\n                  'DSSE and better '\n                  'supports document '\n                  'image '\n                  'understanding.'},\n {'Introduction': 'We consider DSSE as '\n                  'a pixel-wise '\n                  'segmentation '\n                  'problem: each pixel '\n                  'is labeled as '\n                  'background, figure, '\n                  'table, paragraph, '\n                  'section heading, '\n                  'list, caption, etc. '\n                  'We show that our '\n                  'MFCN model trained '\n                  'in an end-to-end, '\n                  'pixels-topixels '\n                  'manner on document '\n                  'images exceeds the '\n                  'state-ofthe-art '\n                  'significantly. It '\n                  'eliminates the need '\n                  'to design complex '\n                  'heuristic rules and '\n                  'extract hand-crafted '\n                  'features '},\n {'Introduction': 'In many cases, '\n                  'regions like section '\n                  'headings or captions '\n                  'can be visually '\n                  'identified. In '},\n {'Introduction': 'To this end, our '\n                  'multimodal fully '\n                  'convolutional '\n                  'network is designed '\n                  'to leverage the '\n                  'textual information '\n                  'in the document as '\n                  'well. To incorporate '\n                  'textual information '\n                  'in a CNNbased '\n                  'architecture, we '\n                  'build a text '\n                  'embedding map and '\n                  'feed it to our MFCN. '\n                  'More specifically, '\n                  'we embed each '\n                  'sentence and map the '\n                  'embedding to the '\n                  'corresponding pixels '\n                  'where the sentence '\n                  'is represented in '\n                  'the document. '},\n {'Introduction': 'One of the '\n                  'bottlenecks in '\n                  'training fully '\n                  'convolutional '\n                  'networks is the need '\n                  'for pixel-wise '\n                  'ground truth data. '\n                  'Previous document '\n                  'understanding '\n                  'datasets '},\n {'Introduction': 'Our main '\n                  'contributions are '\n                  'summarized as '\n                  'follows:'},\n {'Introduction': '• We propose an '\n                  'end-to-end, unified '\n                  'network to address '\n                  'document semantic '\n                  'structure '\n                  'extraction. Unlike '\n                  'previous two-step '\n                  'processes, we '\n                  'simultaneously '\n                  'identify both '\n                  'appearance-based and '\n                  'semantics-based '\n                  'classes.'},\n {'Introduction': '• Our network '\n                  'supports both '\n                  'supervised training '\n                  'on image and text of '\n                  'documents, as well '\n                  'as unsupervised '\n                  'auxiliary training '\n                  'for better '\n                  'representation '\n                  'learning.'},\n {'Introduction': '• We propose a '\n                  'synthetic data '\n                  'generation process '\n                  'and use it to '\n                  'synthesize a '\n                  'large-scale dataset '\n                  'for training the '\n                  'supervised part of '\n                  'our deep MFCN '\n                  'model.'},\n {'Background': 'Page Segmentation. '\n                'Most earlier works on '\n                'page segmentation '},\n {'Background': 'With recent advances '\n                'in deep convolutional '\n                'neural networks, '\n                'several neural-based '\n                'models have been '\n                'proposed. Chen et '\n                'al. '},\n {'Background': 'Logical Structure '\n                'Analysis. Logical '\n                'structure is defined '\n                'as a hierarchy of '\n                'logical components in '\n                'documents, such as '\n                'section headings, '\n                'paragraphs and lists '},\n {'Background': 'Collecting pixel-wise '\n                'annotations for '\n                'thousands or millions '\n                'of images requires '\n                'massive labor and '\n                'cost. To this end, '\n                'several methods '},\n {'Background': 'Unsupervised Learning. '\n                'Several methods have '\n                'been proposed to use '\n                'unsupervised learning '\n                'to improve supervised '\n                'learning tasks. Mairal '\n                'et al. '},\n {'Background': 'Wen et al. '},\n {'Background': 'Language and Vision. '\n                'Several joint learning '\n                'tasks such as image '\n                'captioning '},\n {'Method': 'Our method does supervised '\n            'training for pixel-wise '\n            'segmentation with a '\n            'specialized multimodal '\n            'fully convolutional '\n            'network that uses a text '\n            'embedding map jointly with '\n            'the visual cues. Moreover, '\n            'our MFCN architecture also '\n            'supports two unsupervised '\n            'learning tasks to improve '\n            'the learned document '\n            'representation: a '\n            'reconstruction task based '\n            'on an auxiliary decoder '\n            'and a consistency task '\n            'evaluated in the main '\n            'decoder branch along with '\n            'the per-pixel segmentation '\n            'loss.'},\n {'Multimodal Fully Convolutional Network': 'As '\n                                            'shown '\n                                            'in '},\n {'Multimodal Fully Convolutional Network': 'First, '\n                                            'we '\n                                            'observe '\n                                            'that '\n                                            'several '\n                                            'semantic-based '\n                                            'classes '\n                                            'such '\n                                            'as '\n                                            'section '\n                                            'heading '\n                                            'and '\n                                            'caption '\n                                            'usually '\n                                            'occupy '\n                                            'relatively '\n                                            'small '\n                                            'areas. '\n                                            'Moreover, '\n                                            'correctly '\n                                            'identifying '\n                                            'certain '\n                                            'regions '\n                                            'often '\n                                            'relies '\n                                            'on '\n                                            'small '\n                                            'visual '\n                                            'cues, '\n                                            'like '\n                                            'lists '\n                                            'being '\n                                            'identified '\n                                            'by '\n                                            'small '\n                                            'bullets '\n                                            'or '\n                                            'numbers '\n                                            'in '\n                                            'front '\n                                            'of '\n                                            'each '\n                                            'item. '\n                                            'This '\n                                            'suggests '\n                                            'that '\n                                            'low-level '\n                                            'features '\n                                            'need '\n                                            'to '\n                                            'be '\n                                            'used. '\n                                            'However, '\n                                            'because '\n                                            'max-pooling '\n                                            'naturally '\n                                            'loses '\n                                            'information '\n                                            'during '\n                                            'downsampling, '\n                                            'FCN '\n                                            'often '\n                                            'performs '\n                                            'poorly '\n                                            'for '\n                                            'small '\n                                            'objects. '\n                                            'Long '\n                                            'et '\n                                            'al. '},\n {'Multimodal Fully Convolutional Network': 'We '\n                                            'also '\n                                            'notice '\n                                            'that '\n                                            'broader '\n                                            'context '\n                                            'information '\n                                            'is '\n                                            'needed '\n                                            'to '\n                                            'identify '\n                                            'certain '\n                                            'objects. '\n                                            'For '\n                                            'an '\n                                            'instance, '\n                                            'it '\n                                            'is '\n                                            'often '\n                                            'difficult '\n                                            'to '\n                                            'tell '\n                                            'the '\n                                            'difference '\n                                            'between '\n                                            'a '\n                                            'list '\n                                            'and '\n                                            'several '\n                                            'paragraphs '\n                                            'by '\n                                            'only '\n                                            'looking '\n                                            'at '\n                                            'parts '\n                                            'of '\n                                            'them. '\n                                            'In '},\n {'Text Embedding Map': 'Traditional '\n                        'image semantic '\n                        'segmentation '\n                        'models learn '\n                        'the semantic '\n                        'meanings of '\n                        'objects from a '\n                        'visual '\n                        'perspective. '\n                        'Our task, '\n                        'however, also '\n                        'requires '\n                        'understanding '\n                        'the text in '\n                        'images from a '\n                        'linguistic '\n                        'perspective. '\n                        'Therefore, we '\n                        'build a text '\n                        'embedding map '\n                        'and feed it to '\n                        'our multimodal '\n                        'model to make '\n                        'use of both '\n                        'visual and '\n                        'textual '\n                        'representations.'},\n {'Text Embedding Map': 'We treat a '\n                        'sentence as '\n                        'the minimum '\n                        'unit that '\n                        'conveys '\n                        'certain '\n                        'semantic '\n                        'meanings, and '\n                        'represent it '\n                        'using a '\n                        'lowdimensional '\n                        'vector. Our '\n                        'sentence '\n                        'embedding is '\n                        'built by '\n                        'averaging '\n                        'embeddings for '\n                        'individual '\n                        'words. This is '\n                        'a simple yet '\n                        'effective '\n                        'method that '\n                        'has been shown '\n                        'to be useful '\n                        'in many '\n                        'applications, '\n                        'including '\n                        'sentiment '\n                        'analysis '},\n {'Text Embedding Map': 'Specifically, '\n                        'our word '\n                        'embedding is '\n                        'learned using '\n                        'the skip-gram '\n                        'model '},\n {'Text Embedding Map': ', we maximize '\n                        'the average '\n                        'log '\n                        'probability'},\n {'Text Embedding Map': 'where T is the '\n                        'length of the '\n                        'sequence and C '\n                        'is the size of '\n                        'the context '\n                        'window. The '\n                        'probability of '\n                        'outputting a '\n                        'word w o given '\n                        'an input word '\n                        'w i is defined '\n                        'using '\n                        'softmax:'},\n {'Text Embedding Map': 'where v w and '\n                        'v ′ w are the '\n                        '\"input\" and '\n                        '\"output\" '\n                        'Ndimensional '\n                        'vector '\n                        'representations '\n                        'of w.'},\n {'Unsupervised Tasks': 'Although our '\n                        'synthetic '\n                        'documents '\n                        '(Sec. 4) '\n                        'provide a '\n                        'large amount '\n                        'of labeled '\n                        'data for '\n                        'training, they '\n                        'are limited in '\n                        'the variations '\n                        'of their '\n                        'layouts. To '\n                        'this end, we '\n                        'define two '\n                        'unsupervised '\n                        'loss functions '\n                        'to make use of '\n                        'real documents '\n                        'and to '\n                        'encourage '\n                        'better '\n                        'representation '\n                        'learning.'},\n {'Unsupervised Tasks': 'Reconstruction '\n                        'Task. It has '\n                        'been shown '\n                        'that '\n                        'reconstruction '\n                        'can help '\n                        'learning '\n                        'better '\n                        'representations '\n                        'and therefore '\n                        'improves '\n                        'performance '\n                        'for supervised '\n                        'tasks '},\n {'Unsupervised Tasks': 'Consistency '\n                        'Task. '\n                        'Pixel-wise '\n                        'annotations '\n                        'are '\n                        'laborintensive '\n                        'to obtain, '\n                        'however it is '\n                        'relatively '\n                        'easy to get a '\n                        'set of '\n                        'bounding boxes '\n                        'for detected '\n                        'objects in a '\n                        'document. For '\n                        'documents in '\n                        'PDF format, '\n                        'one can find '\n                        'bounding boxes '\n                        'by analyzing '\n                        'the rendering '\n                        'commands in '\n                        'the PDF files '\n                        '(See our '\n                        'supplementary '\n                        'document for '\n                        'typical '\n                        'examples). '\n                        'Even if their '\n                        'labels remain '\n                        'unknown, these '\n                        'bounding boxes '\n                        'are still '\n                        'beneficial: '\n                        'they provide '\n                        'knowledge of '\n                        'which parts of '\n                        'a document '\n                        'belongs to the '\n                        'same objects '\n                        'and thus '\n                        'should not be '\n                        'segmented into '\n                        'different '\n                        'fragments.'},\n {'Unsupervised Tasks': 'By building on '\n                        'the intuition '\n                        'that regions '\n                        'belonging to '\n                        'same objects '\n                        'should have '\n                        'similar '\n                        'feature '\n                        'representations, '\n                        'we define the '\n                        'consistency '\n                        'task loss L '\n                        'cons as '\n                        'follows. Let'},\n {'Unsupervised Tasks': 'be activations '\n                        'at location '\n                        '(i, j) in a '\n                        'feature map of '\n                        'size C × H × W '\n                        ', and b be the '\n                        'rectangular '\n                        'area in a '\n                        'bounding box. '\n                        'Let each '\n                        'rectangular '\n                        'area b is of '\n                        'size H b × W b '\n                        '. Then, for '\n                        'each b ∈ B, L '\n                        'cons will be '\n                        'given by'},\n {'Unsupervised Tasks': 'Minimizing '\n                        'consistency '\n                        'loss L cons '\n                        'encourages '\n                        'intra-region '\n                        'consistency.'},\n {'Unsupervised Tasks': 'The '\n                        'consistency '\n                        'loss L cons is '\n                        'differentiable '\n                        'and can be '\n                        'optimized '\n                        'using '\n                        'stochastic '\n                        'gradient '\n                        'descent. The '\n                        'gradient of L '\n                        'cons with '\n                        'respect to'},\n {'Unsupervised Tasks': 'since H b W b '\n                        '≫ 1, for '\n                        'efficiency it '\n                        'can be '\n                        'approximated '\n                        'by:'},\n {'Unsupervised Tasks': 'We use the '\n                        'unsupervised '\n                        'consistency '\n                        'loss, L cons , '\n                        'as a loss '\n                        'layer, that is '\n                        'evaluated at '\n                        'the main '\n                        'decoder branch '\n                        '(blue branch '\n                        'in '},\n {'Synthetic Document Data': 'Since our '\n                             'MFCN aims '\n                             'to '\n                             'generate '\n                             'a '\n                             'segmentation '\n                             'mask of '\n                             'the whole '\n                             'document '\n                             'image, '\n                             'pixel-wise '\n                             'annotations '\n                             'are '\n                             'required '\n                             'for the '\n                             'supervised '\n                             'task. '\n                             'While '\n                             'there are '\n                             'several '\n                             'publicly '\n                             'available '\n                             'datasets '\n                             'for page '\n                             'segmentation '},\n {'Synthetic Document Data': 'To '\n                             'address '\n                             'these '\n                             'issues, '\n                             'we '\n                             'created a '\n                             'synthetic '\n                             'data '\n                             'engine, '\n                             'capable '\n                             'of '\n                             'generating '\n                             'large-scale, '\n                             'pixel-wise '\n                             'annotated '\n                             'documents.'},\n {'Synthetic Document Data': 'Our '\n                             'synthetic '\n                             'document '\n                             'engine '\n                             'uses two '\n                             'methods '\n                             'to '\n                             'generate '\n                             'documents. '\n                             'The first '\n                             'produces '\n                             'completely '\n                             'automated '\n                             'and '\n                             'random '\n                             'layout of '\n                             'partial '\n                             'data '\n                             'scraped '\n                             'from the '\n                             'web. More '\n                             'specifically, '\n                             'we '\n                             'generate '\n                             'LaTeX '\n                             'source '\n                             'files in '\n                             'which '\n                             'paragraphs, '\n                             'figures, '\n                             'tables, '\n                             'captions, '\n                             'section '\n                             'headings '\n                             'and lists '\n                             'are '\n                             'randomly '\n                             'arranged '\n                             'to make '\n                             'up '\n                             'single, '\n                             'double, '\n                             'or '\n                             'triple-column '\n                             'PDFs. '\n                             'Candidate '\n                             'figures '\n                             'include '\n                             'academicstyle '\n                             'figures '\n                             'and '\n                             'graphic '\n                             'drawings '\n                             'downloaded '\n                             'using web '\n                             'image '\n                             'search, '\n                             'and '\n                             'natural '\n                             'images '\n                             'from MS '\n                             'COCO '},\n {'Synthetic Document Data': '• For '\n                             'paragraphs, '\n                             'we '\n                             'randomly '\n                             'sample '\n                             'sentences '\n                             'from a '\n                             '2016 '\n                             'English '\n                             'Wikipedia '\n                             'dump '},\n {'Synthetic Document Data': '• For '\n                             'section '\n                             'headings, '\n                             'we only '\n                             'sample '\n                             'sentences '\n                             'and '\n                             'phrases '\n                             'that are '\n                             'section '\n                             'or '\n                             'subsection '\n                             'headings '\n                             'in the '\n                             '\"Contents\" '\n                             'block in '\n                             'a '\n                             'Wikipedia '\n                             'page.'},\n {'Synthetic Document Data': '• For '\n                             'lists, we '\n                             'ensure '\n                             'that all '\n                             'items in '\n                             'a list '\n                             'come from '\n                             'the same '\n                             'Wikipedia '\n                             'page.'},\n {'Synthetic Document Data': '• For '\n                             'captions, '\n                             'we either '\n                             'use the '\n                             'associated '\n                             'caption '\n                             '(for '\n                             'images '\n                             'from MS '\n                             'COCO) or '\n                             'the title '\n                             'of the '\n                             'image in '\n                             'web image '\n                             'search, '\n                             'which can '\n                             'be found '\n                             'in the '\n                             'span with '\n                             'class '\n                             'name \"irc '\n                             'pt\".'},\n {'Synthetic Document Data': 'To '\n                             'further '\n                             'increase '\n                             'the '\n                             'complexity '\n                             'of the '\n                             'generated '\n                             'document '\n                             'layouts, '\n                             'we '\n                             'collected '\n                             'and '\n                             'labeled '\n                             '271 '\n                             'documents '\n                             'with '\n                             'varied, '\n                             'complicated '\n                             'layouts. '\n                             'We then '\n                             'randomly '\n                             'replaced '\n                             'each '\n                             'element '\n                             'with a '\n                             'standalone '\n                             'paragraph, '\n                             'figure, '\n                             'table, '\n                             'caption, '\n                             'section '\n                             'heading '\n                             'or list '\n                             'generated '\n                             'as stated '\n                             'above.'},\n {'Synthetic Document Data': 'In total, '\n                             'our '\n                             'synthetic '\n                             'dataset '\n                             'contains '\n                             '135,000 '\n                             'document '\n                             'images. '\n                             'Examples '\n                             'of our '\n                             'synthetic '\n                             'documents '\n                             'are shown '\n                             'in '},\n {'Implementation Details': 'We perform '\n                            'per-channel '\n                            'mean '\n                            'subtraction '\n                            'and resize '\n                            'each input '\n                            'image so '\n                            'that its '\n                            'longer '\n                            'side is '\n                            'less than '\n                            '384 '\n                            'pixels. No '\n                            'other '\n                            'pre-processing '\n                            'is '\n                            'applied. '\n                            'We use '\n                            'Adadelta '},\n {'Implementation Details': 'For text '\n                            'embedding, '\n                            'we '\n                            'represent '\n                            'each word '\n                            'as a '\n                            '128dimensional '\n                            'vector and '\n                            'train a '\n                            'skip-gram '\n                            'model on '\n                            'the 2016 '\n                            'English '\n                            'Wikipedia '\n                            'dump '},\n {'Implementation Details': 'Post-processing. '\n                            'We apply '\n                            'an '\n                            'optional '\n                            'post-processing '\n                            'step as a '\n                            'cleanup '\n                            'strategy '\n                            'for '\n                            'segment '\n                            'masks. For '\n                            'documents '\n                            'in PDF '\n                            'format, we '\n                            'obtain a '\n                            'set of '\n                            'candidate '\n                            'bounding '\n                            'boxes by '\n                            'analyzing '\n                            'the PDF '\n                            'format to '\n                            'find '\n                            'element '\n                            'boxes. We '\n                            'then '\n                            'refine the '\n                            'segmentation '\n                            'masks by '\n                            'first '\n                            'calculating '\n                            'the '\n                            'average '\n                            'class '\n                            'probability '\n                            'for pixels '\n                            'belonging '\n                            'to the '\n                            'same box, '\n                            'followed '\n                            'by '\n                            'assigning '\n                            'the most '\n                            'likely '\n                            'label to '\n                            'these '\n                            'pixels.'},\n {'Experiments': 'We used three '\n                 'datasets for '\n                 'evaluations: '\n                 'ICDAR2015 '},\n {'Experiments': 'The performance is '\n                 'measured in terms of '\n                 'pixel-wise '\n                 'intersection-over-union '\n                 '(IoU), which is '\n                 'standard in semantic '\n                 'segmentation tasks. '\n                 'We optimize the '\n                 'architecture of our '\n                 'MFCN model based on '\n                 'the DSSE-200 dataset '\n                 'since it contains '\n                 'both appearance-based '\n                 'and semantics-based '\n                 'labels. Sec. 6.4 '\n                 'compares our results '\n                 'to state-of-the-art '\n                 'methods on the '\n                 'ICDAR2015 and '\n                 'SectLabel datasets.'},\n {'Ablation Experiment on Model Architecture': 'We '\n                                               'first '\n                                               'systematically '\n                                               'evaluate '\n                                               'the '\n                                               'effectiveness '\n                                               'of '\n                                               'different '\n                                               'network '\n                                               'architectures. '\n                                               'Results '\n                                               'are '\n                                               'shown '\n                                               'in '},\n {'Ablation Experiment on Model Architecture': 'As '\n                                               'a '\n                                               'simple '\n                                               'baseline '},\n {'Ablation Experiment on Model Architecture': 'Next, '\n                                               'we '\n                                               'add '\n                                               'skip '\n                                               'connections '\n                                               'to '\n                                               'the '\n                                               'model, '\n                                               'resulting '\n                                               'in '\n                                               'Model2. '\n                                               'Note '\n                                               'that '\n                                               'this '\n                                               'model '\n                                               'is '\n                                               'similar '\n                                               'to '\n                                               'the '\n                                               'SharpMask '\n                                               'model. '\n                                               'We '\n                                               'observe '\n                                               'a '\n                                               'mean '\n                                               'IoU '\n                                               'of '\n                                               '65.4%, '\n                                               '4% '\n                                               'better '\n                                               'than '\n                                               'the '\n                                               'base '\n                                               'model. '\n                                               'The '\n                                               'improvements '\n                                               'are '\n                                               'even '\n                                               'more '\n                                               'significant '\n                                               'for '\n                                               'small '\n                                               'objects '\n                                               'like '\n                                               'captions.'},\n {'Ablation Experiment on Model Architecture': 'We '\n                                               'further '\n                                               'evaluate '\n                                               'the '\n                                               'effectiveness '\n                                               'of '\n                                               'replacing '\n                                               'bilinear '\n                                               'upsampling '\n                                               'with '\n                                               'unpooling, '\n                                               'giving '\n                                               'Model3. '\n                                               'All '\n                                               'upsampling '\n                                               'layers '\n                                               'in '\n                                               'Model2 '\n                                               'are '\n                                               'replaced '\n                                               'by '\n                                               'unpooling '\n                                               'while '\n                                               'other '\n                                               'parts '\n                                               'are '\n                                               'kept '\n                                               'unchanged. '\n                                               'Doing '\n                                               'so '\n                                               'results '\n                                               'in '\n                                               'a '\n                                               'significant '\n                                               'improvement '\n                                               'for '\n                                               'mean '\n                                               'IoU '\n                                               '(65.4% '\n                                               'vs. '\n                                               '71.2%). '\n                                               'This '\n                                               'suggests '\n                                               'that '\n                                               'the '\n                                               'pooled '\n                                               'index '\n                                               'should '\n                                               'not '\n                                               'be '\n                                               'discarded '\n                                               'during '\n                                               'decoding. '\n                                               'These '\n                                               'indexes '\n                                               'are '\n                                               'helpful '\n                                               'to '\n                                               'disambiguate '\n                                               'the '\n                                               'location '\n                                               'information '\n                                               'when '\n                                               'constructing '\n                                               'the '\n                                               'segmentation '\n                                               'mask '\n                                               'in '\n                                               'the '\n                                               'decoder.'},\n {'Ablation Experiment on Model Architecture': 'Finally, '\n                                               'we '\n                                               'investigate '\n                                               'the '\n                                               'use '\n                                               'of '\n                                               'dilated '\n                                               'convolutions. '\n                                               'Model3 '\n                                               'is '\n                                               'equivalent '\n                                               'to '\n                                               'using '\n                                               'dilated '\n                                               'convolution '\n                                               'when '\n                                               'd '\n                                               '= '\n                                               '1. '\n                                               'Model4 '\n                                               'sets '\n                                               'd '\n                                               '= '\n                                               '8 '\n                                               'while '\n                                               'Model5 '\n                                               'uses '\n                                               'the '\n                                               'dilated '\n                                               'block '\n                                               'illustrated '\n                                               'in '},\n {'Adding Textual Information': 'We now '\n                                'investigate '\n                                'the '\n                                'importance '\n                                'of '\n                                'textual '\n                                'information '\n                                'in our '\n                                'multimodal '\n                                'model. '\n                                'We '\n                                'take '\n                                'the '\n                                'best '\n                                'architecture, '\n                                'Model5, '\n                                'as our '\n                                'vision-only '\n                                'model, '\n                                'and '\n                                'incorporate '\n                                'a text '\n                                'embedding '\n                                'map '\n                                'via a '\n                                'bridge '\n                                'module '\n                                'depicted '\n                                'in '},\n {'Adding Textual Information': '75.4 '\n                                '75.9 '},\n {'Methods': 'non-text text Leptonica '},\n {'Unsupervised Learning Tasks': 'Here, '\n                                 'we '\n                                 'examine '\n                                 'how '\n                                 'the '\n                                 'proposed '\n                                 'two '\n                                 'unsupervised '\n                                 'learning '\n                                 'tasks '\n                                 '-reconstruction '\n                                 'and '\n                                 'consistency '\n                                 'taskscan '\n                                 'complement '\n                                 'the '\n                                 'pixel-wise '\n                                 'classification '\n                                 'during '\n                                 'training. '\n                                 'We '\n                                 'take '\n                                 'the '\n                                 'best '\n                                 'model '\n                                 'in '\n                                 'Sec. '\n                                 '6.2, '\n                                 'and '\n                                 'only '\n                                 'change '\n                                 'the '\n                                 'training '\n                                 'objectives. '\n                                 'Our '\n                                 'model '\n                                 'is '\n                                 'then '\n                                 'fine-tuned '\n                                 'in a '\n                                 'semisupervised '\n                                 'manner '\n                                 'as '\n                                 'described '\n                                 'in '\n                                 'Sec. '\n                                 '5. '\n                                 'The '\n                                 'results '\n                                 'are '\n                                 'shown '\n                                 'in '},\n {'Comparisons with Prior Art': 'Comparisons '\n                                'on '\n                                'ICDAR2015 '\n                                'dataset '},\n {'Comparisons with Prior Art': 'Comparisons '\n                                'on '\n                                'SectLabel '\n                                'dataset '},\n {'Conclusion': 'We proposed a '\n                'multimodal fully '\n                'convolutional network '\n                '(MFCN) for document '\n                'semantic structure '\n                'extraction. The '\n                'proposed model uses '\n                'both visual and '\n                'textual information. '\n                'Moreover, we propose '\n                'an efficient synthetic '\n                'data generation method '\n                'that yields per-pixel '\n                'ground-truth. Our '\n                'unsupervised auxiliary '\n                'tasks help boost '\n                'performance tapping '\n                'into unlabeled real '\n                'documents, '\n                'facilitating better '\n                'representation '\n                'learning. We showed '\n                'that both the '\n                'multimodal approach '\n                'and unsupervised tasks '\n                'can help improve '\n                'performance. Our '\n                'results indicate that '\n                'we have improved the '\n                'state of the art on '\n                'previously established '\n                'benchmarks. In '\n                'addition, we are '\n                'publicly providing the '\n                'large synthetic '\n                'dataset (135,000 '\n                'pages) as well as a '\n                'new benchmark dataset: '\n                'DSSE-200.'}]\n"
    }
   ],
   "source": [
    "mydoc = minidom.parse(\"CVPR2017.pdf.tei.xml\")\n",
    "divs = mydoc.getElementsByTagName(\"div\")\n",
    "for div in divs: #<div> \n",
    "    if(div.parentNode.nodeName == \"body\"):\n",
    "        try:\n",
    "            x = 'works'\n",
    "        except AttributeError:\n",
    "            continue\n",
    "        else:\n",
    "            for elem in div.childNodes:\n",
    "                if(not elem.nodeName =='formula'):\n",
    "                    if(elem.nodeName == 'head'):\n",
    "                        section = elem.firstChild.data\n",
    "                        continue\n",
    "                    if(elem.nodeName == 'p'):\n",
    "                        headings_para.append({section : elem.firstChild.data})\n",
    "\n",
    "pp = pprint.PrettyPrinter(width=41, depth = 5, compact=True)\n",
    "pp.pprint(headings_seq)\n",
    "pp.pprint(headings_name)\n",
    "pp.pprint(headings_para)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}